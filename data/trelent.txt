The ActWrapper.save_act function saves the ActWrapper object to a pickle file.
The saved ActWrapper can be loaded by another script using the cloudpickle package,
which is used for serializing objects in Python. The saved file is a zip archive containing
the pickled ActWrapper and an auxiliary directory containing all of the necessary data files that were used to create it.
====SPLIT====
The nature_cnn function takes as input a state of size (84, 84, 4) and returns an
output of size 512. The network architecture is the same as that described in the paper:
Nature DQN. In particular:
====SPLIT====
The conv_only function creates a convolutional neural network with the following parameters:
    -convs = A list of tuples that represent the number of outputs and kernel size for each layer. 
    -conv_kwargs = A dictionary containing all other keyword arguments to be passed into layers.convolution2d().
====SPLIT====
The make_vec_env function creates a vectorized environment.
====SPLIT====
The parse_unknown_args function is a helper function that parses arguments from the command line.
It takes in a list of arguments and returns a dictionary with the key being the argument name and value being 
the argument value. If an argument has no equal sign, it will be assigned to whatever variable preceded_by_key is set to.
====SPLIT====
The clear_mpi_env_vars function removes all the MPI environment variables from os.environ,
so that we can run a test without actually running mpirun.  This is useful for testing
that the mpi_required decorator works correctly.
====SPLIT====
The cg function takes as input a function f_Ax that returns the matrix-vector product Ax.
It also takes an initial guess x, and a number of iterations to run. It then performs the following:
    1) computes r = b - Ax(x)
    2) computes z = r / (rdotr/rdotr_0), where rdotr is the norm of r, and 
        rdotr_0 is some initial value of it (e.g., from an earlier iteration). 
        This step should be performed in your cg function!   -- i
====SPLIT====
The observation_placeholder function creates a placeholder for the observation.
The shape of the placeholder is (batch_size,) + ob_space.shape, where batch_size is an optional argument and ob_space 
is obtained from env's observation space via env.observation_space.
====SPLIT====
The observation_input function creates a placeholder for the observation space of the environment.
It also returns an encoded version of that observation space. The function takes in as input:
    -ob_space: An object containing information about the observation space from 
                env.observation_space (i.e., ob_space contains keys such as shape and dtype). 
    -batch size: The number of observations to be fed into it at once, which is None by default, and can be left empty if desired.
====SPLIT====
The encode_observation function takes in an observation space and a placeholder for the current observation.
It returns a tensorflow operation that will encode the observations as per their given space.
If it is a Discrete space, then one-hot encoding is used to turn it into a vector of length n (where n = |ob_space|).
If it is Box or MultiDiscrete, then the observations are returned as-is.
====SPLIT====
The RolloutWorker.save_policy function saves the current policy to a file.
The function takes as input the path where you want to save your policy.
====SPLIT====
The RolloutWorker.logs function returns a list of tuples, where each tuple is (key, value).
The key is a string that describes the value. For example, &quot;success_rate&quot;, or &quot;episode&quot;.
The value is the actual data for that quantity. For example, 0.95 or 500.
====SPLIT====
The smooth function takes a 1D array and smooths it.
The radius of the smoothing function is an input parameter.
If the mode is 'two_sided', then mirror the data for 
smoothing. If mode='causal' then only take values on 
one side of center point.
====SPLIT====
The copy_obs_dict function takes an observation dictionary as input and returns a new observation
dictionary with copied numpy arrays for all observations. This is done so that we can preserve the 
original data while modifying the data in between time steps.
====SPLIT====
The obs_space_info function takes as input an observation space and returns a tuple of three items:
    1. A list of the names of each element in the observation dictionary, which is a list that can contain elements with different types (e.g., ints, floats).
    2. A dictionary mapping each name to its corresponding shape (i.e., a tuple containing integers representing the dimensions).
    3. A dictionary mapping each name to its corresponding data type (i.e., one of 32-bit float, 16-bit float or 64-bit integer).
====SPLIT====
The q_retrace function calculates the target Q-value for a given batch of experiences.
The target Q-value is calculated according to the following formula:
Q_retrace(s,a) = r(s, a) + γ * min_(over t′=t+τ,...)(Q_target(next_state, next_action))
where: 
• s is the current state; 
• a is the action taken in state s; 
• r(s,a) refers to [the reward received after taking action a in state s]; and • γ ∈ [0,
====SPLIT====
The PiecewiseSchedule.value function returns the value of the schedule at a given time.
====SPLIT====
The _subproc_worker function is a helper function that will be used to create the worker processes for the
SubprocVecEnv class. It takes in several arguments, but we are mainly interested in pipe and env_fn_wrapper. The 
pipe argument is a multiprocessing connection object which can be thought of as a portal between two processes 
that lets data flow from one process to another (in this case, it lets the main process send information to each 
of our worker processes). The env_fn_wrapper argument is an instance of VecEnvFactory class which has an attribute x() that returns our instantiated
====SPLIT====
The learn function is the main function for learning. It takes in a series of hyperparameters as arguments,
and then trains an agent using Proximal Policy Optimization with experience replay. The hyperparameters are:
====SPLIT====
The sf01 function takes a numpy array and swaps the first two axes.
The resulting array is then reshaped into a 2D matrix, with each row
representing one of the original images in the batch. This allows us to 
easily apply functions that work on matrices to our entire batch.
====SPLIT====
The pretty_eta function takes a number of seconds and returns a human-readable string.
====SPLIT====
The boolean_flag function is a helper function that adds a boolean flag to the parser.
The boolean_flag function takes four arguments:
    1) The parser object, which will be used to add the flag to the command line interface;
    2) The name of the flag, which should be in all caps and use hyphens instead of underscores;
    3) A default value for whether or not the argument is true (defaults to False); and 
    4) A help message explaining what this argument does. This message will appear when someone runs 'python3 -h' on their script.
====SPLIT====
The get_wrapper_by_name function is a helper function that allows us to access the environment
of any wrapper. For example, if we wanted to access the environment of the AtariPreprocessing wrapper,
we would call get_wrapper_by_name(env, &quot;AtariPreprocessing&quot;). This will return an instance of 
the AtariPreprocessing class which we can then use to modify our environment.
====SPLIT====
The pickle_load function loads a pickled object from disk.
====SPLIT====
The RunningAvg.update function updates the running average of a quantity.
It takes as input a new value for that quantity, and returns the updated running average.
The update is performed using exponential decay, with the supplied gamma parameter controlling 
the rate of decay.
====SPLIT====
The store_args function is a decorator that allows you to define the arguments
that your function takes as inputs.  It also stores those values in self.__dict__,
which can be useful for functions that call other functions with different parameters.
====SPLIT====
The flatten_grads function takes a list of variables and a list of gradients,
and returns the flattened concatenation of all the gradients. This is useful when
applying algorithms such as Conjugate Gradient which require gradient vectors.
====SPLIT====
The nn function creates a neural network.
====SPLIT====
The mpi_fork function is a helper function that executes the same Python
script under MPI, using mpirun to spawn parallel processes.  It's used by
:func:`train_mpi`, but it can also be called directly if you want to distribute
a different script.  The first argument is the number of processes we want to
spawn, and all subsequent arguments are passed directly to `sys.argv`.
====SPLIT====
The get_session function is a convenience function for getting a default TensorFlow session.
It creates the session if it doesn't exist, and returns the current session otherwise.
====SPLIT====
The initialize function initializes all of the variables in a TensorFlow session.
This is particularly helpful when starting a new session and having to call all of
the variables before calling any operations. This function avoids having to explicitly
call tf.initialize_all_variables() every time.
====SPLIT====
The adjust_shape function takes a placeholder and data as input.
If the data is an array, it checks whether its shape is compatible with the placeholder.
If not, it reshapes the data to be compatible with the placeholder's shape. 
It then returns this reshaped array.
====SPLIT====
The wrap_deepmind function takes as input an environment, and returns a new 
environment that is wrapped using the following functions:
- EpisodicLifeEnv - This function treats any single life of the agent as a 
episode. If an agent dies, it will start from the beginning of the level. This 
is done by resetting to state 0 when lives are exhausted. The number of lives is 
set to be 1 initially in this wrapper class. You can change this value if you'd like more than 1 life per episode (e.g., 3).
====SPLIT====
The EpisodicLifeEnv.reset function is a wrapper function that calls the Gym.env.reset()
method, and then checks to see if the episode ended as a result of the reset step. If so, it
sets self.was_real_done=False and returns an observation from an environment with one less life
than it started with (the lives lost in the previous episode don't count towards this env's 
life total). If not, self.was_real_done=True and all of the observations returned by this env 
have their lives intact.
====SPLIT====
The gpu_count function returns the number of GPUs available on the current system.
====SPLIT====
The setup_mpi_gpus function sets the CUDA_VISIBLE_DEVICES environment variable to
the set of IDs corresponding to local GPUs that are visible by each MPI process.
This is useful for multi-GPU runs when you don't want one MPI process to take up
all of the GPUs on your node.  This function should be called before any other GPU-related setup, and it will overwrite whatever value was previously set in os.environ['CUDA_VISIBLE_DEVICES'].  The function returns a list containing the IDs of all GPUs that this particular MPI process can see.
====SPLIT====
The get_local_rank_size function returns a tuple of the local rank and size.
====SPLIT====
The share_file function is used to share a file between processes.
It takes in the MPI communicator and the path of the file as input.
If rank 0 has a copy of that file, it will broadcast it to all other ranks.
Otherwise, it will broadcast an empty string and create a new copy of that 
file on rank 0 with write permission.
====SPLIT====
The dict_gather function takes a dict of data and gathers it from multiple MPI workers.
If some workers do not have data, those keys will simply be omitted from the gathered dict.
The op argument specifies how the values should be combined (e.g., with np.mean).
====SPLIT====
The discount function allows the agent to handle rewards that come in
sequence, rather than just a single number. The discount function is:
====SPLIT====
The PrioritizedReplayBuffer.add function accomplishes two things:
====SPLIT====
The PrioritizedReplayBuffer.update_priorities function is a helper function that updates the priorities of all samples in the replay buffer.
====SPLIT====
The wrap_deepmind_retro function takes as input an environment and returns a wrapped version of the environment.
The function first wraps the environment in a WarpFrame class, which converts frames to grayscale, resizes them to 
size 84 x 84, and then finally scales the pixel intensities down by a factor of 10. Next it clips all rewards to be 
between -5 and 5 (to ensure that no reward dominates too heavily). Finally if frame_stack &gt; 1 it stacks 4 consecutive 
frames together so that our agent can determine movement direction over multiple frames.
====SPLIT====
The make_sample_her_transitions function is a helper function that returns a function which can be used to sample transitions from the replay buffer.
====SPLIT====
The parse_cmdline_kwargs function is a helper function that parses the command line arguments 
(the unknown_args) and returns a dictionary of key-value pairs. The parse_cmdline_kwargs function 
is called in the train() and test() functions, so it can be used to pass parameters into those functions.
====SPLIT====
The compute_geometric_median function computes the geometric median of a set of points.
The geometric median is the point that minimizes the sum of distances to all other points.
It can be multiple points, in which case it returns an array with each entry being a 
geometric median for each set of points given as input.
====SPLIT====
The Keypoint.project function is a helper function that converts the Keypoint object from one image to another.
It takes in two images, and returns a new Keypoint object with its x and y attributes projected onto the second image.
====SPLIT====
The Keypoint.shift function shifts the x and y coordinates of a Keypoint object by a given number of pixels.
====SPLIT====
The Keypoint.draw_on_image function draws the keypoint onto a given image.
The image must be of appropriate shape: (height, width, 3) when not grayscale and (height, width) when
grayscale. The color is determined by the keypoint_color class attribute. If multiple keypoints are to be drawn on 
the same image, then each should have a unique label assigned.
====SPLIT====
The Keypoint.generate_similar_points_manhattan function generates a list of points that lie
on the manhattan distance around the given Keypoint. For example, if S is the number of steps
that we take (S is an argument to this function), then for one particular point P, there will be S+2*S+2
points generated around it:
====SPLIT====
The Keypoint.copy function is a convenience function that creates a new Keypoint object with the same attributes as the original.
It is equivalent to:
====SPLIT====
The Keypoint.deepcopy function creates a new Keypoint object with the same x and y values as the original.
It is useful for creating copies of keypoints that can be modified without affecting the original.
====SPLIT====
The KeypointsOnImage.on function is a convenience function for users who have
a keypoints object and an image (or array) of the same shape. It will create a new
KeypointsOnImage instance where all keypoint coordinates are adjusted to fit into
the new image shape. This way, it becomes possible to draw a keypoint on an image, even if the 
keypoint was outside of the old image. The original KeypointsOnImage instance is not modified by this function.
====SPLIT====
The KeypointsOnImage.draw_on_image function draws the keypoints onto a given image.
It is implemented in a separate class because this function is quite commonly used,
for example in keypoint augmentation functions.
====SPLIT====
The KeypointsOnImage.shift function shifts the xy-coordinates of all keypoints by a given
x and y offset.
====SPLIT====
The KeypointsOnImage.copy function copies the KeypointsOnImage instance.
It is equivalent to copy.copy(keypoints_on_image). It does not copy the keypoints, i.e.
it is not a deep copy.
====SPLIT====
The KeypointsOnImage.deepcopy function is a very high-level method that creates a new
KeypointsOnImage instance with the same keypoints as this one. It does not copy
the other data of this KeypointOnImage instance (e.g. the shape information). This
is often enough for most use cases and also in terms of performance.
====SPLIT====
The BoundingBox.project function takes a BoundingBox object and two shapes,
from_shape and to_shape. It returns a new BoundingBox object with the coordinates
of its corners projected into the coordinate system defined by to_shape. For example,
if we have an image of size (1000, 1000) that is later resized to (100, 100), then every pixel in the original image will correspond to 100x100 pixels in the resized image. So if we have a bounding box around one of those pixels before resizing it will now be around 1/10th of a pixel. This function projects all four corner
====SPLIT====
The BoundingBox.extend function extends the bounding box by a given number of pixels on all sides.
====SPLIT====
The BoundingBox.intersection function takes two BoundingBox objects as arguments.
It returns a new BoundingBox object that represents the overlapping region of the two input bounding boxes.
If there is no overlap, it returns None.
====SPLIT====
The BoundingBox.union function takes two BoundingBox objects and returns a new BoundingBox object that represents the union of the bounding boxes.
====SPLIT====
The BoundingBox.iou function calculates the intersection over union of two bounding boxes.
The function returns a float between 0 and 1, where 1 indicates perfect overlap and 0 no overlap.
====SPLIT====
The BoundingBox.is_fully_within_image function checks whether the bounding box is fully within the image area.
====SPLIT====
The BoundingBox.is_partly_within_image function checks whether the bounding box is at least partially within the image.
====SPLIT====
The BoundingBox.is_out_of_image function checks whether the bounding box is fully or partly outside of an image.
This can be useful to determine whether a bounding box that was supposed to be cropped actually contains any data
or not (e.g. when the cropping area is outside of the image's boundaries).
====SPLIT====
The BoundingBox.clip_out_of_image function clips the bounding box to image boundaries.
====SPLIT====
The BoundingBox.draw_on_image function draws the bounding box on an image.
It is a wrapper around BoundingBox.draw_box_on_image().
The function supports drawing of the following attributes in the BB object:
- label (e.g. &quot;dog&quot;) and all attributes of BB class, such as x, y, w and h (e.g., &quot;dog&quot;, x=10, y=20)
- keypoints ([(x0,y0), ...]) with labels [&quot;label0&quot;, ...] or None for unlabeled keypoints or no keypoints at all  # TODO make optional argument
====SPLIT====
The BoundingBox.extract_from_image function takes an image and a bounding box,
and returns the part of the image that is inside the bounding box.
If pad=True, then if any side of the BB is outside of the image, it will pad that side with zeros.
====SPLIT====
The BoundingBox.copy function creates a new BoundingBox object with the same
coordinates as the original.  If any of the coordinates are None, then they will
be replaced by their original values.  This allows for easy copying of bounding boxes.
====SPLIT====
The BoundingBoxesOnImage.draw_on_image function draws the bounding boxes onto a given image.
====SPLIT====
The BoundingBoxesOnImage.remove_out_of_image function removes all bounding boxes that are partially/fully outside of
the image.
====SPLIT====
The BoundingBoxesOnImage.clip_out_of_image function removes all BoundingBoxes from the
BoundingBoxesOnImage object that are outside of the image plane.
====SPLIT====
The BoundingBoxesOnImage.deepcopy function creates a deep copy of the BoundingBoxesOnImage object.
This means that all information about the bounding boxes is copied to another object, i.e. no reference to 
the original bounding box objects or their data will be stored in the copy.
====SPLIT====
The Emboss function is a nonlinear filter that slightly sharpens the edges of an image.
It computes a convolution with the following kernel:
```
    [[-2 - 1  0], [-2 - 1  0], [-2 - 1  0]]
    [[-2 - 1  0], [ 2 10 11], [-2 - 1  0]]
    [[-2 - 3/8+0.5   6/8+0.5   9/8+0.5 ], [-3/4-0.25 4     7     3/4 + .25 ], [7
====SPLIT====
The EdgeDetect function applies a simple edge detector filter to images.
It is implemented as a convolution with the following kernel:
====SPLIT====
The DirectedEdgeDetect function is a wrapper around the Convolve function. It takes in an image and outputs
the same image with edges enhanced or suppressed by altering the direction of lines. The parameters are:
    alpha (float) - A value between 0 and 1 that represents how much to enhance edges. 0 means no enhancement, 
                    while 1 enhances all edges by a significant margin. This parameter is optional, with a default 
                    value of 0.
====SPLIT====
The normalize_shape function =============================
====SPLIT====
The project_coords function takes a list of coordinates and the shape of the image they are from,
and returns them in the shape of an image that is passed to it. For example, if you have a bounding box
in an image with (1000, 1000) pixels and you want to resize it to (200, 400), project_coords will scale 
the x values by 200/1000 = 0.2 and y values by 400/1000 = 0.4 for each coordinate point.
====SPLIT====
The AdditivePoissonNoise function adds poisson noise to the input images.
Poisson noise is comparable to gaussian noise, but results from the fact that 
the discrete poisson distribution has fatter tails than the normal/gaussian 
distribution. This function can be used as a replacement for AdditiveGaussianNoise, 
if one wants to generate images with more extreme contrasts than would have been allowed by AdditiveGaussianNoise.
====SPLIT====
The Dropout function is a regularization technique that approximates training a large number of
high-dimensional neural networks with saturating nonlinearities. It is based on the idea that
neurons of high activation levels are less likely to be dropped out. During training, Dropout nodes
are selected at random to be dropped and their contribution to the activation of downstream neurons
is temporally removed on a per-sample basis. A parameter controls how much dropout influences the network:
====SPLIT====
The CoarseDropout function is a wrapper around the FromLowerResolution class. It takes an image and multiplies
the pixel values with a value between 0 and 1, where 0 means no coarse dropout at all and 1 meaning that all pixels
will be replaced by their average color (for each channel). The used Binomial distribution will have as parameter
a probability of p per pixel. A typical usage is to set p=0.2 for images with a size of 20x20 pixels resulting in an
effectively random image at 20x20 resolution, where roughly 2% of the image's pixels are randomly replaced by their 
average
====SPLIT====
The ImpulseNoise function adds salt and pepper noise to an image.
====SPLIT====
The SaltAndPepper function replaces a random selection of pixels with salt and pepper noise.
The function takes the probability as an input, where it will replace that fraction of pixels in an image 
with either 0 or 255. The per_channel parameter can be used to influence how often the replacement happens per channel. 
If it is set to True, then exactly half of all images (and for those images half of each channel) will be affected.
====SPLIT====
The Pepper function replaces a fraction of all pixels in an image with black-ish pixel values.
The fraction is controlled via the p parameter.
If `per_channel` is set to True, the same fraction will be applied to each color channel.
====SPLIT====
The CoarsePepper function replaces pixels in an image with pepper, i.e. black-ish pixels.
This is similar to Dropout, but works on a coarser scale and thus gives more coherent results.
====SPLIT====
The ContrastNormalization function is a wrapper for the contrast module.
It normalizes the image's brightness and contrast to a certain degree.
The alpha parameter controls the amount of normalization: higher values mean 
higher contrast and brightness, lower values decrease those parameters.
====SPLIT====
The is_single_float function returns True if the input is a single float, and False otherwise.
====SPLIT====
The is_integer_array function returns True if the input is an array of integers.
====SPLIT====
The is_float_array function returns True if the input is a numpy array and
the data type of the array is a sub-type of np.floating, which in this context
refers to float types (np.float32, np.float64) or equivalent unsigned integer
types (np.uint8, np.uint16). The function returns False otherwise.
====SPLIT====
The new_random_state function is a convenience function for generating new random states.
It accepts either an integer as seed or None, in which case it will generate a seed automatically.
If the fully_random parameter is set to True, then it will generate completely random seeds by using np.random.randint().
This means you can get different results each time (if you want).
====SPLIT====
The copy_random_state function is a helper function that allows the user to copy the internal state of a random number generator.
It takes two arguments, random_state and force_copy. If force_copy is set to False (the default), then it will return the same
random number generator as was passed in, unless it's np.random and not forcing a copy. Otherwise, if force_copy is True, then 
it will always return a new random number generator with its own independent internal state.
====SPLIT====
The derive_random_states function is a helper function that returns a list of n
random states derived from the given random state. The returned list contains n 
random states, each one derived from the previous random state in the list. This 
is useful for deriving multiple seeds when seeding multiple independent operations.
====SPLIT====
The _quokka_normalize_extract function normalizes the input to this function,
i.e. it returns a BoundingBox object that describes the location of the quokka image in the image.
The resulting bounding box is guaranteed to be contained in an image of shape (643, 960, 3).
If 'extract' is already a BoundingBox this function does nothing and just returns 'extract'.
If 'extract' is a tuple then it will be converted into a BoundingBox object using e.g.:
====SPLIT====
The _compute_resized_shape function takes a shape tuple as input and returns another shape tuple.
The returned tuple is computed based on the provided `to_shape`, which can be:
====SPLIT====
The quokka function returns an image of a quokka as a numpy array.
It has the following options to change the output image:
* `size`: The minimum size of the output image. Is rounded to integer pixels.
    If None, then the output image is not resized.
* `extract`: A tuple (x, y, width, height) defining an extracted ROI from 
    the input file or input array representing this file (relative coordinates).
====SPLIT====
The quokka_segmentation_map function returns a segmentation map for the standard example quokka image.
The segmentation map is an 8-bit 2D array of shape (643, 960) and has values 0 where there are white pixels in the original image and 255 everywhere else.
The purpose of this function is to provide a means to make it easy to perform various transformations on that background while still retaining all keypoints.
====SPLIT====
The quokka_keypoints function returns a KeypointsOnImage object containing the
annotated keypoints of a single quokka image. The function extracts the annotations
from a JSON file and places the keypoints on top of the quokka image, optionally resizing it first.
The parameters are:
====SPLIT====
The quokka_bounding_boxes function returns bounding boxes for the quokka image.
====SPLIT====
The quokka_polygons function returns a list of polygons, each represented by 
a list of their vertices. The polygons are positioned on the image without any 
alignment to the image's aspect ratio (i.e. all polygons will be in an upright 
position). The returned coordinates are absolute pixel values from 0 to 639 for 
the width and 0 to 479 for the height.
====SPLIT====
The angle_between_vectors function returns the angle between two vectors.
====SPLIT====
The compute_line_intersection_point function computes the intersection point of two lines.
====SPLIT====
The draw_text function takes an image and a y, x coordinate pair. It then
draws text on the image at that point. The color of the text is green by
default but can be set to any RGB value. The default font size is 25, but can
be increased or decreased depending on your needs.
====SPLIT====
The imresize_single_image function ...
====SPLIT====
The compute_paddings_for_aspect_ratio function computes the amount of pixels that need to be added
to an image in order to change its aspect ratio (width/height) so that it matches a target aspect ratio.
It uses numpy indexing under the hood for fast computations.
The function supports batches of images, each with an arbitrary number of channels.
====SPLIT====
The pad_to_aspect_ratio function :
====SPLIT====
The pool function reduces the size of the input image by applying a function to local patches
of the image. The block_size parameter controls the size of this local patch and is specified as
a list with two or three elements: [height, width]. If only two elements are provided, then a square
patch (same height and width) is used.
====SPLIT====
The avg_pool function takes an array and a block size, and returns the average of all
the values in each block. The default value for cval is 0, but it can be set to any
value. If preserve_dtype is True (default), the output dtype will be that of arr; if False,
the output dtype will be np.float64.
====SPLIT====
The max_pool function takes an array and a block size, and returns the maximum value of each
block in the array.  The block size is a tuple that specifies the number of cells to include
in each block along each dimension.  For example, if arr has shape (10, 10) and we want blocks
of size (2, 2), then there will be 4 blocks: one with values at indices [0:2][0:2], another with values at [3:5][3:5], etc.
====SPLIT====
The draw_grid function takes a list of images as input and lays them out in a grid.
====SPLIT====
The show_grid function displays a grid of images.
====SPLIT====
The imshow function displays an image in a figure window.
====SPLIT====
The warn_deprecated function is a helper function that allows us to warn users
that they are using deprecated functionality.  It's stacklevel argument can be
used to ensure that the warning refers to the proper line of code, and not an
internal function call.
====SPLIT====
The HooksImages.is_activated function is a hook function that you can
overwrite in your own class (derived from the HooksImages class) which is
called by the activator function. It allows you to implement your own logic to
decide whether an augmentation block shall be applied or not. E.g. you could
implement logic to make sure that every 10th augmentation block is skipped (as
an example). This method has four parameters: images, augmenter, parents and default.
====SPLIT====
The HooksImages.postprocess function is a function that takes in an array of images and returns the same array
of images after some postprocessing has been done to it. This postprocessing can be anything, but it is most 
commonly used to convert the image from a numpy array back into a PIL Image or PyTorch Tensor. The HooksImages 
class comes with two built-in postprocessors: (i) ToPILImage - converts numpy arrays back into PIL Images; and (ii) 
ToTensor - converts PIL Images or numpy arrays into PyTorch Tensors.
====SPLIT====
The Pool.pool function is a wrapper around the Pool class which automatically
calls initialization and cleanup functions before and after the pool function.
The arguments of Pool.pool are identical to those of Pool().
====SPLIT====
The Pool.map_batches function is a wrapper around the Pool.map function that handles
batch ids instead of batch data. For each batch id, it calls the _Pool_starworker
method to get a worker to process that specific batch. It then returns all results in
a list, which preserves the order of batches and allows you to match up results with their
corresponding batches.
====SPLIT====
The Pool.map_batches_async function is a replacement for the Pool.map_async function,
which can be used to map over batches of items instead of individual items. The 'batches'
argument should be a list of lists (or tuples), where each sublist contains the indices
of the objects that are being mapped over in parallel. For example, if you want to compute
the average value for every pixel in an image using multiple processes on your computer's CPU, 
you could do something like this:
====SPLIT====
The Pool.imap_batches function is a generator that yields batches of samples.
It does not return them all at once, but rather in real-time as they are ready.
This means it can be used to process infinite batches of samples without ever running out of RAM or CPU time.
The function needs to know how many batches will be processed in total (in order to close the pool and join()).
This number is called 'num_batches' and can either be provided as parameter or inferred from the first batch yielded by 'batches'.
====SPLIT====
The Pool.imap_batches_unordered function is a generator that yields batches of samples.
It does not guarantee the order of the yielded batches, but it guarantees that only one batch will be computed at a time.
The implementation uses multiprocessing.Pool internally and therefore inherits all its limitations (i.e. no more than 
65k samples per batch are allowed).
====SPLIT====
The Pool.terminate function kills all the worker processes in the pool.
It is different from Pool.close() because it terminates all workers immediately,
even if they are currently processing jobs that will not be completed before termination.
====SPLIT====
The BatchLoader.terminate function is used to gracefully shut down the BatchLoader.
It does this by:
    * Checking if the main_worker_thread is alive, and if so, waiting for it to finish.
    * Giving minimal time for generated batches to be added to the queue and gracefully shut down.
    * If any workers are alive, they are terminated (see below).
====SPLIT====
The BackgroundAugmenter.get_batch function is a generator that yields batches of data from the source queue.
The BackgroundAugmenter will continue to generate batches until all workers have finished, after which a None value
will be yielded by the function. The function will return None if there are no more batches in the queue and all 
workers have finished.
====SPLIT====
The BackgroundAugmenter._augment_images_worker function is a helper function of the BackgroundAugmenter class.
It is not meant to be used manually. It runs in a background process and loads batches from the queue_source, augments images 
and puts them in the queue_result. The loading from disk and writing back to disc is done by this function alone, no 
threads or multiprocessing involved.
====SPLIT====
The BackgroundAugmenter.terminate function is used to terminate all the workers in the BackgroundAugmenter.workers list.
It does this by calling worker.terminate() on each worker in the list, and then waiting for them to finish their current task before terminating completely.
====SPLIT====
The UnnormalizedBatch.to_normalized_batch function is a helper function that
takes an UnnormalizedBatch instance and returns a Batch instance. The
UnnormalizedBatch class is used to store data before it has been normalized,
so this function normalizes the data in the batch and then returns it as a new
batch. This means that all of the augmentation functions can be applied to this
data without having to worry about whether or not each element in the batch has been normalized.
====SPLIT====
The Positive function is a wrapper around the ForceSign function.
It takes an input parameter and returns it, but with the sign forced to be positive.
The mode argument defaults to &quot;invert&quot;, which means that if the input parameter's sign is already positive, it will be returned as-is; if its sign is negative, then it will be inverted before being returned.
====SPLIT====
The Negative function takes a single parameter, other_param, and applies the
following logic to it:
====SPLIT====
The Polygon.area function computes the area of a polygon.
====SPLIT====
The Polygon.project function takes a Polygon and projects it onto a new line string.
The function returns the projected polygon as another Polygon object.
====SPLIT====
The Polygon.find_closest_point_index function finds the index of the point in a polygon that is closest to a given coordinate.
====SPLIT====
The Polygon.is_fully_within_image function checks if the polygon is fully within the image area.
====SPLIT====
The Polygon.is_partly_within_image function checks whether the polygon is at least partially inside the image area.
====SPLIT====
The Polygon.is_out_of_image function checks whether the polygon is
fully or partly outside of the image area. This function does not check
whether the polygon is actually inside of a binary mask (if such exists).
The function also works if the polygon lies completely within itself, i.e. it doesn't 
have to be an &quot;open&quot; and/or &quot;closed&quot; polygon in that case.
====SPLIT====
The Polygon.clip_out_of_image function -----------------------------------------------------------------------
Checks if the polygon is fully or partly outside of an image. If so, the polygon will be clipped so that it is completely inside of an image plane. This function returns a new Polygon object with adjusted coordinates. The original Polygon object remains unchanged.
====SPLIT====
The Polygon.extract_from_image function extracts the area of the image that is covered by the polygon.
It does so by calling Polygon.to_bounding_box, which returns a BoundingBox object corresponding to
the minimum bounding box that contains all points of the polygon. It then calls BoundingBox.extract_from_image,
which returns an image containing only those parts of the original image that are inside this bounding box.
====SPLIT====
The Polygon.change_first_point_by_coords function
====SPLIT====
The Polygon.change_first_point_by_index function takes a Polygon object and an integer index as input.
It returns a new Polygon object whose first point is the point in the original polygon at the given index,
and whose remaining points are unchanged.
====SPLIT====
The Polygon.to_shapely_polygon function converts the Polygon object to a shapely polygon.
This is useful for plotting the polygon using matplotlib.
====SPLIT====
The Polygon.to_shapely_line_string function converts the Polygon's exterior points to a Shapely LineString object.
The function can also convert the Polygon's interior rings, if any exist, to Shapely LineStrings as well.
The function takes two optional arguments: closed=False and interpolate=0.  The closed argument defaults to False, which means that no Shapely LinearRing objects will be created for the interior rings of the polygon (if there are any).  If you want a linear ring for each of the polygon's interior rings, set this argument equal to True.  The interpolate argument defaults to 0; it controls
====SPLIT====
The Polygon.to_bounding_box function converts a polygon to a bounding box.
The function takes no arguments and returns the bounding box of the polygon.
====SPLIT====
The Polygon.to_keypoints function converts the polygon's exterior points to a list of imgaug.augmentables.kps.Keypoint objects.
====SPLIT====
The Polygon.to_line_string function converts a polygon to a line string.
====SPLIT====
The Polygon.from_shapely function takes a Shapely Polygon object as input and returns
a list of ndarrays containing the coordinates of that polygon's exterior. This is useful
for converting Shapely Polygons to an image mask, as required by many segmentation/instance
segmentation methods.
====SPLIT====
The Polygon.exterior_almost_equals function checks whether the exterior of a polygon is almost equal to another
polygon. This is useful for checking if the coordinates of an already-extracted polygon are correct, or if they have
been computed by some algorithm. The function works as follows:
====SPLIT====
The Polygon.copy function creates a new Polygon object that is identical to the original.
The exterior of the new Polygon can be set to a list of points, and its label can be changed.
====SPLIT====
The Polygon.deepcopy function creates a new Polygon object with the same label and exterior as the original
Polygon. The exterior of the new Polygon is a copy of the old one, but it's not just a reference to it. This 
means that if you change any coordinates in either polygon, they will be changed in both.
====SPLIT====
The PolygonsOnImage.on function is a convenience function for copying polygons
to a new image size. It is equivalent to the following:
====SPLIT====
The PolygonsOnImage.draw_on_image function draws the polygons on an image.
It returns a new image with drawn polygons. The original image is not modified.
The draw operation depends on which parameters are set:
====SPLIT====
The PolygonsOnImage.remove_out_of_image function removes polygons from this object that are fully or partly outside of the image.
====SPLIT====
The PolygonsOnImage.clip_out_of_image function removes polygons from an image.
====SPLIT====
The PolygonsOnImage.deepcopy function creates a new PolygonsOnImage instance
with all its properties copied.
====SPLIT====
The PolygonsOnImage.deepcopy function creates a new PolygonsOnImage instance
with all its properties copied.
====SPLIT====
The MultiPolygon.from_shapely function takes a shapely.geometry.MultiPolygon or
shapely.geometry.Polygon and converts it into a MultiPolygon containing the Polygons
that make up the input geometry.
====SPLIT====
The SweepLine.get_intersections function returns a list of all the intersections that have occurred
during the sweep.  The returned list is sorted by x-coordinate, and each element in the list is a tuple
of (x, y) coordinates.
====SPLIT====
The _ABCTree.min_item function returns the key-value pair with the minimum key in the tree.
====SPLIT====
The _ABCTree.max_item function returns the key and value of the maximum item in a tree.
====SPLIT====
The _ABCTree.prev_key function returns the key of the previous item in
the tree, or None if there is no such item.  If default is given, it will be
returned if no previous item exists.  Otherwise a KeyError will be raised.
====SPLIT====
The _ABCTree.succ_key function returns the key of the successor to a given key.
If no such successor exists, it returns ``default`` (which defaults to None).
====SPLIT====
The OpenSimplex.noise2d function maps the coordinates (x, y) to a value in the interval [-2.0, 2.0].
The mapping is defined by four corners of a rectangle: S(+/- 1, +/- 1), and T(+/- 0.5, +/- 0.5).
The gradient points toward T from S: dS/dx = +Tdy; dS/dy = -Txdy; dT/dx = +Sy-ys; and dT/dy = -Sxsy.
====SPLIT====
The InColorspace function is a wrapper around the WithColorspace function. It executes the child
augmenter(s) in a modified colorspace where only one of the three channels (usually alpha channel) is
non-zero and all others are zero. The non-modified channels remain unchanged, e.g. R=G=B=128 stays 128
regardless of what to_colorspace is set to.
====SPLIT====
The Grayscale function is a convenience function that converts an image to grayscale.
It uses the CIE Luv colorspace to convert the image's colorspace from RGB/BGR/Lab/HLS
to gray. It requires matplotlib and numpy (installable via pip). The alpha channel is
preserved during this transformation.
====SPLIT====
The LineString.height function returns the height of the LineString object.
====SPLIT====
The LineString.width function returns the width of a LineString object.
====SPLIT====
The LineString.get_pointwise_inside_image_mask function returns a boolean array of the same shape as the LineString's coordinates,
with True values at each coordinate point that is inside the image plane and False values for those outside.
====SPLIT====
The LineString.compute_neighbour_distances function computes the distance between each pair of consecutive points in a LineString object.
====SPLIT====
The LineString.compute_pointwise_distances function computes the distance from each point in a LineString to some other geometric object.
====SPLIT====
The LineString.compute_distance function computes the minimum distance between two LineStrings.
It does this by computing the distance pointwise, and then returning the minimum value.
====SPLIT====
The LineString.project function takes a LineString object and two shapely shapes,
and returns a new LineString object with the coordinates projected to the new coordinate system.
====SPLIT====
The LineString.is_fully_within_image function checks whether the LineString is fully inside the image area.
It does so by checking if all of its points are within the image area.
The default value for this function is False, which means that a LineString will not be considered as being fully 
inside an image if one or more of its points are outside of it.
====SPLIT====
The LineString.is_partly_within_image function checks whether the LineString is at least partially inside an image plane.
This can be checked by looking for intersections between the LineString and all four sides of the image plane.
If any of these intersection points are inside (or on) the image, then it returns True, otherwise False.
====SPLIT====
The LineString.find_intersections_with function finds all intersections between the LineString object and another geometry.
====SPLIT====
The LineString.draw_mask function returns a heatmap-like numpy array of the same size as
the input image, where all pixels outside of the polygon are black and all pixels inside
of the polygon are white. This is similar to LineString.draw_lines_heatmap_array, but with
a different alpha value for each pixel.
====SPLIT====
The LineString.draw_lines_heatmap_array function creates a heatmap array from the line string.
The shape of the output array is (H,W) and each pixel denotes how much does it overlap with the line.
====SPLIT====
The LineString.draw_points_heatmap_array function draws the line string on a heatmap array.
====SPLIT====
The LineString.draw_heatmap_array function creates a heatmap array from the line string.
The heatmap is drawn over an image whose shape is image_shape.
The size of the lines and points can be specified using parameters alpha_lines, alpha_points, size_lines and size_points.
If antialiased = True then the edges of the lines will be antialiased in order to make them look smoother.
====SPLIT====
The LineString.draw_points_on_image function draws all points on the image.
====SPLIT====
The LineString.draw_on_image function draws the line string on an image.
It will apply its color and thickness to the image, but any other values of
the image's ``array`` attribute (e.g. brightness, contrast, etc.) will not be
modified by this function.
====SPLIT====
The LineString.extract_from_image function extracts a rectangular sub-region from the given image,
where the line string is contained in or intersects with. The rectangle will be large enough to contain
the whole line string and have an aspect ratio close to 1.0 (i.e. it may be wider or taller than necessary).
The extraction happens by multiplying the line string coordinates with heatmap alpha values, which are then used as alpha channel for a white image of shape ``(H,W,[C])`` where ``H`` is the height of the extracted region and ``W`` its width; C is set to zero if there is no color channel
====SPLIT====
The LineString.concatenate function concatenates two LineStrings into a single LineString.
It does this by creating a new array of coordinates that is the result of appending the second
LineString's coordinates to the first LineString's coordinates.
====SPLIT====
The LineString.subdivide function takes a LineString object and returns a new
LineString object with more points. The number of points in the new LineString is
given by the argument `points_per_edge`. If `points_per_edge` is 1, then you get
back the same LineString. If `points_per_edge` is 2, then you get back the line
string with twice as many points (but with no change to shape or length). And so on.
====SPLIT====
The LineString.to_keypoints function converts the LineString object into a list of Keypoint objects.
The Keypoint class is part of imgaug, not scikit-image.
====SPLIT====
The LineString.to_bounding_box function converts a LineString to a BoundingBox.
====SPLIT====
The LineString.to_polygon function converts a LineString object into a Polygon object.
The function takes no arguments and returns the converted Polygon object.
====SPLIT====
The LineString.to_heatmap function converts a LineString to a heatmap array.
====SPLIT====
The LineString.to_segmentation_map function takes an image shape and returns a segmentation map,
i.e. a numpy array with one channel where each pixel denotes the line id (0 = no line, 1 = first
line, 2 = second line etc.). All lines will be drawn inside their respective ids. The function
can also take several optional arguments:
====SPLIT====
The LineString.coords_almost_equals function is a helper function that allows us to compare two LineStrings
with each other. It takes another LineString as input and returns True if the distance between every pair of points
in the two LineStrings is less than or equal to some specified tolerance value (default 1e-6). If one of the input 
LineStrings has no coordinates, it will return False.
====SPLIT====
The LineString.almost_equals function is a helper function that determines whether two LineStrings are
almost equal. It does this by comparing the coordinates of each vertex in the LineString to one another, and 
returning True if they are all within a certain distance of one another. The default distance is 1e-4, which 
is equivalent to four ten-thousandths of a decimal place (or 0.0004). This can be changed by passing in an 
argument for max_distance.
====SPLIT====
The LineString.copy function creates a new LineString object with the same coordinates and label as the original.
If no parameters are given, it returns a copy of itself.
====SPLIT====
The LineStringsOnImage.draw_on_image function is a wrapper around the
LineString.draw_on_image function: it receives an image and calls that
functions for each LineString object on the LineStringsOnImage. The other parameters are passed through to each call
to LineString.draw_on_image, so see there for more details.
====SPLIT====
The LineStringsOnImage.clip_out_of_image function clips all LineString
instances of this object to fit within the image with the given shape.
LineStrings that are entirely outside of the image will be removed. The
result is a new LineStringsOnImage object without any LineString instances,
which is then free to contain only those LineStrings which still intersect with the image plane after clipping. This function can be used to crop a label mask by removing all pixels/voxels which lie outside of the physical extent of their origin annotation(s).
====SPLIT====
The LineStringsOnImage.copy function copies the LineStringsOnImage object.
====SPLIT====
The LineStringsOnImage.deepcopy function creates a deep copy of the LineStringsOnImage object.
====SPLIT====
The blend_alpha function takes in two images, and produces a single output image using alpha blending.
The alpha parameter is the weight that will be given to the foreground image when it is blended with the background.
This function can blend images of different sizes, as long as they are at least 2x2 in size (smaller than this will not work).
It also works on both color and grayscale images; if a grayscale image is provided as input, then it will return a 
grayscale output. The inputted images must have the same number of channels (i.e., must both be color or both
====SPLIT====
The SimplexNoiseAlpha function is a wrapper around SimplexNoise and AlphaElementwise. It returns an image
similar to the one returned by SimplexNoise, with some values modified by an alpha layer. The alpha layer is
modeled after the simplex noise (the brightness variations) and uses a sigmoid function to smoothly transition
between 0 and 1.
====SPLIT====
The OneOf function is a wrapper around the SomeOf function. It allows you to specify
a single child for each image in the batch, and will randomly choose one of those children
to return for every image in the batch. This allows you to specify a distribution over augmenters.
====SPLIT====
The AssertLambda function is a wrapper around the Lambda function. It allows you to define an
assertion for input data, executed before and after the Lambda function. The assertion can be
performed on images, heatmaps, keypoints or polygons (provided via the respective lambda-functions).
If any of them is set to None, then that assertion will not be performed by AssertLambda.
The purpose of this class is to prevent bugs in neural network layers from silently existing in your codebase.
An example use case would be: You want to verify that images provided as input during training always have a
====SPLIT====
The MotionBlur function applies a motion blur to the input images.
====SPLIT====
The Clouds function creates clouds by combining two cloud layers.
====SPLIT====
The Fog function creates an image that looks like a watercolor painting of a foggy landscape.
It is based on the CloudLayer function and uses similar parameters to control the clouds.
The main difference is that Fog adds small white dots to the image which make it look like foggy mist.
This effect can be achieved by using smaller alpha values than in CloudLayer and by increasing sparsity which controls how many white dots will be added to the image.
====SPLIT====
The Snowflakes function adds falling snowflakes to images.
====SPLIT====
The SegmentationMapOnImage.draw function takes a segmentation map and draws it on an image.
It returns the augmented image.
The input segmentation map is expected to have encoded classes as different integers
(like cv2.imread would read an image file in integer format). The values of the generated segmap are not important, only the classes are used here.
The background class id can be set via SegmentationMapOnImage(..., class_id=X). If not set, 0 will be used as background class id (same behaviour as cv2.imread).
====SPLIT====
The SegmentationMapOnImage.draw_on_image function takes an image and draws on top of it.
It returns a new image with the drawn content. The input image is not modified.
The segmentation map object given to the function must have a 'get_arr()' method.
====SPLIT====
The SegmentationMapOnImage.pad_to_aspect_ratio function ----------------------------------------------------------------------------------------------------
====SPLIT====
The SegmentationMapOnImage.resize function resizes the segmentation map array.
====SPLIT====
The SegmentationMapOnImage.to_heatmaps function converts a SegmentationMapOnImage object to a HeatmapsOnImage object.
====SPLIT====
The SegmentationMapOnImage.from_heatmaps function creates a SegmentationMapOnImage object from one or more heatmaps.
The heatmap(s) may be provided as HeatmapsOnImage objects, numpy arrays or tuples of (H,W).
The class_indices parameter can be used to map the values in each heatmap to classes. This is useful if you are providing multiple
heatmaps that each represent different segmentations for the same set of classes and you wish to map them all into one SegmentationMapOnImage.
====SPLIT====
The SegmentationMapOnImage.deepcopy function creates a deep copy of the segmentation map object.
This means that not only the array is copied but also its class instance variables like shape and nb_classes.
====SPLIT====
The EventQueue.offer function adds an event to the queue.
If the event is of type END, it is added to a list at the beginning of
the queue. Otherwise, it is added to a list at the end of the queue.
====SPLIT====
The HeatmapsOnImage.draw function draws the heatmaps onto a given image.
====SPLIT====
The HeatmapsOnImage.draw_on_image function draws the heatmaps on an image.
It is a wrapper around HeatmapsOnImage.draw() and simply calls it multiple times, once per heatmap.
====SPLIT====
The HeatmapsOnImage.invert function inverts the heatmap(s) that are already
present on the (already inverted) HeatmapsOnImage object. This can be useful
when you created a HeatmapsOnImage object via HeatmapsOnImage.from_0to255()
with mode=&quot;nearest&quot; and inverted=False, performed some operations involving
randomization or interpolation, and want to invert the heatmap(s) again so that it is back in its original state.
====SPLIT====
The HeatmapsOnImage.pad_to_aspect_ratio function is a helper function that can be called
by the __init__ method of the HeatmapsOnImage class. It allows for easier padding of heatmaps to
an aspect ratio without cropping, as this operation can be awkward and confusing due to the fact that
the heatmap's array is not exactly one pixel tall and wide. The HeatmapsOnImage object itself does not contain any info on where it was generated from, e.g. which sizes were used for padding etc., but these details are available in its &quot;arr_0toX&quot; attribute (which represents 0-to-X image before padding).
====SPLIT====
The HeatmapsOnImage.to_uint8 function converts the heatmap to a 0-to-255
8-bit numpy array (dtype is np.uint8). This is useful if a pre-trained
neural network requires this format for its input image data.
====SPLIT====
The HeatmapsOnImage.from_uint8 function creates a HeatmapsOnImage object from an array of heatmap values
of shape (height, width, channel) where the last channel is interpreted as being the heatmap value.
The resulting HeatmapsOnImage object will have its max_value property set to 1.0 and its min_value property set to 0.0.
====SPLIT====
The HeatmapsOnImage.from_0to1 function is a convenience function for
converting heatmaps that are in the range [0.0, 1.0] to a new HeatmapsOnImage
object with min_value and max_value set to 0.0 and 1.0 respectively.
====SPLIT====
The HeatmapsOnImage.change_normalization function modifies heatmap values to be
relative to the image's maximum value instead of 1.0. This is used
after a heatmap was generated on an image with normalization (i.e., having
pixel values in [0, 255]) and you wish to denormalize the heatmaps such that
they now have pixel values in [0, 1].
====SPLIT====
The HeatmapsOnImage.deepcopy function creates a deep copy of the HeatmapsOnImage object.
This means that all data is copied and the new object does not share any mutable numpy arrays with
the original. This is important for threading or other parallel applications where objects may be
otherwise altered during processing.
====SPLIT====
The MutableHeaders.setdefault function sets the value of a key if that key does not exist in the headers.
If it does exist, then it returns the existing value for that key.
====SPLIT====
The MutableHeaders.append function appends a key-value pair to the MutableHeaders object.
The function takes two arguments:
    - key: The string that will be used as the key in the dictionary. This argument is required and must be of type str.
    - value: The string that will be used as the value in the dictionary. This argument is also required and must also 
            be of type str.
====SPLIT====
The BaseSchemaGenerator.parse_docstring function parses the docstring of a function or method.
It returns a dictionary with the parsed information, which is then used to generate an OpenAPI schema.
The docstring should be in YAML format and contain some key-value pairs that describe how to use 
the function or method.
====SPLIT====
The StaticFiles.get_directories function returns a list of directories that contain static files.
====SPLIT====
The StaticFiles.get_response function is a coroutine that accepts an incoming request and returns a response.
It does this by first calling the StaticFiles.lookup_path function, which looks up the path in our static files directory 
and returns the full path to any file found there as well as its stat result (if it exists). If no file is found, 
lookup_path will return None for both values of full_path and stat_result. If we do have a file to serve, we call 
StaticFiles.file_response with those values to get back a Response object that can be returned from our app
====SPLIT====
The StaticFiles.check_config function checks to see if the StaticFiles directory exists.
If it does not exist, then an error is raised. If it does exist, then nothing happens.
====SPLIT====
The StaticFiles.is_not_modified function checks to see if the request headers contain an &quot;if-none-match&quot; or 
&quot;if-modified-since&quot; key. If they do, it compares their values to the response headers' &quot;etag&quot; and 
last_modified&quot;. If either of these values match, then is_not_modified returns True. Otherwise, it returns False.
====SPLIT====
The build_environ function builds an environ dict from the scope and body.
====SPLIT====
The WebSocket.receive function accepts a WebSocket message and returns it.
If the client is still connecting, then the function will return a &quot;websocket.connect&quot;
message; if it has disconnected, then it will return a &quot;websocket.disconnect&quot; message.
====SPLIT====
The WebSocket.send function accepts a message object, which is expected to have
a type key and value. The value of the type key should be one of the following:
websocket.accept, websocket.close, or websocket.send.
====SPLIT====
The get_top_long_short_abs function returns the top 10 positions with the largest long and short exposure,
as well as absolute exposure. The function accepts a single positional argument, which is a dataframe of 
positions. It also has default values for the number of positions returned for each type (long/short). 
The function returns three dataframes: one containing the top long positions, one containing the top short 
positions and one containing all other positions.
====SPLIT====
The get_max_median_position_concentration function returns a dataframe that contains the following columns:
max_long, median_long, median_short and max_short. The max long column contains the maximum allocation to long positions for each asset in our portfolio. The median long column contains the 50% allocation to long positions for each asset in our portfolio. The median short column contains the 50% allocation to short positions for each asset in our portfolio and finally,the max short colummn conatins the minimum position size that we will hold on any given security (in percentage terms).
====SPLIT====
The get_long_short_pos function returns a DataFrame containing the long and short exposure of each asset.
The net liquidation is the sum of all long and short positions, including cash.
====SPLIT====
The compute_style_factor_exposures function computes the style factor exposures of a portfolio.
====SPLIT====
The plot_style_factor_exposures function plots the total exposure to a style factor (e.g., momentum)
over time. The function accepts as input a dataframe of style exposures, and an optional name for the 
style factor being plotted. If no name is provided, then the function will use the column name from 
the dataframe as its label.
====SPLIT====
The compute_sector_exposures function computes the sector exposures of a portfolio.
====SPLIT====
The plot_sector_exposures_longshort function plots the long and short exposures to sectors.
It takes in a list of sector names, a DataFrame containing the long exposure to each sector, 
a DataFrame containing the short exposure to each sector, and an axis object (optional). 
The function returns an axis object with both long and short exposures plotted on it.
====SPLIT====
The plot_sector_exposures_gross function creates a stacked area plot of the gross exposure to sectors.
The function takes in a list of DataFrames, each containing the sector exposures for one portfolio.
It also takes in an optional dictionary mapping names to sectors (if not provided, uses SECTORS).
It returns an Axes object.
====SPLIT====
The plot_sector_exposures_net function plots the net exposures to sectors.
It takes as input a list of DataFrames, where each DataFrame contains the 
exposures to a sector for one month. The function also takes an optional 
input called sector_dict which is used in case you want to plot specific names 
for the sectors instead of just plotting numbers from 1-11 (the default). It also has an optional input called ax that allows you to pass in an Axes object if you would like to add more elements or modify the existing plot. The function returns an Axes object with your visualization.
====SPLIT====
The compute_cap_exposures function computes the following:
====SPLIT====
The plot_cap_exposures_net function plots the net exposure to market cap buckets.
It takes as input a list of lists, where each sublist is the exposures for one bucket.
The function also accepts an optional matplotlib axis object, which it uses to plot 
the data on.
====SPLIT====
The compute_volume_exposures function computes the following:
====SPLIT====
The create_full_tear_sheet function combines a number of plots into a single
tear sheet. It includes returns, positions, and transactions (with
roundtrips). This function is used to help create full tear sheets of
interesting cases for the user.
====SPLIT====
The create_position_tear_sheet function creates a tear sheet of the positions 
over time. This is intended to be run after the backtest and assumes that you have 
a DataFrame of positions and a Pandas Series of Returns called returns in your algorithm.
====SPLIT====
The create_txn_tear_sheet function creates a tear sheet of the following plots:
- turnover plot
- daily volume plot
- histogram of daily turnover rates (time weighted) 
    - this is a distribution of how much each equity was traded over time, 
      normalized by total shares traded for that day.  This helps us understand how many days each equity was traded.
====SPLIT====
The create_capacity_tear_sheet function ...
====SPLIT====
The create_perf_attrib_tear_sheet function creates a tear sheet of performance attribution 
given a returns DataFrame and the corresponding factor loadings, common factor returns, and 
transactions (if any). It displays these in four sections: 1) Cumulative Returns Over Time, 2) 
Performance Attribution Summary Statistics by Quantile, 3) The Contribution of Each Specific 
Return Factor to Performance Attributions by Quantile Grouping (if applicable), 4) The Total Risk 
Exposure of Each Specific Factor Exposures. This function is used for both single asset analysis as well as multi-asset analysis.
====SPLIT====
The daily_txns_with_bar_data function takes in two dataframes: transactions and market_data. 
It then creates a new dataframe called txn_daily that contains the sum of all transaction amounts per symbol per day. 
The amount column is set to the absolute value of the amount column, so that negative values are eliminated. The price and volume columns are taken from market_data, which was created by taking daily bars from IEX for each symbol in our list of symbols (called tickers). We then reset the index on txn_daily so it's now indexed by date instead of by symbol.
====SPLIT====
The days_to_liquidate_positions function takes in a portfolio of positions and returns the number of days required to fully liquidate each position. The function uses the following inputs:
    - positions: A dataframe containing daily position values for multiple assets.
    - market_data: A dataframe containing daily market_data for one or more assets. This is used to compute how much volume can be consumed by a given asset at any given time step (see max_bar_consumption).
    - max_bar_consumption: The maximum amount of volume that can be consumed by an asset in a single day (default value = 0.2,
====SPLIT====
The get_low_liquidity_transactions function returns a DataFrame containing the date, symbol, and 
percentage of the daily bar consumed by single transactions on that day. The function takes two arguments:
transactions - A DataFrame of transaction data returned from get_txn_vol()
market_data - A DataFrame of market data returned from get_pricing()
====SPLIT====
The apply_slippage_penalty function computes a slippage penalty for each transaction.
The penalty is equal to:
    1) 10% of the dollar value of the trade times 
    2) The square root of the percentage volume traded on that day.
====SPLIT====
The map_transaction function takes a transaction dictionary as an input and returns a new dictionary with the following keys: sid, symbol, price, order_id, amount (number of shares), commission (total commission paid for this transaction), dt.
====SPLIT====
The make_transaction_frame function takes a DataFrame of transactions and returns a DataFrame with the following columns:
====SPLIT====
The get_txn_vol function computes the daily total value of shares and dollars
transacted in a portfolio. The function accepts a DataFrame containing one row
per symbol per day, with columns for 'amount' (the number of shares transacted)
and 'price' (the price per share). It returns two Series objects: the first, 
txn_volume, contains the value transacted per day; the second, txn_shares contains 
the number of shares transacted per day. Both Series have a DateTimeIndex containing 
all dates in which there were transactions.
====SPLIT====
The adjust_returns_for_slippage function adjusts returns for slippage.
====SPLIT====
The get_turnover function computes the turnover of a portfolio.
Turnover is defined as the percentage change in positions, scaled by average gross book value (AGB) of all portfolios.
The denominator can be either AGB or portfolio_value, and it defaults to AGB.
====SPLIT====
The _groupby_consecutive function takes a DataFrame of transactions and returns a DataFrame with the following characteristics:
- The index is the datetime at which the trade occurred.
- The columns are 'symbol', 'amount', 'price' (the average price of all orders in that block), and 'vwap' (the volume weighted average price).
====SPLIT====
The extract_round_trips function is a helper function that takes in the transactions dataframe and portfolio value dataframe as arguments. It then groups the transactions by symbol, sorts them by date, creates a new column for signed price (price * amount) and another column for absolute amount. Then it iterates through each row of the transaction dataframe to see if there are any negative prices or negative amounts which would indicate an open position or closed position respectively. If so, it adds these values to their respective stacks until either a positive price or positive amount is encountered at which point it closes out all positions with that stock on that day and calculates pnl based on those
====SPLIT====
The add_closing_transactions function adds closing transactions to the transactions DataFrame.
Closing transactions are necessary for round-trips, as they correct for the number of shares held at 
the end of a time period. This function is called by add_closing_transactions in perf/__init__.py
====SPLIT====
The apply_sector_mappings_to_round_trips function takes in a DataFrame of round trips and a dictionary mapping
symbols to sectors. It returns the same DataFrame with the symbol column replaced by the sector specified in 
the dictionary.
====SPLIT====
The gen_round_trip_stats function generates a dictionary of statistics for the round trips.
The function accepts a dataframe of round trips as an input and returns a dictionary with 
the following keys: pnl, summary, duration, returns and symbols. The pnl key holds another 
dictionary with keys for all long and short trades that make up the round trip strategy's P&amp;L.  
Each key has subkeys for mean, max drawdown (in percentage terms), standard deviation (in absolute terms) 
and sharpe ratio (returns relative to volatility). The summary key holds another dictionary with keys 
for total number of
====SPLIT====
The print_round_trip_stats function prints out the statistics of a set of round trips.
It takes as input a DataFrame containing all the transactions from an event study, and 
prints out several tables that describe the performance of each individual trade. The 
tables are: summary stats, PnL stats, duration stats, return stats and symbol-wise 
stats.
====SPLIT====
The perf_attrib function computes several performance metrics, including:
====SPLIT====
The compute_exposures function computes the exposures of a set of positions to a factor.
The function takes as input:
- A DataFrame containing the positions for each day in the backtest (the index contains 
  dates and there is one column per ticker). The values represent either absolute or 
  relative position values, depending on whether pos_in_dollars is True or False. If it's false, then we assume that all assets are dollar-denominated and divide by 100 to get their percentage equivalents. This allows us to treat all assets as having an equal weight within a single portfolio. We refer to these
====SPLIT====
The create_perf_attrib_stats function creates a summary of performance attribution statistics.
It takes as input the output from create_perf_attrib function and returns a series of 
statistics describing the performance attribution results. The first statistic is annualized specific return, 
which is calculated by taking the total return over the period divided by number of years in that period (daily). 
The second statistic is annualized common return, which is calculated using total returns minus specific returns.  
The third statistic returned in annualized total return, which represents cumulative daily returns over time horizon.
====SPLIT====
The show_perf_attrib_stats function takes in a DataFrame of returns, positions, factor returns and factor loadings
and displays the following performance metrics:
- Annualized Specific Return (Annualized Return - Risk Free Rate)
- Annualized Common Return (Annualized Total Return - Risk Free Rate)
- Annualzied Total Return 
- Specific Sharpe Ratio = Average Specific Returns / Standard Deviation of Specific Returns
====SPLIT====
The plot_returns function creates a plot of the cumulative returns, 
the cumulative specific returns and the cumulative common returns.
It also plots the cost if it is provided.
====SPLIT====
The plot_factor_contribution_to_perf function creates a plot of the cumulative returns
for each factor in the dataframe passed to it. The function also includes horizontal lines 
at zero, and shows how each factor contributes to performance. This is useful for visualizing 
the results of attribution calculations.
====SPLIT====
The _stack_positions function converts the positions DataFrame from wide to long format.
It also removes cash from the portfolio and converts dollar values to percentages.
====SPLIT====
The _cumulative_returns_less_costs function computes the cumulative returns of a strategy, less its costs.
====SPLIT====
The format_asset function converts a Zipline Asset object into its corresponding
symbol name. This is useful for DataFrames that have an Asset column, as the
column can be mapped to the symbol name instead of its unique integer value. For
example:
====SPLIT====
The vectorize function is a decorator that takes in a function and returns
a vectorized version of the function. The vectorized version of the function 
can take in one or more pandas Series objects (or numpy arrays) and will return 
a pandas Series object (or numpy array). This allows us to apply our functions 
to entire columns or rows within dataframes.
====SPLIT====
The print_table function prints out a table from a Pandas DataFrame.
====SPLIT====
The detect_intraday function detects whether or not an algorithm that trades
in-the-money options has significant in-the-money daily transactions. The function
returns True if the ratio of in-the money daily option transactions to total 
option transactions is less than a user specified threshold. This indicates that 
trading occurred during the day, and should be further investigated.
====SPLIT====
The check_intraday function is used to determine whether the strategy is intraday. If it is, then we need to use a different metric for estimating positions and generating transaction costs.
====SPLIT====
The estimate_intraday function computes the effect of intraday transactions on account equity.
====SPLIT====
The clip_returns_to_benchmark function takes in two returns series, rets and benchmark_rets.
It then clips the first return series to start on the same date as the second return series.
This is because TensorTrade's backtest assumes that all assets begin trading at the start of our 
benchmark data (in this case, JSE Listed Property ETFs). This function allows us to clip a 
returns timeseries so it begins on or after a given date.
====SPLIT====
The to_utc function converts a DataFrame's index from the local timezone to UTC.
====SPLIT====
The get_symbol_rets function returns a Series of asset returns given a symbol.
====SPLIT====
The sample_colormap function creates a list of n_samples colors sampled from the colormap specified by cmap_name.
====SPLIT====
The customize function is a decorator that allows the user to control
the context of the plot. The following parameters are available:
    set_context=True (default) : use rcParams from seaborn.plotting_context()
    set_context=False : use rcParams from matplotlib.rcsetup.all_backends()
====SPLIT====
The plotting_context function is a wrapper around the seaborn plotting context.
It sets up the default matplotlib rc parameters to be consistent with a particular style,
and it also adjusts those defaults for whatever plotting context is specified. 
For example, if you set the 'talk' context, then you will get larger text and lines that are appropriate for slide presentations.
====SPLIT====
The axes_style function is a convenience function that allows one to
quickly set the style of the current figure. The following styles are
supported:
====SPLIT====
The plot_monthly_returns_heatmap function creates a heatmap of the monthly returns of the strategy.
It takes in an optional ax, which is defaulted to None. It also takes in any other keyword arguments that 
matplotlib accepts for its functions.
====SPLIT====
The plot_annual_returns function creates a bar graph of the returns for each year.
It also plots a horizontal line that indicates the mean return across all years.
The plot_annual_returns function has one parameter, which is an instance of pd.Series containing daily returns.
====SPLIT====
The plot_monthly_returns_dist function creates a histogram of the distribution of monthly returns.
It uses the aggregate_returns function to calculate monthly returns from daily returns, and then plots
the results using matplotlib's hist function. The mean is also plotted on the graph in gold, with a dashed line 
representing zero.
====SPLIT====
The plot_holdings function creates a plot of the number of shares held each day.
It takes in three arguments: returns, positions, and legend_loc. The default value for legend_loc is 'best'.
The function then uses the matplotlib library to create a line graph with two lines: one representing daily holdings and 
the other representing average monthly holdings. It also includes an average daily holdings line over all months.
====SPLIT====
The plot_long_short_holdings function plots the number of long and short holdings for a strategy.
It takes in a returns object, positions object, and an optional ax parameter.
The function then creates two DataFrames: df_longs and df_shorts. 
These contain the number of longs held at each time step (df_longs) 
and the number of shorts held at each time step (df_shorts). The function then uses fill between to create two graphs: one for long holdings, one for short holdings.
====SPLIT====
The plot_drawdown_periods function creates a plot of the top 10 drawdown periods.
====SPLIT====
The plot_drawdown_underwater function creates an underwater plot for a given time period.
Underwater plots are commonly used to identify periods of drawdown (i.e., when the price of an asset is below its 
maximum historical value) for a particular investment. The y-axis represents the percentage drawdown, while the x-axis 
represents time.
====SPLIT====
The plot_perf_stats function creates a boxplot of the performance statistics
of the returns and factor returns.  The bootstrap_values are extracted from
the perf_stats_bootstrap function, which computes various statistics for each column in the dataframe.  
The boxplot is created using seaborn's boxplot function, which takes as input a pandas DataFrame or 2D numpy array.  
The orient parameter is set to 'h' so that we can view our dataframe more easily.
====SPLIT====
The show_perf_stats function prints out a table of performance metrics for the backtest.
It uses the timeseries.perf_stats function to calculate the desired statistics, and then it prints them out in a table using utils.print_table:
====SPLIT====
The plot_returns function creates a plot of the returns for the strategy's backtest.
It accepts two parameters, `returns` and `live_start_date`, with live_start_date being optional.
If live_start_date is provided, it will also show the performance of the backtest since that date.
====SPLIT====
The plot_rolling_returns function creates a plot of the cumulative returns 
over time. The x-axis shows the date and y-axis shows the cumulative returns. 
The function accepts two parameters, which are returned_series and factor_returns.
====SPLIT====
The plot_rolling_beta function creates a chart that depicts the rolling 6 and 12 month beta versus time.
It takes in a timeseries of returns, factor_returns, and legend_loc as parameters.
The function then plots the rolling 6 and 12 month beta versus time on an axis.
====SPLIT====
The plot_rolling_volatility function creates a plot of the rolling volatility 
of a strategy's returns when compared to that of a benchmark. The function takes 
as input two timeseries: one for returns and another for the benchmark's returns. 
The user can specify the lookback window, or number of months, which is used to compute 
the rolling volatility. The default value is 6 months.
====SPLIT====
The plot_rolling_sharpe function creates a plot of the rolling Sharpe ratio versus date.
It is useful for analyzing the performance of a strategy. A Sharpe ratio of 1 or higher
indicates a better-than-average return, while a ratio less than one indicates an average 
or worse return. The function also plots the average (sharpe) line which demarcates 
the boundary between above and below average returns.
====SPLIT====
The plot_gross_leverage function creates a plot of gross leverage versus time.
====SPLIT====
The plot_exposures function plots the long, short, and net exposures of a
given portfolio. The function accepts three parameters: returns (a pandas Series
of portfolio returns), positions (a pandas DataFrame representing the dollar
amount of each position held at each time), and ax (an Axes object to plot on). 
The function creates an overlayed area chart showing the exposure of each 
position as well as a line plot showing the overall exposure. The function assumes that 'cash' is not a position in your portfolio.
====SPLIT====
The plot_max_median_position_concentration function produces a chart of the 
maximum and median long and short positions by sector. The maximum position concentration 
is defined as the highest single position's concentration relative to total exposures in that sector. 
The median position concentration is defined as the median long/short exposure of all holdings in a given sector.
====SPLIT====
The plot_sector_allocations function creates a plot of the sector allocations over time.
It takes in a returns series, and sector allocation dataframe as inputs.
The function also accepts an axis object to be used for plotting, but if no axis is provided then one will be created. 
The function also has keyword arguments that can be passed through to the plot method of the pandas DataFrame class.
====SPLIT====
The plot_return_quantiles function creates a boxplot of the returns dataframe.
It takes in a parameter for the returns dataframe, and an optional live_start_date 
which is just a string that determines when to start plotting out-of-sample 
returns. It also accepts an ax which is just the variable name for matplotlib's 
axis object, and any other keyword arguments that sns.boxplot() can take.
====SPLIT====
The plot_turnover function produces a chart of turnover vs. date.
====SPLIT====
The plot_slippage_sweep function creates a plot of the cumulative returns given an initial investment,
and plots the cumulative returns for multiple values of slippage. 
The plot_slippage_sweep function has one required parameter, and five optional parameters: 
returns (the pandas DataFrame returned by get_returns), positions (the pandas DataFrame returned by create_txn_data), 
transactions (a pandas Series with share transactions as values and date indices), ax (a matplotlib Axes object to attach the plot to) , 
slippageParams(an array containing various
====SPLIT====
The plot_slippage_sensitivity function creates a plot of the average annual returns given additional per-dollar slippage.
The function accepts four parameters: returns, positions, transactions and ax. The default value for ax is None which means that matplotlib will use plt.gca() to grab the current axes of the default figure.
====SPLIT====
The plot_daily_turnover_hist function creates a histogram of the daily turnover rates.
It takes in positions and transactions as arguments, and returns an axis object.
====SPLIT====
The plot_daily_volume function creates a plot of the daily trading volume for each day in the backtest period.
It also plots a horizontal line that indicates the average daily trading volume during this period.
The function accepts two arguments: returns and transactions, both pandas Series objects. 
Returns is used to determine when and how much to buy/sell, while transactions records all of these trades.
====SPLIT====
The plot_txn_time_hist function creates a histogram of the time of day that
transactions occur. The x-axis is broken into bins corresponding to five minute
intervals and the y-axis shows the proportion of transactions in each bin. This
plot can be used to visualize if there are any patterns in when trades occur.
====SPLIT====
The show_worst_drawdown_periods function displays the 'worst' drawdown periods.
====SPLIT====
The plot_monthly_returns_timeseries function creates a timeseries plot of the returns dataframe.
It takes in the returns dataframe as an argument and plots it on a matplotlib figure. It also uses 
seaborn to create a barplot of the monthly returns, which is then plotted on top of the timeseries plot. 
The function also adds horizontal lines for yearly boundaries and zero return values.
====SPLIT====
The plot_round_trip_lifetimes function creates a plot of the round trip durations for each symbol.
It is called by the analyze function and takes no parameters. The plot_round_trip_lifetimes function 
returns an axis object.
====SPLIT====
The show_profit_attribution function displays the profitability of each traded name.
It shows the PnL attributed to each name as a percentage of the total PnL for all names.
====SPLIT====
The plot_prob_profit_trade function plots the probability of making a profitable decision (beta distribution)
against the belief of that decision. The Beta distribution is a continuous probability distribution on [0, 1]
with two parameters alpha and beta. This function takes in round_trips dataframe which contains information about 
each trade made by our strategy such as PnL, open time, close time etc. It also takes in an ax object to allow for 
multiple subplots if necessary.
====SPLIT====
The plot_cones function plots the upper and lower bounds of a cone on
a graph. The function takes as input parameters:
    name (str): Name of the strategy to plot. This is used in the title of
        the graph, and as a label for each individual cone.
====SPLIT====
The var_cov_var_normal function computes the VaR and CVaR of a portfolio
given a confidence level, P, c, mu and sigma. The default values for mu
and sigma are 0 and 1 respectively.
====SPLIT====
The sortino_ratio function is a performance analysis function that measures the return per unit of 
volatility that an investment has generated. It is a ratio calculated by subtracting the risk free rate from 
the return and dividing this difference by the standard deviation of negative returns. The ratio shows how much extra
returns are generated for every unit of volatility during a period.
====SPLIT====
The downside_risk function computes the downside deviation below a threshold.
====SPLIT====
The sharpe_ratio function computes the annualized return, annualized volatility, and annualized sharpe ratio of a portfolio.
The function takes in an array of daily returns for an asset or portfolio as well as an optional risk free rate. 
If no risk free rate is passed in then it assumes a return of 0 for the risk free rate. The function also assumes 252 trading days per year unless otherwise specified.
====SPLIT====
The rolling_beta function computes a rolling beta for each point in time.
It takes the following parameters:
returns: A pandas Series representing period percentage returns.
factor_returns: A pandas Series representing period percentage returns of the benchmark factor to which betas are computed.  # noQA
rolling_window: Integer window size for calculating rolling beta (default 6 months).
====SPLIT====
The gross_lev function computes the gross leverage of a strategy.
====SPLIT====
The perf_stats function computes a suite of performance metrics and
returns them as a pandas Series. The specific set of metrics returned
depend on the inputs given (see docstring for details). In addition to the 
metrics, the function also returns some other useful information about each metric:
====SPLIT====
The perf_stats_bootstrap function calculates a distribution of performance
statistics using bootstrap sampling. The function takes in a pandas Series or DataFrame,
and returns the mean, median, 5th percentile and 95th percentile of the distribution. 
The user can also choose to include factor-related statistics by passing in a second series.
====SPLIT====
The calc_bootstrap function calculates the bootstrap value for a given statistic.
It takes in a function, returns, and optional factor_returns. The function passed should have returns and factor_returns as its first two arguments. Additional arguments can follow.
====SPLIT====
The calc_distribution_stats function calculates the mean, median, standard deviation, 5th percentile
25th percentile, 75th percentile and 95th percentile of a given array. The function also returns the 
interquartile range (IQR) which is calculated as the difference between the 75% and 25% percentiles. 
The IQR is useful for identifying outliers in data.
====SPLIT====
The get_max_drawdown_underwater function returns the peak, valley, and recovery dates given an 
underwater DataFrame. For example, a peak of Sep 12 means that if you invested 1 dollar on Sep 12 and 
sold everything 10 days later (i.e., on Oct 2), you would have made a maximum loss of 0.102811 or 10%.
====SPLIT====
The get_max_drawdown function returns the maximum drawdown of a strategy.
It takes in a series of prices and returns the largest percentage drop from peak to trough.
====SPLIT====
The get_top_drawdowns function returns the top drawdowns from a set of returns.
It takes two arguments:
    -returns = A pandas Series representing period percentage returns.
    -top = An integer indicating how many of the worst drawdowns to list.
====SPLIT====
The gen_drawdown_table function creates a drawdown table showing the top 10 
drawdowns and their duration. The function takes two arguments: returns, which is 
a pandas Series of asset returns, and top, which is an integer indicating how many 
largest drawdowns to include in the output.
====SPLIT====
The rolling_volatility function computes the annualized volatility of a 
time series of asset returns. It takes two arguments:
    1) returns - a pandas Series representing period percentage returns.
    2) rolling_vol_window - an integer representing the number of days over which to calculate 
       annualized volatility. This is usually calculated over a 60 day window (i.e., two months).
====SPLIT====
The rolling_sharpe function computes the rolling Sharpe ratio of a strategy.
It is based on sub function _get_daily_returns().
The input parameters are: returns (a pandas Series) and rolling_sharpe_window (an integer).
The output is a pandas Series of the same size as input parameter returns.
====SPLIT====
The simulate_paths function simulates a path for the returns of an asset.
It takes as input:
is_returns - The returns of an asset, usually generated by simulate_asset_daily_returns.
num_days - The number of days to simulate. This is usually the number of trading days in a year (252). 
starting value - The starting value for the given asset, defaulted to 1 (representing initial investment). 
num samples - Number of paths to generate, defaulted at 1000.  Returns num samples simulated paths.
====SPLIT====
The summarize_paths function takes a DataFrame of paths and returns a DataFrame with the mean, standard deviation,
and cone-of-influence bounds for each path. The cone_std parameter allows you to specify the number of standard 
deviations to use for your cone. The starting_value parameter allows you to specify what value should be used as 
the starting point for your cumulative returns curve.
====SPLIT====
The extract_interesting_date_ranges function extracts returns for a subset of interesting time periods.
====SPLIT====
The model_returns_t_alpha_beta function fits a Student T model to the returns data.
It uses the returns of a benchmark index as features in the regression.
The function assumes that all assets are independent, and thus does not apply any form of 
covariance matrix estimation or condition number analysis. It also does not use any information 
about asset correlations (such as an autocorrelation matrix). The model is fit using Markov chain Monte Carlo sampling.
====SPLIT====
The model_returns_normal function fits a model to the returns data and returns 
the posterior distribution of the parameters. The function takes three inputs:
data, samples, and progressbar. Data is expected to be a pandas Series object with 
daily returns. Samples specifies the number of posterior samples to draw using PyMC3's 
sample method (default value is 500). Progressbar determines whether or not to display 
a progress bar in your terminal while sampling from the posterior (default value is True).
====SPLIT====
The model_best function fits a Student T model to the returns of two groups.
It then calculates the probability that each group has a higher Sharpe ratio than
the other, as well as the probability that their means are different. The function
returns a trace object which can be used for further analysis.
====SPLIT====
The model_stoch_vol function fits a stochastic volatility model to the provided data.
====SPLIT====
The plot_stoch_vol function plots the absolute returns of a stock and the stochastic
volatility process that was fit to those returns. The function takes three arguments:
data, which is a pandas DataFrame containing the absolute returns of a stock, trace, 
which is an array containing samples from the posterior distribution of latent volatility 
processes for each date in our dataset (the number of columns = number of dates), and ax, 
which is a matplotlib Axes object on which we plot. If no Axes object is provided by the user, 
the function will create one itself.
====SPLIT====
The compute_bayes_cone function computes the Bayesian cone for a set of predictions.
The starting value is 1, unless otherwise specified.
====SPLIT====
The compute_consistency_score function computes the consistency score for a given set of predictions and test returns.
The consistency score is defined as the maximum median future return, divided by half its range. 
For example, if the median future return is -10% with a range from -20% to +20%, then the consistency score will be computed as:
(-10%) / (20%-0%) = -50%. Therefore, this function would compute 50 for each row in our dataframe.
====SPLIT====
The run_model function runs the model of choice on the data provided.
It returns a trace object which contains samples from posterior distribution.
The function also has an optional ppc argument, which if set to True will return 
a 500xN matrix of posterior predictive checks (where N is the number of days in 
the test data). This can be useful for assessing whether or not you have enough 
samples to trust your result.
====SPLIT====
The plot_bayes_cone function creates a Bayesian cone plot of returns using the 
    posterior distribution of expected returns and volatility. The function takes in 
    parameters:
====SPLIT====
The _GetNextLogCountPerToken function is a helper function for the Log*() functions.
It's purpose is to uniquify the logging messages, so that repeated log messages will be
distinguished from each other. The way it accomplishes this feat is by internally maintaining
a dictionary of strings and their corresponding counters. For every call to _GetNextLogCountPerToken(),
the counter for that string increments by 1, so that future calls with the same string will yield a different counter value
====SPLIT====
The log_every_n function is designed to be used in code sections that occur
frequently and would normally be logged with a level of INFO.  If such a message
is logged at the level of INFO or higher, it will only be displayed once every n
times where n is the value passed into log_every_n.  This allows for all levels
of DEBUG messages to display on screen while reducing the spammy nature of too many
INFO and WARNING messages.
====SPLIT====
The log_if function is a helper function that logs a message at the given level
if the condition is true.  The condition should be an expression that evaluates to
a boolean value.  If it evaluates to True, then the message will be logged at the
given level (which must be one of logging.DEBUG, logging.INFO, or logging.WARNING).
The msg argument should contain a string suitable for passing as an argument to one of Python's log functions: either str() or repr().   The args are passed on directly to one of Python's log functions.
====SPLIT====
The google2_log_prefix function is a custom log prefix function for Google2
logging. It prepends the following information to each log line:
  * The severity level of the message (e.g., INFO, WARNING, ERROR) in single quotes;
  * The month, day, hour, minute and second of when the message was logged;
  * The number of microseconds since midnight UTC on January 1st 1970; and
  * The process ID followed by a space character followed by the name of this file.
====SPLIT====
The create_distributed_session function creates a MonitoredTrainingSession object that handles the
distributed execution of the training loop. The chief role of this function is to create a session with
the correct configuration for distributed training, and then to invoke hooks on that session according to
the specifications in the provided task_spec. This function also performs some bookkeeping such as saving
checkpoints and summaries at regular intervals, as well as determining when it's time to stop the training loop.
====SPLIT====
The Trainer.validation_metrics function returns a list of tuples, where each tuple contains the name
of the metric and its average value across all batches in the validation set. The Trainer class uses this
to store validation metrics for each epoch.
====SPLIT====
The Trainer.train_and_validate_to_end function trains the model on a dataset, and validates it every 50 steps.
It continues training until the session is stopped or told to stop (by setting self._sess.should_stop() to True).
====SPLIT====
The load_matt_mahoney_text8_dataset function downloads a text8.zip file from the internet 
(if it is not already available locally) and unzips it into a list of words. The function also 
removes very rare words (words that appear less than 5 times in the corpus). Finally, the function 
returns a list of words.
====SPLIT====
The load_imdb_dataset function loads the IMDB dataset from a local directory.
It returns four numpy arrays: X_train, y_train, X_test, and y_test.
X is a list of reviews; each review is a list of word indices (encoding a sequence of words).
y is the sentiment label associated with that review: 0 or 1.
====SPLIT====
The load_nietzsche_dataset function downloads the nietzsche dataset if it is not present in the specified directory.
It then returns a string containing all of the words in this dataset, which have been converted to lowercase.
====SPLIT====
The load_wmt_en_fr_dataset function downloads the English-to-French translation dataset from the internet.
It then extracts it, puts it in a folder called data/wmt_en_fr, and transforms the format of each file to match
the format used by our translation model (English sentences with tab separation from their corresponding French 
translations). The function returns two paths: one for the training set and one for the development set. 
The paths are returned as strings that can be passed to other functions.
====SPLIT====
The load_flickr25k_dataset function downloads the Flickr25k dataset, extracts it into a folder called data/flickr25k,
and returns a list of images in that folder. The tag parameter is used to filter the images by keyword.
====SPLIT====
The download_file_from_google_drive function downloads a file from Google Drive.
It takes two arguments:
    1) ID: The unique identifier for the file you wish to download. This is visible in the browser address bar when you 
    click on the file in Google Drive and looks something like this: 1jhQbZAZoYXf-6ywz_7L8l4UYx3A2eWts
====SPLIT====
The load_celebA_dataset function downloads the celebA dataset from google drive and returns a list of image paths.
The celebA dataset consists of over 200,000 celebrity images with annotations. The images are cropped to remove parts 
of the image that don't include a face, then resized to 32 x 32 pixels.
====SPLIT====
The assign_params function assigns the parameters in params to the network.
   The function takes as input a session, params, and network. It then iterates 
   through each parameter in params and assigns it to its corresponding parameter 
   in the network. Finally, it outputs a list of operations that can be run to 
   assign these parameters.
====SPLIT====
The load_and_assign_npz function loads a network from an .npz file and assigns the parameters to another network.
====SPLIT====
The save_npz_dict function saves the variables in a list to an npz file.
The save_npz_dict function takes as input a session, and a list of variables.
It then creates an npz file with the name 'model.npz'. The npz file stores 
the data for each variable in two keys: 'name' and 'var'. For example, if you 
have one variable called &quot;weights&quot;, then the key &quot;weights&quot; will contain the 
variable weights (as numpy array). Similarly, if you have two variables called 
&quot;weights&quot; and &quot;bias&quot;, then there will
====SPLIT====
The save_ckpt function saves the model's variables in a checkpoint file.
The function takes as input:
    - sess (the current session)
    - mode_name (a string describing the name of the checkpoint file)  # default value is 'model.ckpt'
    - save_dir (a string describing the directory where to save checkpoints)  # default value is 'checkpoint'
    - var_list (an optional list of variables to save, otherwise it will use all global variables that are tf.Variable objects). Default value is None. If you want to specify which variables to save, pass in
====SPLIT====
The load_ckpt function loads a checkpoint file and restores the parameters of a model.
It is useful for loading checkpoints that have been saved in another environment, or when you want to load an existing model without redefining the graph.
The function takes as input: session, name of the checkpoint file (defaults to &quot;model.ckpt&quot;), directory where checkpoints are stored (defaults to &quot;checkpoint&quot;), list of variables that you want it to restore (if none it will restore all), whether or not you wish for print output about which variables are being restored and if you wish for the function to try and find the latest checkpoint.
====SPLIT====
The load_npy_to_any function loads a .npy file from the specified path and returns it as a dictionary.
If no path is specified, the function will load from the current directory.
The name parameter specifies what file to load.
====SPLIT====
The load_file_list function creates a list of all the files in a given directory.
It can be used to load an image dataset from disk, for instance. 
The function takes three arguments: path, regx and printable.
====SPLIT====
The load_folder_list function creates a list of all the folders in the specified path.
The load_folder_list function takes one argument, which is a string that specifies the path to 
the folder containing all of your data files. The load_folder_list function returns a list 
containing strings for each file name.
====SPLIT====
The exists_or_mkdir function checks whether a directory exists. If it does not exist, the function creates it.
The function takes two arguments: path and verbose (default True). The path argument is the name of the directory to be checked or created, while verbose determines whether messages will be printed to stdout.
====SPLIT====
The maybe_download_and_extract function checks if the data exists, and if not downloads it.
   The maybe_download_and_extract function accepts three parameters:
       - filename: The name of the (to be) downloaded file.
       - working_directory: A directory path to search for the file in and download to (if not found in script root directory).
       - url: The web address to download from.
====SPLIT====
The natural_keys function is used to sort the list of files in order by their
numerical value rather than lexicographically. For example, if you have a list of
files named:
====SPLIT====
The threading_data function is a convenience function that allows for the processing of large amounts of data in parallel.
It takes as input a list or array, and splits it into multiple threads based on the thread_count parameter.
The fn parameter must be a function that takes in one element from the list/array and returns an output (or outputs).
If no thread_count is specified, then all elements are processed sequentially by calling fn once per element.  If there are n elements in the array/list, then if thread_count=n each element will be processed by its own dedicated thread.
====SPLIT====
The affine_transform_keypoints function takes a list of coordinates and an affine transform matrix
and returns the transformed coordinates. The function first converts the coordinate list to a numpy array,
transposes it so that each row is one set of coordinates, inserts a column of 1s at the end for translation,
multiplies it by the transform matrix (which is 3x3), then removes extraneous columns and transposes back to 
the original format.
====SPLIT====
The projective_transform_by_points function transforms an image by first finding the projective transformation
that best maps the source points to destination points, and then applies that transformation to the image. The 
parameters of this function are:
====SPLIT====
The rotation function rotates the image by a random angle between -20 and 20 degrees.
The rotation is centered on the center of the image, where it remains after rotating.
The function takes in an input x, which is an array of images. The function then loops through each image in x and 
applies a random rotation to each one.
====SPLIT====
The crop function takes an image and crops it to a specified size.
====SPLIT====
The crop_multi function crops multiple images (x) to the specified height and width.
The is_random parameter specifies whether or not random cropping should be done.
If it is False, then the central crop will be taken with respect to the center of each image.
If it is True, then a random crop will be taken for each image.
====SPLIT====
The flip_axis function flips the input array along a given axis.
====SPLIT====
The flip_axis_multi function flips the data along a given axis.
    The function takes three arguments:
        x - the input data to be flipped, can be a list or numpy array.
        axis - the axis along which to flip over (0 for vertical flipping, 1 for horizontal flipping)
        is_random - if True, then it randomly flips with 50% chance; otherwise it always flips.
====SPLIT====
The shift function shifts the image in a random direction. The shift is either vertical, horizontal or both.
The shift amount is chosen randomly within the given interval (hrg, wrg).
====SPLIT====
The brightness function is used to change the brightness of an image.
The function takes in a single parameter, x, which is the image. 
The gamma value can be set as a random variable or left at its default value of 1. 
This function returns the modified image.
====SPLIT====
The illumination function adjusts the brightness of training images. It is useful for helping the
training process along by adding some variety to the input data set. Since there are only two labels,
a little augmentation is necessary in order to make a good training set - you can't just add more 
images with different labels because then you have nothing to train against! Augmenting your data 
with randomness can be tricky, however, since it would defeat the purpose of prediction if what your 
network predicts depends on random chance. To account for this, we use a probability distribution that 
we define ourselves so that our use
====SPLIT====
The adjust_hue function takes an RGB image as input and returns a new RGB image
with the color of every pixel adjusted along the hue dimension. The adjust_hue
parameters hout is either a positive number (in which case it represents the 
amount to add to or subtract from the hue channel in each pixel) or else a random 
number between -hout and +hout. If is_offset is True, then hout will be added to 
the hue channel; otherwise, it will be subtracted. If is_clip is True, then values 
that are greater than 1 or less than 0
====SPLIT====
The imresize function resizes an image to a specified size.
====SPLIT====
The pixel_value_scale function takes an image as input and returns a new image with pixel values scaled by the specified value.
The function can also be used to clip the pixel values at specified lower and upper bounds.
====SPLIT====
The samplewise_norm function is a helper function that normalizes the input data.
It performs two tasks:
    1) If samplewise_center=True, it centers the data by subtracting the mean of each sample.
    2) If samplewise_std_normalization=True, it divides each input by its standard deviation.
====SPLIT====
The featurewise_norm function normalizes the input x by subtracting its mean and dividing it by its standard deviation.
The function takes in 3 parameters: x, mean, std. If a value for mean is provided as an argument (i.e., if an argument
for mean is passed into featurewise_norm), then the input x will be normalized using that value for the mean; otherwise, 
the input will be normalized using its own data's average (mean). Similarly, if a value for std is provided as an 
argument to featurewise_norm), then the input will be normalized using that value for standard deviation; otherwise
====SPLIT====
The get_zca_whitening_principal_components_img function computes the ZCA whitening principal components from a given image dataset.
The function takes as input:
- X (the original image dataset) and returns:
- principal_components (the ZCA whitening principal components).
====SPLIT====
The zca_whitening function takes as input the image data and the principal components 
and returns a whitened image. The function first flattens the image to a 1D array, 
then multiplies it by the principal components, and reshapes it back into its original shape.
====SPLIT====
The drop function takes an image as input and returns a new image with the specified percentage of its pixels set to zero.
====SPLIT====
The pt2map function takes a list of points and returns a 2D numpy array
representing the point coordinates as 1s in an otherwise empty image.
This is useful for showing the shape of a particular contour, for instance.
====SPLIT====
The parse_darknet_ann_str_to_list function takes a string of annotations from the Darknet annotation file and parses it into a list.
The function splits the string on newline characters, then iterates over each line in the resulting list. 
If there are 5 items in each line (which is expected since we expect 5 columns per annotation), 
then that line is split on whitespace and converted to floats where appropriate. The result of this parsing operation is returned as an array.
====SPLIT====
The parse_darknet_ann_list_to_cls_box function takes a list of annotations from the Darknet annotation file and 
returns two lists: one containing the class labels for each bounding box, and another containing all of the bounding
box coordinates in [xmin, ymin, xmax, ymax] format.
====SPLIT====
The obj_box_horizontal_flip function takes an image and a list of bounding boxes as inputs.
It also has two optional arguments: is_rescale and is_center. If these are both set to False, 
the function will flip the image horizontally along its vertical axis, which means that the x-coordinates 
of every bounding box will be flipped as well. The widths of all bounding boxes will remain unchanged; only their 
locations (x-coordinate plus width) on the x-axis are changed by this function. If either or both of these optional arguments are set to True, then different behavior occurs
====SPLIT====
The obj_box_imresize function takes an image and a list of bounding boxes as inputs.
It resizes the image to the desired output size (e.g., [100, 100]) and transforms 
the coordinates of the bounding boxes in such a way that they fit this new image.
====SPLIT====
The remove_pad_sequences function removes the padding from a list of sequences.
====SPLIT====
The sequences_get_mask function takes a list of sequences and returns a masking array.
The mask is 1 for all non-padded tokens in the sequence, and 0 for all padded tokens.
This allows you to use the returned array as an index into your embedding layer, 
so that you can treat padded values as just another word vector.
====SPLIT====
The keypoint_random_crop function takes an image and a list of keypoints as input.
It then randomly crops the image to a target size, maintaining the relative position of all keypoints.
The function returns three items: the cropped version of the original image, 
a new list containing all keypoint coordinates in relation to the cropped images coordinate system, 
and a mask that is zero anywhere outside of it's body.
====SPLIT====
The keypoint_random_flip function takes in an image and a list of keypoints, 
and returns the same image flipped horizontally along with the new keypoints. 
The function also flips the mask if it is passed in as an argument. The function assumes that all keypoint coordinates are given as (x,y) pairs.
====SPLIT====
The keypoint_random_resize function takes an image and a list of keypoints as input.
It then resizes the image to some random scale between 0.8 and 1.2 times its original size, 
and adjusts the keypoints according to this scaling factor.
====SPLIT====
The discount_episode_rewards function takes a list of rewards and applies discount factor gamma to the rewards,
which is 0.99 by default. The discounted_r array is initialized with zeros, and the running_add variable starts at zero
as well. Then for each time step in the episode t, we apply discount factor gamma to our reward r[t] (if it's not 0). 
The running add variable then accumulates this discounted reward value over future timesteps.
====SPLIT====
The cross_entropy_reward_loss function takes in the logits, actions, and rewards from the environment.
It then uses a sparse_softmax_cross_entropy with logits function to calculate cross entropy loss between 
the action taken by the agent and what we want it to take. We multiply this by our reward function so that 
we only train on good actions.
====SPLIT====
The log_weight function computes the weighted log probability of a batch of data.
The weights are used to adjust for the imbalance in number of positive and negative examples.
This function is useful when computing cross entropy loss, which requires this weighted log probability as its input.
====SPLIT====
The choice_action_by_probs function takes in a tuple of probabilities and an optional list of actions.
If no list is provided, the function will assume that there are two actions (0 and 1) with equal probability.
The function returns an action selected by sampling from the inputted distribution.
====SPLIT====
The cross_entropy function takes the output of a neural network (a tensor) and compares it to the target labels.
It returns a score that we want to minimize by optimizing our weights. The lower the score, the better our model's
prediction performance.
====SPLIT====
The sigmoid_cross_entropy function takes the output of a neural network (a Tensor), and compares it to the target
using sigmoid activation. The result is a scalar Tensor, which represents the loss.
====SPLIT====
The binary_cross_entropy function takes as input two tensors, output and target.
It then computes the binary cross entropy loss between the two tensors.
The function returns a scalar which is the mean of this loss.
====SPLIT====
The normalized_mean_square_error function computes the normalized mean square error between two tensors.
The normalized_mean_square_error function takes as input two tensors: output and target, and returns a scalar denoting the 
normalized mean square error between them. The shape of both output and target should be either [batch_size, n] or [batch_size, w, h].
====SPLIT====
The cross_entropy_seq_with_mask function accepts logits, a target label sequence, and a mask over the target
label sequence. It returns the average cross entropy loss between all of the non-masked labels and their
corresponding logit predictions. This function is helpful because it allows us to compute cross entropy losses on
a batch of sequences which have variable lengths without having to pad or truncate any of those sequences.
====SPLIT====
The maxnorm_regularizer function enforces the max-norm regularization.
The max-norm is a constraint that places an upper bound on the magnitude of the weights in a given layer.
This function takes no arguments and returns a function which can be used to apply max-norm regularization to each column of weight matrix.
====SPLIT====
The ramp function is a helper function that takes in an input tensor and returns the same
tensor with its values clipped between 0 and 1. This is useful for ensuring that our loss
is always positive, which can sometimes be problematic when using certain activation functions.
====SPLIT====
The swish function is defined as the product of a sigmoid function and its argument.
It is designed to be applied to real numbers in the range (-inf, inf).
The swish function has been found useful in many applications, including deep learning.
For example, it can be used as a replacement for ReLU with better performance.
====SPLIT====
The pixel_wise_softmax function is a helper function that implements the 
softmax activation in tensorflow. It takes as input a 4D tensor of shape [batch_size, height, width, channels]
and returns another 4D tensor of the same shape and dtype. The output has one channel for each class (i.e., 2 for binary classification)
====SPLIT====
The retrieve_seq_length_op3 function calculates the length of each sequence in a batch.
The input is assumed to be 3-D Tensor(s) with shape [batch_size, ? (any other dimensions), max_sequence_length].
The output is 1-D Tensor with shape [batch_size] containing the lengths of sequences in the batch.
If data contains any non-zero element, then its position along axis 2 is recorded and subsequently summed up.
For example: if data = [[[0 0 0]  [[0 0 1]  [[0 1 0]  [[0 1 1]    # where &quot;
====SPLIT====
The BasicConvLSTMCell.state_size function returns the size of states that are returned by this cell.
This function is a symbolic method for getting the shape of an output tensor given its state_size, 
which is a list or tuple containing 2 elements: c and h where c represents number of units in LSTM cell's 
c state and h represents number of units in LSTM cell's h state. The return value is a TensorShape object 
containing both dimensions (number of units in c and number of units in h). This function can be useful when you want to create placeholders for states, but need to know their
====SPLIT====
The DeformableConv2d._tf_repeat function is a helper function that allows for the use of TensorFlow's
tf.repeat function, which does not exist in tensorflow 1.14 (the version installed on Ubuntu 18.04).
The tf_repeat function is used to replicate the input tensor multiple times along one dimension.
====SPLIT====
The DeformableConv2d._tf_batch_map_coordinates function is a TensorFlow implementation of the
equation given in the Deformable Convolution paper (Bilinear Sampler) for mapping coordinates from
the input image to a set of output feature maps. The function takes as input an NxHxWxC Tensor, and 
returns an N*H*W*Kx2 matrix where K is the number of kernels in the layer. Each row corresponds to 
a coordinate mapping between one sample from each kernel and one sample from each channel in the image.
====SPLIT====
The DeformableConv2d._tf_batch_map_offsets function takes as input a batch of 3D tensors (a, b, c) and an offset
tensor (d), each with the shape: [batch_size, height, width]. It then performs the following operations:
    1. Reshapes all inputs to have shape [batch_size * height * width]
    2. Creates a grid tensor with shape [height * width * batch_size] x [-2,-3], where -2 is the height dimension and -3 is 
       the width dimension of input tensors. The elements in this grid are ordered from left to
====SPLIT====
The minibatches function iterates through the provided inputs and targets, 
and returns a tuple of batches of inputs and corresponding batches of targets. 
The minibatches function also accepts an optional argument called batch_size that allows you to specify how large you want each batch to be. 
If no value is passed in for batch_size, then the default value is 20.
====SPLIT====
The TensorHub.save_model function saves the model's architecture and parameters to MongoDB.
It also serializes the parameters into a binary format for fast loading later.
The function takes in a TensorFlow network object as an argument, and uses that to create 
the model's architecture document in MongoDB.
====SPLIT====
The TensorHub.find_top_model function is a helper function that finds the top model in the database based on
the input arguments. The input arguments are passed as kwargs and must be exactly the same as what was used to train
the model. If no models match, then False is returned. Otherwise, a tensorflow session is created and loaded with 
the parameters of the top model from mongoDB (stored in params_id). The network object containing all information 
about this network architecture (including its graph) can be found at self._network.
====SPLIT====
The TensorHub.delete_model function deletes a model from the database.
It takes in a dictionary of arguments, and uses those to delete the specified model from the database.
The function returns nothing.
====SPLIT====
The TensorHub.save_dataset function saves a dataset to the database.
It takes as input:
    - A dataset (a numpy array)
    - The name of the dataset (optional, if none is given then it will be called &quot;dataset_x&quot; where x is an integer)
====SPLIT====
The TensorHub.find_top_dataset function finds the top-most dataset in the database that matches a given set of parameters.
The function takes as input a dictionary of key-value pairs, where each key is an attribute name and each value is either 
a single string or list of strings. The function returns a single Dataset object if one exists that matches all keys and 
values exactly; otherwise it returns False.
====SPLIT====
The TensorHub.find_datasets function searches the database for datasets that match the given parameters.
It returns a list of dataset objects, which can be used to access their data files.
The function takes as input a dictionary of parameters and values, and uses them to filter down the set of all datasets in 
the database that have parameter values matching those provided in the input dictionary.
====SPLIT====
The TensorHub.delete_datasets function deletes all datasets in the database that match the given keyword arguments.
The function takes a single argument, which is a dictionary of keywords and values. The following keywords are supported:
    project_name - A string indicating the name of the project whose datasets should be deleted. If this keyword is not specified, then no dataset will be deleted.
====SPLIT====
The TensorHub.save_training_log function saves the training log to the database.
The function accepts a dictionary of key-value pairs as input, and uses them to create a new entry in the TrainLog collection.
The following keys are required:
====SPLIT====
The TensorHub.save_validation_log function saves the validation log to the database.
It takes a dictionary of key-value pairs as input, and uses them to create a new document in the ValidLog collection.
The function also prints out what it has done, and returns nothing.
====SPLIT====
The TensorHub.delete_training_log function deletes all training logs matching the given query.
The query is a dictionary of MongoDB queries. The function returns None.
====SPLIT====
The TensorHub.delete_validation_log function deletes all validation logs from the database.
It takes in a dictionary of arguments, which are listed below:
    - project_name (str): The name of the project to delete validation logs for.
====SPLIT====
The TensorHub.create_task function creates a new task in the database.
It takes as input:
    - task_name (str): The name of the task to be created. This will be used by other functions to refer to this specific task, so it should be descriptive and unique among all tasks in your project.
    - script (str): The path of the script that you want TensorHub to run when executing this Task. This can either be an absolute path or a relative path from where you are running TensorHub's python code, but it must exist on whatever machine that you are using for execution (i.e., if
====SPLIT====
The TensorHub.run_top_task function is a wrapper function that calls the TensorHub.run_task function
on the top task in the queue, which is sorted by priority and push time. The run_top_task function also 
handles exceptions raised by TensorHub.run_task, and ensures that tasks are properly requeued if they fail.
====SPLIT====
The TensorHub.delete_tasks function deletes all tasks in the database that match the given query.
The query is a dictionary of key-value pairs, where each key is an attribute of a task and each value
is what that attribute must match to be deleted.
====SPLIT====
The TensorHub.check_unfinished_task function checks the database to see if there are any tasks that have not been completed.
If there are, it returns True. If not, it returns False.
====SPLIT====
The augment_with_ngrams function takes a list of unigrams and returns a list of ngrams.
It does this by taking the unigram list, zipping it with itself, then iterating over the result to create ngrams.
The function also hashes each ngram using sha256 and modulo arithmetic to assign an id between 0 and num_buckets - 1.
====SPLIT====
The load_and_preprocess_imdb_data function loads the IMDB dataset, and performs a number of preprocessing steps:
    1. It maps all of the text to lowercase.
    2. It maps all digits to a special symbol &quot;NUM&quot;.
    3. For each document, it computes an array mapping each word (in order) to its frequency count in that document.
====SPLIT====
The read_image function reads in an image from a specified path.
====SPLIT====
The read_images function reads in images from a list of image file names, and returns a list of numpy arrays.
The function takes as input:
    1) img_list: A list containing the names of the image files to be read.
    2) path: The path to where the images are stored on disk (if not included, it assumes that all files are in 
        current directory).  This is useful if you want to read in an entire folder full of images instead 
        just a single file.
====SPLIT====
The save_image function saves an image to a specified path.
====SPLIT====
The save_images function takes as input a list of images, size, and image_path.
It then saves the images to the specified path in a grid format based on the size.
If there are more than 10 images in the list, it will raise an error.
====SPLIT====
The draw_boxes_and_labels_to_image function takes as input an image, bounding box coordinates, class number,
and scores. It then labels the bounding boxes and draws them to the image. The function returns a visual 
representation of what was drawn to the image.
====SPLIT====
The CNN2d function accepts a 4-D array of data (a tensor) and plots the CNN layers as images.
The default is to plot all layers, but you can also specify which layers you want plotted.
This function is intended for use only with ReLU activation functions in the hidden CNN layers.
====SPLIT====
The tsne_embedding function takes as input the embeddings and reverse_dictionary, 
and plots the corresponding t-SNE visualization of the embedding. The function also 
takes in two arguments: plot_only and second. The plot_only argument allows you to 
plot only a subset of all the embeddings (in this case, we are plotting only 500 words). 
The second argument is used to control how long it takes before showing each image; if you set it to 0, then every image will be shown instantly. This can be useful for quickly inspecting your results as you’re developing your
====SPLIT====
The draw_weights function visualizes the weights of a neural network.
It takes as input an array W of dimension n x m, where each row is a weight vector for one neuron.
Each column in W corresponds to the connectivity between that neuron and another layer consisting of n_col neurons.
The function plots all the weight vectors; thus, if there are m columns (m &lt; n_col), then only the first m will be plotted.
====SPLIT====
The data_to_tfrecord function converts images and labels into a tfrecord file.
The function takes in the following parameters:
images: The image data to be converted to tfrecord format.
labels: The corresponding labels of the image data being converted to tfrecord format.
filename: The name of the file that will be created by this function as output, e.g., &quot;train&quot; or &quot;test&quot;.  This parameter is used for naming purposes only, so that we can call it once and use it repeatedly when converting all training and test datasets into separate .tfrecords files.
====SPLIT====
The read_and_decode function takes in a filename, and returns an image and its label.
The function will output the following:
    1. img: The decoded jpeg image casted to float32 from tf.string data type
    2. label: The integer corresponding to the correct class of the image (0-9)
====SPLIT====
The Layer.print_params function prints the names and values of all the parameters in a network.
It's helpful to understand what each layer is keeping track of, especially while debugging.
====SPLIT====
The Layer.print_layers function prints out the layers of a network in order,
with layer names and shapes. It accepts as input an optional list of Layer objects to print,
otherwise it will print all the layers in the model. This function is helpful for inspecting
models and debugging.
====SPLIT====
The Layer.count_params function counts the total number of scalars composing the weights and biases of a Layer.
It can be useful for debugging and providing some statistics on how complex a network is.
====SPLIT====
The Layer.get_all_params function returns a list of all the parameters in the Layer.
====SPLIT====
The Layer._get_init_args function is a helper function that returns the arguments of the __init__ method
of a class. It is used to save these arguments in the Layer object, so that they can be accessed later on.
The skip parameter allows you to specify how many layers back in time you want to go when finding out which 
arguments were used when creating an instance of this class. For example, if layer A calls layer B and layer B 
calls Layer C, then setting skip=2 will allow you to access all three layers' init args.
====SPLIT====
The roi_pooling function takes as input a feature map and an array of bounding boxes.
It then computes the maximum over all bounding boxes in each region of interest, 
and returns the result. The output has size (batch_size, num_boxes, pool_height * pool_width)
====SPLIT====
The prefetch_input_data function creates a batch of images and labels at
each training step. The batch size is equal to the number of shards (files)
that are processed in parallel by each GPU. This allows us to shuffle all
training data only once per epoch instead of after every shard.  It outputs two objects: an image tensor and a label tensor.
====SPLIT====
The batch_with_dynamic_pad function batches input sequences and adds padding so that all input sequences in the batch are of the same length.
====SPLIT====
The _bias_scale function is a helper function that multiplies the input tensor by the bias term.
It also handles reshaping of the bias term if necessary.  The _bias_scale function is used in
the convolutional layers to scale their output before being passed into a nonlinearity.
====SPLIT====
The _bias_add function adds a bias vector to a tensor.
====SPLIT====
The batch_normalization function takes as input a tensor and normalizes it along the given axis.
The normalization is done by calculating the mean and standard deviation of the element in that axis,
and then subtracting the former and dividing by the latter. The elements for which either standard deviation or mean is zero are not normalized (i.e., they are simply copied over). 
The function also takes an offset value, which can be used to avoid division by zero if we know that all elements will be positive (elements might still be equal to zero in places where both mean and std were close to zero).
====SPLIT====
The compute_alpha function computes the alpha value for a given input.
The function takes in an input tensor and returns an output tensor of the same shape.
The output is a scalar that represents the computed alpha value for that element.
====SPLIT====
The flatten_reshape function takes a tensor and flattens it into a 1D vector.
It also reshapes the tensor to have the shape (-2, -3), where -2 is the batch size and 
-3 is the dimension of each element in that batch. This function was created because 
the input data for an LSTM must be in this format.
====SPLIT====
The get_layers_with_name function returns a list of all the layers in the network that contain
the name specified by the user. The function takes as input two parameters: net, which is a tensorflow
network object, and name, which is a string containing what we are looking for. We then iterate through 
all of the layers in net using its all_layers attribute and compare each layer's name to our search term 
(name). If there is any overlap between our search term and a layer's name (e.g., if they are exactly equal), 
we append that layer to our output list.
====SPLIT====
The get_variables_with_name function returns a list of all trainable variables whose name contains the input string.
The function takes three inputs:
name - The string to match variable names against. If no name is given, it will raise an exception.
train_only - A boolean value that decides whether or not to return non-trainable variables as well. By default, this is set 
to True and so only trainable variables are returned (unless there are no trainable variables in which case it will 
return whatever it can find). This option should be set to False if you want the function to also return non-trainable
====SPLIT====
The initialize_rnn_state function takes a state and returns the same state.
This function is used to create an initial hidden state for the decoder RNN, 
which is a required parameter when calling the dynamic_rnn_decoder function. 
The dynamic_rnn_decoder creates its first DecodeStep, which requires this initial hidden state.
====SPLIT====
The list_remove_repeat function removes any repeated elements from a list.
====SPLIT====
The ternary_operation function takes a tensor as input and returns the sign of the tensor.
The function first computes a threshold value using _compute_threshold, which is then used to 
determine the sign of each element in the input tensor. The final output is returned.
====SPLIT====
The _add_notice_to_docstring function adds a notice to the docstring of a function.
====SPLIT====
The alphas function creates a tensor of alphas with shape [N] where N is the number of classes.
The values are initialized to alpha_value, and will learn to different values during training.
====SPLIT====
The predict function is meant to be used with the TFLearn estimators.
It takes a tensorflow session, network, input placeholder and output operation
and returns the predicted values for that given input. It will automatically 
handle batching and all other aspects of running the prediction on multiple 
inputs in parallel. This function is especially helpful when using large neural 
networks with many weights/biases where it would otherwise be quite tedious to 
write code for predicting outputs manually.
====SPLIT====
The evaluation function takes as input the true labels and the predicted labels.
It then calculates a confusion matrix, f-score, accuracy and macro f-score.
The function returns these values in a tuple: (confusion_matrix, f-scores, accuracy).
====SPLIT====
The get_random_int function returns a list of random integers between the minimum and maximum values
provided. The default number of integers returned is 5, but this can be changed by passing an integer
value for the number parameter. If a seed value is provided, it will be used to initialize the random 
number generator so that the same sequence of numbers will be generated each time get_random_int is run.
====SPLIT====
The exit_tensorflow function is used to close tensorboard and nvidia-process if available.
It's not possible to do this via the regular python sys.exit() function, as tensorflow will throw an error.
====SPLIT====
The open_tensorboard function opens a TensorBoard instance on a new web browser window,
   and the user is automatically redirected to this window.
====SPLIT====
The clear_all_placeholder_variables function clears all placeholder variables from the global scope.
It is useful for clearing out the clutter of previous sessions, and it also helps prevent variable name collisions.
====SPLIT====
The set_gpu_fraction function sets the fraction of memory that will be used by the GPU.
====SPLIT====
The generate_skip_gram_batch function creates a data batch and label batch for Skip-Gram training.
The batches are created from the data list defined above. The labels are each of the words in 
the context window of the target word defined by skip_window (e.g., if skip_window=2, then two 
words before and two words after). The labels array is a 2D array with shape [batch_size, 1]. 
The elements in this array correspond to each other as well as to their positions within the buffer.
====SPLIT====
The sample function uses the a parameter as an input and returns an index of
the element with the highest value. If temperature is set to 1, which is the
default behavior, we will get back the argmax, or the element with the highest
value. This is what we want to use in our generative model; however, there are 
some cases where this may not be ideal. For example: if all elements have equal 
probability (all sum to 1), then we may want more variability so that each run 
of generation does not produce exactly similar results (thus making it easier for us humans to
====SPLIT====
The sample_top function is a helper function that returns the index of the highest value in an array.
It also takes two arguments, one optional and one required. The optional argument allows you to pass in a different
array than the array passed into sample_top as long as it has at least top_k number of elements. The required 
argument is top_k which defaults to 10 if not specified.
====SPLIT====
The create_vocab function creates a vocabulary dictionary from the training data.
The vocab_dict is a dictionary with keys as word and values as an id for that particular word.
The unk_id is the id of '&lt;UNK&gt;' token in our vocabulary.
====SPLIT====
The read_words function reads in the file nietzsche.txt and replaces all
occurrences of '\n' with '&lt;eos&gt;'. It also splits the contents of the file into a list.
====SPLIT====
The read_analogies_file function reads the analogy question file into a list of lists.
The analogy question file is assumed to be in a format like questions-words.txt, 
which has four words per line separated by spaces:
====SPLIT====
The build_reverse_dictionary function builds a reverse dictionary from the word_to_id dictionary.
The reverse dictionary is a simple python dict with the keys and values swapped.
This will make it easier to look up a word based on its ID later.
====SPLIT====
The build_words_dataset function creates a dataset of integers suitable for training a word embedding model.
The function extracts words from the data_path and puts them into buckets based on their lengths.
It returns two lists: the integer representation of the words, and the corresponding word lengths before they were put in buckets. 
The rest is just bookkeeping to maintain a vocabulary of coded words.
====SPLIT====
The save_vocab function creates a vocabulary file from the training data.
It is called by train() and takes as input:
    count, which is a list of 2-tuples (word, frequency) for each word in the vocabulary. 
    name='vocab.txt', which specifies the name of the file to save to.
====SPLIT====
The sentence_to_token_ids function converts a string to a list of integers representing the 
    words in the sentence. The integers are the dictionary indices of the words.
====SPLIT====
The data_to_token_ids function performs the following:
    1. Tokenize data into sentences and then to tokens
    2. Add any special tokens (if applicable)
    3. Map all of the tokens to their IDs in a vocabulary
====SPLIT====
The moses_multi_bleu function takes in a list of predicted sentences and a list of reference sentences,
and returns the BLEU score using the MOSES multi-bleu script. The MOSES multi-bleu script uses the official 
TensorFlow implementation for calculating sentence level BLEU scores. The function also supports lowercasing 
and tokenization if specified.
====SPLIT====
The SimpleVocabulary.word_to_id function takes a word and returns the corresponding ID. If the word is not in
the vocabulary, it returns the ID of &lt;UNK&gt; (unknown).
====SPLIT====
The Vocabulary.word_to_id function takes a word and returns the corresponding ID.
If the word is not present in the vocabulary, it returns the unknown word id.
====SPLIT====
The Vocabulary.id_to_word function takes a word ID and returns the corresponding word.
If the ID is greater than or equal to len(self.reverse_vocab), then it returns the &lt;UNK&gt; token,
otherwise it returns self.reverse_vocab[word_id]. The Vocabulary class stores two dictionaries: 
self.vocab and self.reverse_vocab.
====SPLIT====
The main_restore_embedding_layer function :
====SPLIT====
The createAndStartSwarm function creates a swarm of Hypersearch workers and
returns the jobID of the swarm.  This function will block until all of the
workers complete, which takes several seconds to several minutes, depending on
the search space size.  The client can use this jobID to check in on how the
search is progressing:
====SPLIT====
The getSwarmModelParams function returns a JSON string containing the model
parameters from the given swarm model. The returned dictionary has two keys:
  - 'modelConfig' : A dict containing all of the per-stream model parameters for
    this specific instance of the named swarm model. This dict is implemented as a frozen-set
    to ensure that its structure does not change in different invocations of NuPIC.
====SPLIT====
The enableConcurrencyChecks function enables concurrency checks.
====SPLIT====
The _getCommonSteadyDBArgsDict function returns a dictionary of the common
arguments that are passed to MySQLdb.Connect when connecting to a NuPIC
SteadyDB database.  The return value looks like this:
====SPLIT====
The _getLogger function is a helper function that will create a new logger
object for the class that calls it.  It will use the calling class name as the
name of the logger and set its level to whatever is passed in.  If no level is
passed in, then it defaults to logging debug messages.  The function returns an
instance of a logger object which has been configured appropriately.
====SPLIT====
The ConnectionWrapper.release function :
====SPLIT====
The ConnectionWrapper._trackInstanceAndCheckForConcurrencyViolation function is called by the ConnectionWrapper constructor to check for concurrency
   violations.  If a concurrency violation is detected, an error message is
   logged and then ConcurrencyExceededError() exception is raised.
====SPLIT====
The SingleSharedConnectionPolicy.close function closes the connection to the database.
====SPLIT====
The SingleSharedConnectionPolicy.acquireConnection function acquires a connection from the pool.
====SPLIT====
The PooledConnectionPolicy.close function closes the connection pool.
====SPLIT====
The PooledConnectionPolicy.acquireConnection function acquires a database connection from the pool.
It returns a ConnectionWrapper object that wraps the underlying database connection and provides some useful methods for working with it.
====SPLIT====
The PerTransactionConnectionPolicy.close function closes the connection to the database.
====SPLIT====
The PerTransactionConnectionPolicy.acquireConnection function acquires a new database connection from the pool.
====SPLIT====
The PerTransactionConnectionPolicy._releaseConnection function closes the cursor and then the database connection.
====SPLIT====
The KNNAnomalyClassifierRegion._classifyState function is the function that actually classifies a state vector
   using the KNN algorithm. It takes in a single record and returns its classification label, which is an array of 
   strings. The function first checks if this record has been previously classified by looking at each item's 
   &quot;setByUser&quot; attribute, which gets set to True if the user annotates it as an anomaly or non-anomaly. If it has not 
   been manually classified yet, then we look at where this record lies relative to all other records in terms of 
   distance (using our _calcDistance method
====SPLIT====
The KNNAnomalyClassifierRegion._constructClassificationRecord function is 
responsible for creating the classification vector. The _classificationRecord 
is a data structure that holds three values:
====SPLIT====
The KNNAnomalyClassifierRegion._addRecordToKNN function is the function that actually adds a record to the KNN classifier.
It does this by creating an anomaly vector for the record, and then passing it to _knnclassifier._knn.learn().
The _knnclassifier._knn object is an instance of pyspark.ml's LocalKNNModel class, which wraps a scikit-learn KNeighborsClassifier model in PySpark (see https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/classification.html#LocalKNNModel).
====SPLIT====
The KNNAnomalyClassifierRegion._deleteRecordsFromKNN function is a helper function that deletes
the given records from the KNN classifier. It also decrements the number of total records in 
the KNNClassifierRegion instance and increments the number of knnclassifier._categoryRecencyList.
====SPLIT====
The KNNAnomalyClassifierRegion._deleteRangeFromKNN function is a helper function that deletes the indices of the training data passed into it from the KNN classifier.
====SPLIT====
The KNNAnomalyClassifierRegion._recomputeRecordFromKNN function is a helper function that computes the category of an anomaly record using the KNNClassifierRegion.
====SPLIT====
The KNNAnomalyClassifierRegion._labelToCategoryNumber function takes a string label and returns the integer that 
the KNNAnomalyClassifierRegion will use to represent that label in its internal state. This mapping of string labels
to integers is saved in the KNNAnomalyClassifierRegion's self.saved_categories member variable for later recall during
prediction by the _recallCategoryNumber function. If a previously unseen string label is given to the 
KNNAnomalyClassifierRegion, then this function will add it to self.saved_categories and return an integer value for it.
====SPLIT====
The KNNAnomalyClassifierRegion._labelListToCategoryNumber function takes a list of labels and returns the category number.
   Each label is converted to its corresponding category number using the _labelToCategoryNumber function, then all of these 
   numbers are added together to return one integer value which serves as the index for that particular set of labels in 
   self._categoryList.
====SPLIT====
The KNNAnomalyClassifierRegion._categoryToLabelList function takes a category index and returns the list of labels associated with that category.
====SPLIT====
The KNNAnomalyClassifierRegion._getStateAnomalyVector function takes a state variable from the CLA Region and 
converts it to a numpy array of 1's and 0's based on the state vector. This function is used in both training mode 
and inference mode, so we need to make sure that it always returns the same output given the same input. The code for this function is shown below:
====SPLIT====
The KNNAnomalyClassifierRegion.getLabels function is a RESTful function that returns the labels of the
records contained in the classifier's anomaly cache. The list of returned labels will be in ascending order
of ROWID. If there are no cached records, an empty list will be returned.
====SPLIT====
The KNNAnomalyClassifierRegion.removeLabels function removes the given label(s)
from any records that currently have it in their anomalyLabel list. If the label
isn't already in that list, this does nothing to that record. If all labels for a
record are removed, the entire record is removed from KNN training and prediction.
====SPLIT====
The CategoryFilter.match function is used to determine if a record should be kept.
It iterates through the filterDict and checks each field for the presence of any of the categories in that field.
If at least one category is found in at least one field, then match returns True (and the record will be kept).
Otherwise, it returns False (and the record will not be kept).
====SPLIT====
The SpatialPooler.stripUnlearnedColumns function removes all the columns that have never been 
active from the set of columns available to be active. This is useful when you want to apply global
decay on synapses and then later reenable learning for a section of your input space. With a 
deterministic SP, this function does nothing because every column has been learned at some point in time.
====SPLIT====
The SpatialPooler._updateMinDutyCycles function updates the minimum duty cycles
of each column. The min duty cycles are calculated as a fraction of the maximum 
overlap value for each column. The fraction is specified by the parameter 
minPctOverlapDutyCycle in nupic.math.spatial_pooler.
====SPLIT====
The SpatialPooler._updateMinDutyCyclesGlobal function is responsible for 
updating the minimum duty cycles. The min duty cycles are used to determine 
the minimum activity of a column. This activity is used to keep a column in the 
spatial pooler, even if it has reached its overlap threshold. The min duty cycle 
is calculated as the max overlap duty cycle multiplied by a constant taken from:
====SPLIT====
The SpatialPooler._updateMinDutyCyclesLocal function is the local version of
the SpatialPooler._updateMinDutyCycles function. It takes in a single column and updates
its min duty cycles accordingly. The reason why we need this function is because when we are using
a global inhibition, it becomes difficult to calculate the min duty cycles for columns that have no 
overlap with any inputs. This happens because even if all of the input weights to a column were set 
to 0, it would not be &quot;removed&quot; from its neighbors' overlap calculations (since there are none). In order to 
accomplish this, we keep
====SPLIT====
The SpatialPooler._updateDutyCycles function updates the duty cycle of each column.
====SPLIT====
The SpatialPooler._avgColumnsPerInput function calculates the average number of columns per input.
It does this by creating a vector with each element representing the number of columns for each input, and then dividing it by the total number of inputs.
====SPLIT====
The SpatialPooler._avgConnectedSpanForColumn1D function computes the average connected
span of a column. The connected span of a column is the average number of consecutive
inputs that are connected to by synapses. This function is used in spatial pooling,
where we only want to increase permanence values for inputs that have no other nearby 
columns with overlapping synapses. If there are no unpredicted inputs, then we can 
use this calculation to select candidate columns from the input space which are not 
overlapping with any other columns in the SP region. The reason why it's an average is because each input has N(N-
====SPLIT====
The SpatialPooler._avgConnectedSpanForColumn2D function computes the average connected
span of a column. The connected span of a column is the average number of ON bits in
the potential synapses that are connected to input bits in the column.  If no input
bits are active, then 0 is returned as the average.  Otherwise, if some subset of the
potential synapses are active (at least 1), then this returns an appropriate value for
that subset.
====SPLIT====
The SpatialPooler._bumpUpWeakColumns function is a helper function of the
SpatialPooler class. It takes in an array of weak column indices and increments
the permanence values for each synapse on the input potential pool to move them 
closer to becoming active columns. This function is called by the SpatialPooler's 
compute method when learning is turned on, but only if there are some weak columns.
====SPLIT====
The SpatialPooler._raisePermanenceToThreshold function is a helper function of the compute method. It takes in two parameters:
====SPLIT====
The SpatialPooler._initPermConnected function is used to initialize the permanence values of potential synapses.
It takes in a parameter called p, which is the fraction of all potential synapses that will start off connected.
The function then randomly assigns each potential synapse with a permanence value between 0 and 1. 
If it randomly draws a number greater than or equal to p, then that potential synapse is granted permission to connect.
====SPLIT====
The SpatialPooler._initPermNonConnected function is used to initialize the permanence values of 
non-connected synapses to a value between 0 and 1. This function is called by the SpatialPooler.__init__ 
method when initializing the permanences of non-connected synapses. The exact value that a non-connected 
synapse will be initialized to depends on some factors, such as:
====SPLIT====
The SpatialPooler._initPermanence function creates the matrix of permanences for a column. The
permanence values are initialized to a random number between 0 and 1 (exclusive) such that
they all exceed the 'synPermConnected' value.  Then they are adjusted by the call to 
'SpatialPooler._updatePermanencesForColumn' which uses the connectedPermanenceDec and 
connectedPermanenceInc parameters from teh SpatialPooler class constructor along with an input vector, 
the permanence values, and one other parameter specific to each column called 'segUpdateValidDuration'.
====SPLIT====
The SpatialPooler._updateBoostFactorsGlobal function is called every time the SpatialPooler._updateBoostFactors function is called.
The _updateBoostFactorsGlobal function takes no arguments and returns nothing. 
It updates the boost factors of each column in the spatial pooler based on its current active duty cycle.
====SPLIT====
The SpatialPooler._updateBoostFactorsLocal function is the local version of 
the global _updateBoostFactors function. It takes in a single column and updates 
its boost factor based on the duty cycle before and after inhibition. The target 
activation level for each column is calculated based on its neighboring columns, 
and this information is used to update its boost factor.
====SPLIT====
The SpatialPooler._inhibitColumns function is the primary means by which a 
SpatialPooler class object inhibits columns. This function takes in an input 
matrix overlaps, where each element represents the overlap score for a given 
column (columns with more overlapping inputs have higher overlap scores), and a 
desired density of active columns. The density argument can be specified as an 
integer (e.g., 4) to indicate that approximately 4 columns should be active, or as
====SPLIT====
The SpatialPooler._inhibitColumnsGlobal function is the second part of the inhibiton phase. It takes in a list of 
overlap values and outputs a list of indices corresponding to columns whose overlap values are above or equal to their 
stimulus threshold. The function also enforces both density and stimulus threshold limits on the output set.
====SPLIT====
The SpatialPooler._inhibitColumnsLocal function takes in a list of columns that have been
overlapping and are candidates for learning. It also takes in the current overlap score for
each column as well as the density of active columns within a local inhibition area (the size 
of which is set by the user). It calculates how many active colums should be present in this 
local inhibition area and returns an array with 1s at indices corresponding to the appropriate columns.
====SPLIT====
The SpatialPooler._getColumnNeighborhood function is a helper function that returns the neighborhood of columns within
the inhibition radius. This function takes in the center column and returns all columns that are within the inhibition 
radius from it. If wrapAround is set to true, then this will return all columns in range regardless of their location 
(i.e., they could be in a different wrapping slice). Otherwise, it will only return those columns which are strictly 
within the specified radius.
====SPLIT====
The SpatialPooler._getInputNeighborhood function is a helper function that
returns the neighborhood of a given input index. It returns the indices of
inputs within the potential pool, and takes into account boundary conditions.
====SPLIT====
The Array function creates a NTA_Array&lt;T&gt; object.
====SPLIT====
The Region.getInputNames function returns a list of the names of all inputs to this region.
====SPLIT====
The Region.getOutputNames function returns a list of the names of all outputs that are currently defined for the region.
====SPLIT====
The Region.getParameter function is used to get the value of a parameter.
====SPLIT====
The Region.setParameter function is a helper function that allows the user to set
the value of a parameter in the region.  It accepts two arguments, paramName and value.
If the Region has an internal method called _setParameter(paramName,value), then this
method is used to set the parameter; otherwise, if there is an internal method called
_getParameterMethods(), which returns (setterMethod, getterMethod) for some parameter name 'paramName', then it calls:
====SPLIT====
The Network._getRegions function is a property that returns a CollectionWrapper
object containing the regions in the network. The CollectionWrapper object wraps
the collection of regions, and when elements of the collection are retrieved from it,
it will go through this function to create region objects with these properties:
====SPLIT====
The SDRClassifierRegion.writeToProto function writes the state of the SDRClassifierRegion to a protobuf.
====SPLIT====
The SDRClassifierRegion.readFromProto function reads a protobuf into the current object.
====SPLIT====
The OPFModelRunner.run function performs the following:
====SPLIT====
The OPFModelRunner.__runTaskMainLoop function is meant to be called from the
runTaskMainLoop function in opfutils.py.  This function calls the run method of
the model instance stored in self._model, and repeatedly calls self._periodic.tick()
for each iteration through a loop that processes input records (either from a file source or an online data source).  Each time this loop executes, it expects to get input record in one of two formats: 1) as a result of processing an input file source; 2) as the output of prediction from Online Prediction Framework (OPF). The OPFModelRunner class handles all cases for
====SPLIT====
The OPFModelRunner._finalize function performs final actions to close out the
running task.  Specifically, it:
====SPLIT====
The OPFModelRunner.__createModelCheckpoint function creates a model checkpoint in the NuPIC file system.
It also updates the corresponding entry in the jobs database with a reference to this checkpoint.
The model is saved as an .nta file, which contains both the model and its associated data files.
====SPLIT====
The OPFModelRunner.__deleteModelCheckpoint function is called by the OPFModelRunner.__cleanup function, which is typically invoked when an error condition is detected.  The purpose of this function is to delete any model checkpoints associated with the model that was being trained when it crashed, so that we don't think the models are still valid (i.e., because they may be in a state where they can't be restarted).
====SPLIT====
The OPFModelRunner.__getOptimizedMetricLabel function is used to determine the label of the metric that will be optimized.
The function first looks for a key in the metrics dictionary whose name matches one of several patterns, and then returns
the corresponding value from that key. The patterns are specified by self._optimizeKeyPattern, which is set in 
OPFModelRunner._configureLogging(). If no matching keys are found, an exception is raised; if more than one matching key 
is found, an exception is also raised.
====SPLIT====
The OPFModelRunner._getFieldStats function is a helper function that returns the minimum and maximum values of each field in the input data.
====SPLIT====
The OPFModelRunner._updateModelDBResults function is responsible for updating the model database with metrics and results information.
====SPLIT====
The OPFModelRunner.__checkIfBestCompletedModel function checks if the current model is the best completed model so far.
If it is, then it sets self._isBestModel to True and updates jobResults['bestValue'] with the current metric value.
It returns self._isBestModel, jobResults (the dictionary of all results from previous models), and a string representation of 
jobResults.
====SPLIT====
The OPFModelRunner._writePrediction function writes the prediction result to a file.
The name of the file is given by self._predictionLogFilePath.
If this function is called multiple times, it appends all predictions to the same output file.
====SPLIT====
The OPFModelRunner.__flushPredictionCache function writes the contents of the
prediction cache to a prediction output stream.  The function is called by
the OPFModelRunner.__writeRecordsCallback function, which is itself called by
the OPFExperiment class during its execution (see opf_experiment.py).
====SPLIT====
The OPFModelRunner.__deleteOutputCache function is responsible for ensuring that the output cache is closed and deleted.
====SPLIT====
The OPFModelRunner._initPeriodicActivities function is responsible for creating the periodic activities that are used to update model metrics in the models table and job record in the jobs table.
====SPLIT====
The OPFModelRunner.__checkCancelation function is called periodically by the OPF
   to check if the currently running model has been canceled. If it has,
   OPFModelRunner.__checkCancelation will raise an exception causing the currently
   running model to end its execution loop. This function may also be used in cases where a 
   model should dynamically adapt to early stopping; for example, if a sensor with poor 
   quality data is detected, a model may choose to prematurely exit its training loop and 
   gracefully finish training with reduced accuracy.
====SPLIT====
The OPFModelRunner.__checkMaturity function checks whether the model has matured.
====SPLIT====
The OPFModelRunner.__setAsOrphaned function is called when the model's process has been killed by the scheduler.
The OPFModelRunner.__setAsOrphaned function updates the job status to 'completed' and sets it's completion reason to 'orphaned'.
====SPLIT====
The HsState.readStateFromDB function reads the state from the database and
stores it in memory. It is called by both of the following functions:
  * hsObj.setSwarmState()
  * hsObj._cjDAO.jobGetFields()
====SPLIT====
The HsState.getFieldContributions function computes the field contributions to
the overall model error.  For each encoder in a predicted field, we compute the
percent better than random guessing (using just that encoder) and also the absolute
amount better than random guessing.  The &quot;random guess&quot; is taken as the performance of a single-field model using an encoder with matching parameters that was trained on all of the same data as this current model, but without any fields enabled for prediction (i.e. it's used for baseline/random guessing). This is so multiple models with different numbers of fields enabled don't end up having different weights due
====SPLIT====
The HsState.getAllSwarms function returns a list of swarmIds for all swarms in the given sprint.
====SPLIT====
The HsState.getCompletedSwarms function returns a list of swarmIds that have been completed.
====SPLIT====
The HsState.getCompletingSwarms function returns a list of swarmIds that are
currently in the &quot;completing&quot; state.  A swarm is in the &quot;completing&quot; state when
it has received predictions from all its expected workers and is now processing
those predictions to produce a final result.  Once a swarm transitions out of
the completing state, it becomes eligible for shutdown (if auto-expiration is on) and can be removed by calling HsState.cleanupCompletedSwarms().
====SPLIT====
The HsState.bestModelInSprint function returns the best model ID and its
errScore from the given sprint.  The best model is determined by comparing
the errScores of all models in the sprint.  If there are no models in this
sprint, then (None, numpy.inf) is returned.
====SPLIT====
The HsState.setSwarmState function is used to update the state of a running
search. It is called by the SwarmWorker class (in swarm.py) as part of the
swarm's _pollChildren() method, which in turn gets called every time one of our
swarm workers completes a job and returns its results to us. The HsState object
is passed in as an argument to SwarmWorker, and it's setSwarmState function is
called by SwarmWorker with three arguments:
====SPLIT====
The HsState.anyGoodSprintsActive function returns True if any of the sprints in the HsState.state['sprints'] list have a status of 'active'.
====SPLIT====
The HsState.isSprintCompleted function returns True if the sprint with the given index is completed, False otherwise.
If there are fewer than sprintIdx+2 elements in self._state['sprints'], then this function returns False.
====SPLIT====
The MultiEncoder.addEncoder function adds an encoder to the MultiEncoder.
====SPLIT====
The Spec.invariant function is a helper function that verifies the following
conditions:
====SPLIT====
The Spec.toDict function converts a Spec object to a dict of dicts.
====SPLIT====
The ModelChooser.updateResultsForJob function is called by the swarm implementation
   whenever it decides that a model needs to be retrained.  This function will update
   the results for all models in the job, and select either a new best model or promote
   an existing sub-optimal but previously best model.
====SPLIT====
The createEncoder function creates an encoder that will be used to convert the
raw data into SDRs. The encoder is created using the ScalarEncoder class, which
converts a scalar value into a bitmap of neurons. The parameters for this encoder
are:
====SPLIT====
The ExperimentDescriptionAPI.__validateExperimentControl function validates the experimentControl field of an
experiment description. It does this by first checking to see if the experimentControl field is present in the
description, and then it checks that each taskLabel in the tasks list is unique. The function returns nothing if these
conditions are met.
====SPLIT====
The _matchReportKeys function is used to extract the report items of interest.
It takes a list of regular expressions and returns a list of all report items that match any one
of those regular expressions.  For example, if you wanted to find all the keys in your report that contained 'time' or 'memory', you could pass it an argument like:
====SPLIT====
The _getReportItem function is a helper function that takes in a string of the form
'key:subkey:subsubkey' and returns the value at that location in the results dictionary.
For example, if itemName = 'results:test_name', then _getReportItem(itemName, results) will return 
the value for 'test_name' from within the nested dictionary structure of 'results'. This allows us to 
access values from deep within our nested data structure.
====SPLIT====
The _handleModelRunnerException function ...
====SPLIT====
The runModelGivenBaseAndParams function is responsible for:
   - Running a model given the base description file and the params.py generated by Swarm.
   - Store these results back in the models table in our database (e.g., to help with automatic analysis down the road)
====SPLIT====
The rCopy function recursively copies a dictionary.
====SPLIT====
The rApply function is a recursive function that applies a function to all
values in the dictionary.  The values are accessed by keys, and the keys form
a tuple which is passed into the function as an argument.  This allows for easy
access to deeply nested dictionaries.
====SPLIT====
The clippedObj function is a utility function for generating ReST text. It
is used to print objects in a concise way, by avoiding printing the entire
contents of long lists and dictionaries. This allows users to get an idea of
what the contents of these objects are without having to scroll through the 
entirety. The clippedObj function takes one argument, obj, which can be any 
Python object (elements from lists and dictionaries will be recursively parsed).
====SPLIT====
The loadJsonValueFromFile function loads a JSON value from the file located at inputFilePath.
====SPLIT====
The PeriodicActivityMgr.tick function is responsible for running activities that are due to be executed.
It does this by iterating over the list of activities and checking if they are due to be executed. If an activity is due, it runs it and then checks if it should be repeated or not.
====SPLIT====
The rUpdate function takes two dictionaries and recursively updates the first
with information from the second.  If a key in the second dictionary is also in
the first, and is a dictionary, then rUpdate is called on that key.  If a key in
the second dictionary is also in the first but isn't a dictonary, it replaces
whatever's there.
====SPLIT====
The dictDiffAndReport function compares two dictionaries and returns a dictionary of the differences.
The returned dictionary has three keys: inAButNotInB, inBButNotInA, and differentValues. The value of each key is a list of the corresponding values that are unique to that key's argument (the first argument if both are passed, otherwise the second).
====SPLIT====
The dictDiff function compares two dictionaries, da and db. It returns a
dictionary of the following form:
====SPLIT====
The _setRandomEncoderResolution function is used to set the resolution of a random encoder.
The function takes in an encoder from model_params.MODEL_PARAMS[&quot;modelParams&quot;][&quot;sensorParams&quot;]
and sets the resolution accordingly using the minResolution and maxValue parameters.
====SPLIT====
The HTMPredictionModelClassifierHelper.removeLabels function removes the labels specified in the labelFilter from
the anomalyLabel field of each record with record ROWID greater than or equal to start and less than end. If end is
not specified, it removes labels from records with ROWID greater than or equal to start. The function assumes that a
label should be removed if it appears in labelFilter (even if other labels appear as well), and does not re-compute 
which records are 'novel' (i.e., likely true anomalies) after removing a label.
====SPLIT====
The HTMPredictionModelClassifierHelper._addRecordToKNN function adds a record to the KNN classifier.
====SPLIT====
The HTMPredictionModelClassifierHelper._deleteRecordsFromKNN function removes the records from the KNN classifier.
====SPLIT====
The HTMPredictionModelClassifierHelper._deleteRangeFromKNN function removes the prototypes identified by a start and end index from the KNN classifier.
====SPLIT====
The HTMPredictionModelClassifierHelper._recomputeRecordFromKNN function is a helper function that computes the category of an input record by using the KNN classifier.
====SPLIT====
The HTMPredictionModelClassifierHelper._constructClassificationRecord function creates a _CLAClassificationRecord instance for the HTMPredictionModelClassifierHelper class. 
The helper class is designed to interface with the CLA Classifier in order to record 
the inputs, outputs, and parameters of its classification processes. The function first retrieves 
the active columns from the Spatial Pooler using getOutputData(&quot;bottomUpOut&quot;) on SP region. It then retrieves 
the predicted columns from the Temporal Memory using getOutputData(&quot;topDownOut&quot;). The anomaly score is calculated by taking 1 minus (number of correctly predicted cols / total number of
====SPLIT====
The HTMPredictionModelClassifierHelper.compute function is where the actual classification occurs.
The helper classifies the current record based on the saved states of the model. The algorithm for 
classification involves checking if there is a match between predicted patterns and records in 
the saved_states buffer within a certain window (self._temporal_window). If there are matches, then 
the mode classification score of those matching records is returned as this records final classifier result.
====SPLIT====
The HTMPredictionModelClassifierHelper.setAutoDetectWaitRecords function is used to set the autoDetectWaitRecords variable.
====SPLIT====
The SPRegion._allocateSpatialFDR function allocates the spatial pooler instance that will be used by the region.
====SPLIT====
The SPRegion.compute function :
   Computes the region's output for a given input. The region uses its
   spatial pooler to filter the input, and it uses its temporal memory to
   remember sequences of inputs it has seen in the past. If this is the first
   time this SPRegion has produced an output for this sequence, it will form a new 
   sequence. If not, it will append to the existing sequence. In any case, 
    after doing its pattern matching, sparse distributed representation and 
    temporal memory bookkeeping (SPRegion's compute does all three), \n\n
====SPLIT====
The SPRegion._compute function is where the actual bottom-up computation happens. It
gets called from the compute() method of the parent Region class, and is passed input
and output data. The inputs are a dictionary containing buffers for the incoming bottom-up
inputs, and other necessary arguments (e.g., whether to perform inference). The outputs are a 
dictionary into which the outputs are placed; these will include an anomaly score, among other things.
====SPLIT====
The SPRegion._initEphemerals function is called at the end of __init__ to
initialize attributes that are not stored as node parameters.  These include:
  - _spatialPoolerOutput: A 2D numpy array (numColumns x numInputs) used to store a
    &quot;compressed&quot; version of the SDR for the purpose of tracking input overlap.
====SPLIT====
The FunctionSource._cacheSequenceInfoType function is a helper function for the FunctionSource class.
It determines what type of sequence info will be returned by calls to getNextRecordDict().
The following types are supported:
====SPLIT====
The _getTPClass function returns the class corresponding to the given
temporalImp string.  This allows us to dynamically instantiate different types of
temporal memory classes, each using a different C++ or Python implementation.
====SPLIT====
The _buildArgs function takes a function and creates a dictionary of arguments
that the function can take as input.  For example, if I define a function:
====SPLIT====
The TMRegion._compute function computes the output of the TMRegion.
It is called from within the TMRegion.compute() method, which is called from 
within the nupic.regions.tm_region._TMRegion.execute method, which is called in 
turn by the nupic engine during execution of a network (for example, during 
network learning). The compute function accepts an input vector and computes an 
output vector that gets stored in one or more output slots of this region node:
====SPLIT====
The TMRegion.finishLearning function is called at the end of learning.
It makes sure that all segments are marked as &quot;clean&quot;, i.e. with
the &quot;learning&quot; bit set to zero, which causes segments to be eligible for
inference or for segment deletion (if they have no synapses). This function also resets the sequence states so that a new sequence can begin being learned.
====SPLIT====
The computeRawAnomalyScore function takes in a set of active columns and previous predicted columns,
and returns the raw anomaly score. The raw anomaly score is the percent of active columns that were NOT 
predicted. In this function, we calculate how many total number of predictions were made for each time step 
(total number of activeColumns). We also keep track of how many correct predictions (predictedActiveColumns) 
were made for each time step. The raw anomaly score is then calculated by subtracting the correct prediction 
percentage from 1.
====SPLIT====
The Anomaly.compute function takes in a value and computes the anomaly score
based on the current model state. The function can return either a single
anomaly score or an array of anomaly scores, one for each individual element in
the input value. If using raw mode, the returned array contains one element 
per active bit in column SDR. In likelihood mode, it contains one element per 
input record.
====SPLIT====
The Plot.addGraph function adds a graph to the Plot object.
It takes as input an array of data and an optional position argument (an integer) which specifies where on the plot the graph will be added. 
The default value is 111, which places it in the top right corner of all three graphs.
====SPLIT====
The Plot.addHistogram function adds a histogram to the plot.
====SPLIT====
The Plot.add2DArray function adds a 2D array to the plot.
====SPLIT====
The Plot._addBase function adds a new subplot to the figure.
It takes as input an integer position and optional strings for xlabel and ylabel.
The function returns the newly created axes object.
====SPLIT====
The getVersion function returns the version number of the software.
  
  :returns: The version number of the software.
  :rtype: string
====SPLIT====
The nupicBindingsPrereleaseInstalled function checks whether a pre-release version of nupic.bindings is installed.
If so, it returns True; otherwise, it returns False.
====SPLIT====
The findRequirements function finds the requirements.txt file in the repository
and parses it into a list of strings.  It then checks to see if there is an
optional nupic.bindings-0.3.* egg available for installation on this system and
if so removes it from the requirements list, since this is a development only
package that should not be installed on end-user systems.
====SPLIT====
The _indentLines function indents all but the first line of a multi-line string
by an amount specified by indentLevels.  The first line is not indented.
The _indentLines function returns the resulting string.
====SPLIT====
The _generateMetricSpecString function generates a string that can be used to
recreate the metric specified by the caller.  The returned string is a class-name
and arguments, as needed, for instantiating an object of that class.  For example,
for a MetricSpec object with all default arguments:
====SPLIT====
The _generateFileFromTemplates function takes a list of template files, opens them,
and writes the lines to the output file.  The templates are read from &lt;installPath&gt;/templates.
The replacementDict is used to replace any instances of keys in the templates with values.
====SPLIT====
The _getPropertyValue function is used to get the value of a property from the options dictionary.
If no value exists in the options dictionary, then it will attempt to use a default value if one exists.
If no default was specified, then an error will be thrown.
====SPLIT====
The _getExperimentDescriptionSchema function returns a Python dictionary containing the experiment
description schema.  This is used by the validateExperimentDescription function to determine whether
a given experiment description object contains all of the necessary fields and only those fields.
====SPLIT====
The _generateExtraMetricSpecs function generates the metric spec string for each
extra metric specified in the metrics configuration file.  For example, if the
user configures to use two extra metrics, this function will generate strings of
the form:
====SPLIT====
The _getPredictedField function returns the name of the predicted field and its type.
====SPLIT====
The _generateInferenceArgs function generates the inferenceArgs string to be
   included in the generated model.py file.  This function uses a single input,
   options['inferenceArgs'], which is a dictionary of arguments used to configure
   an instance of OPFDynamicModelHelper().
====SPLIT====
The expGenerator function generates a description.py and permutations.py file
for the caller script to use for running an experiment using OPF RunExperiment.
The expGenerator function accepts a JSON-formatted string that specifies the
experiment to generate the description.py and permutations files for, as well as
the name of an output directory where it will place those generated files:
====SPLIT====
The parseTimestamp function parses a timestamp string into a datetime object.
The supported formats are:
  * %Y-%m-%dT%H:%M:%S.000Z (e.g., 2011-01-24T09:00:00.000Z)
  * %Y/%m/%d %H:%M (e.g., 2011/01/24 09:00)
====SPLIT====
The parseBool function converts a string to a boolean value. 
  
  * If the string is &quot;true&quot; or &quot;t&quot;, it returns True.
  * If the string is &quot;false&quot; or &quot;f&quot;, it returns False.
====SPLIT====
The unescape function is used to undo the escaping of special characters
in a string.  The following character mappings are applied:
====SPLIT====
The parseSdr function takes a string of 0's and 1's as input, and returns
a list of integers. The function raises an exception if the provided string
contains any characters other than 0 or 1.
====SPLIT====
The parseStringList function parses a string of space-separated integers into a list of integers.
For example, parseStringList('2 3 4') returns [2,3,4].
====SPLIT====
The coordinatesFromIndex function takes an index and a list of dimensions,
and returns the corresponding coordinates in the space. For example, if we have
a 2D space with dimensions [3, 3], then:
====SPLIT====
The indexFromCoordinates function takes a list of coordinates and a list of dimensions.
It returns the index that corresponds to the given coordinate in an array with those dimensions.
For example, if you have an array with 2 rows and 3 columns, 
the indices for each element will be 0-5.  If you pass [0, 1] as coordinates and [2, 3] as dimensions, 
then this function will return 4 because there are 4 elements in the first row (0-3) and 5 elements in the second row (0-4).
====SPLIT====
The neighborhood function returns the coordinates of all cells within a given radius of the center cell.
====SPLIT====
The CoordinateEncoder._neighbors function takes a coordinate and a radius,
and returns all the coordinates that are within the radius of the specified
coordinate.  For example:
====SPLIT====
The CoordinateEncoder._topWCoordinates function takes a list of coordinates and returns the top w
coordinates by order.  For example, if there are 12 coordinates, and w is set to 5, then the 5
coordinates with the highest orders will be returned.  If w is set to more than 12, then all coordinates
will be returned.
====SPLIT====
The CoordinateEncoder._hashCoordinate function converts a tuple of integers (or list of integers) into
a 64-bit integer. This is accomplished by converting the 64-bit integer to a 128-bit string, and then taking
the first 64 bits as the hash value.
====SPLIT====
The CoordinateEncoder._orderForCoordinate function generates a random number for each 
coordinate in the input vector. These are used to determine the order of the coordinates 
in the bit-encoding, which must be consistent across all training iterations. This is 
necessary because serializing and deserializing vectors requires encoding information.
====SPLIT====
The CoordinateEncoder._bitForCoordinate function is a helper function that takes in a coordinate and returns the
index of the bit at that coordinate. For example, if you pass in (0, 0) as your coordinate, it will return 0. If you 
pass in (3, 2), it will return 7.
====SPLIT====
The binSearch function takes in an array and a value, then returns the index of the value if it is found in the array. If not, it returns - 1.
====SPLIT====
The Connections.createSegment function creates a new Segment on the cell.
====SPLIT====
The Connections.destroySegment function removes a Segment from the list of
Segments for a Cell. It also:
  - decrements _numSynapses, where synapse is an element of the segment's _synapses list.
  - decrements self._numSynapses, where synapse is an element of the segment's _synapses list.
  - removes all elements from segment._synapses that are currently in self._synapsesDistal or self._segmentForFlatIdx (but not those in self._newSynapseCountForSegment). This might leave some entries in _segmentForFlat
====SPLIT====
The Connections.createSynapse function creates a new synapse on the specified segment, 
   with the provided permanence. If a synapse already exists in that location, it will be replaced. 
   The presynapticCell is also recorded in the synapse and used to update self._synapsesForPresynapticCell.
====SPLIT====
The Connections.destroySynapse function removes a synapse from the list of synapses in the segment.
It also removes that segment from the presynaptic map for this synapse.
====SPLIT====
The Connections.computeActivity function computes the activity of each segment.
====SPLIT====
The Connections.numSegments function returns the number of segments in a cell.
====SPLIT====
The Connections.read function reads serialized data from a file and creates
a Connections instance containing that data. The Connections instance is returned.
====SPLIT====
The Configuration.getString function is a helper function that reads the
configuration property value from the configuration file.  It first checks
the environment variable '&lt;CONFIG_PREFIX&gt;&lt;PROPERTY&gt;' for an override value, and if none exists, it checks the configuration file.  The environment variable allows values to be overridden without having to modify the configuration file.
====SPLIT====
The Configuration.getBool function is a helper function that converts the value of a configuration property
into a boolean.  It accepts two values: 0 and 1.  Any other value will raise an exception.
====SPLIT====
The Configuration.set function is a class method that allows you to set the value of a property.
====SPLIT====
The Configuration.dict function returns a dictionary of the properties that have been
configured for this application.  The dictionary is created by reading from the following sources:
====SPLIT====
The Configuration.readConfigFile function reads a JSON file containing configuration parameters and their values.
The function returns a dictionary of the form {parameter_name: parameter_value}.
====SPLIT====
The Configuration.getConfigPaths function returns a list of paths that are searched to find the configuration files.
The NTA_CONF_PATH environment variable is used to determine where to search for the conf files. If NTA_CONF_PATH is not defined,
then it will use a default set of directories: ../conf/default, ../conf and ./conf
====SPLIT====
The addNoise function adds noise to the input image.
====SPLIT====
The generateCoincMatrix function creates a sparse matrix of nCoinc rows and
length columns, with nCoinc non-zero elements.  The non-zeros are evenly
distributed throughout the columns, and are values 1.0f in this example.  The
result is returned as an SM32 matrix (all input parameters default to 10).
====SPLIT====
The generateVectors function creates a list of vectors, each vector is
a binary array with the length specified by the user. The number of 1's in each
vector is approximately equal to the activity parameter. The function also returns
the number of 1's in each vector as well for further analysis.
====SPLIT====
The generateSimpleSequences function generates a list of sequences. 
Each sequence is a list of integers between 0 and 9, inclusive. The length of the sequence is determined by sampling from the distribution defined by seqLengths (which can be passed in as an argument to generateSimpleSequences). If max(seqLength) &lt;= nCoinc then we sample from the uniform distribution on [0,nCoinc], otherwise we sample each element independently with replacement from [0,nCoinc]. We return a list containing all sequences.
====SPLIT====
The generateHubSequences function generates a list of sequences where the 
center element is a hub element. The hub elements are randomly chosen from the 
list of hubs passed in as an argument. The sequence length is randomly chosen 
from the list of sequence lengths passed in as an argument. Each generated sequence 
is appended to a returned list.
====SPLIT====
The generateSimpleCoincMatrix function generates a coincidence matrix.
====SPLIT====
The generateSequences function creates two lists: a list of sequences and a
list of patterns. The function returns these as tuple, where the first element
is the sequence list and the second is the pattern list. Each item in each
list is a dictionary with keys 't' (the time step at which it was presented)
and 'patterns' (an array containing nonzero indices for each coincidence).
====SPLIT====
The sameTMParams function compares two TM instances to see if they have the same
   parameters. This is useful for checking that a saved and loaded TM has the same
   parameters.
====SPLIT====
The sameSegment function takes two sequences of segments and compares them to
see if they are identical. It ignores order of segments, differences in the number
of synapses, and any missing or extra synapses. If all these things are equal, then
the sameSegment function returns True; otherwise it returns False.
====SPLIT====
The spDiff function compares two spatial poolers, SP_A and SP_B. It does this by comparing the
   Connections Matrices, Permanence Matrices, Potential Connection Matrices and firing boosts between
   the two poolers.
====SPLIT====
The _accumulateFrequencyCounts function accepts a vector of values and returns a vector
of counts.  The returned vector is &quot;dense&quot;, meaning that each value from 0 to the largest
value in the input array is included.  If there are no occurrences of a particular value, its count
is zero.
====SPLIT====
The _fillInOnTimes function fills in the durations of all on bits.
====SPLIT====
The averageOnTimePerTimestep function computes the average on-time of each
column that was active during a timestep.  The function takes two parameters:
vectors, which is a numpy array containing the output activity from a spatial pooler
and numSamples, which is the number of columns to test.  If numSamples is None then all
the columns will be tested.
====SPLIT====
The averageOnTime function computes the average on-time, the frequency counts
of each on-time encountered, and returns them.  The average on time is computed
by dividing the total length of time for which any element was &quot;ON&quot; by the total
number of elements that were ever &quot;ON&quot;.  The frequency counts are a list of pairs
containing each on-time duration found (expressed in milliseconds) and its count.
====SPLIT====
The plotHistogram function plots a histogram of the on-times.
====SPLIT====
The populationStability function calculates the percentage of bits that match between two consecutive vectors.
The population stability is calculated by taking a random sample of vectors from the set and calculating their average
similarity. The similarity is defined as the number of matching bits divided by total number of bits in both vectors. 
This process is repeated multiple times to get an accurate representation of how similar each pair are to one another on average.
====SPLIT====
The percentOutputsStableOverNTimeSteps function calculates the percentage of outputs that remain
stable over N time steps.  The function takes a list of vectors as input, where each vector represents
the active elements in the output layer at a given timestep.  For each window of N time steps, this function
calculates whether any element flips states (i.e., becomes active on one timestep and inactive on another).  
If no elements flip states within the window, then it is assumed that all elements have remained stable -- hence 
the percentage is calculated as 100 * (number of stable windows / total number of possible windows).
====SPLIT====
The computeSaturationLevels function computes the saturation levels for each column of
   cells.  It takes as input a vector of outputs from our HTM network and returns a list
   containing two items:
====SPLIT====
The checkMatch function compares the prediction to the input and returns four
values:
  1. foundInInput: The number of elements in the prediction that are also in the input.
  2. totalActiveInInput: The total number of active elements in the input, including those not considered by checkMatch (e.g., inactive bits).
  3. missingFromInput: The number of elements in the prediction that are not present at all in the input (i.e., they're missing from both it and from checkMatch's perspective). This is always zero when sparse=True, since sparse vectors only have active bits by
====SPLIT====
The getCentreAndSpreadOffsets function computes the center and spread offsets
for a space.  The center offsets are the coordinates where the centers of
hypotheses will be placed in the space.  The spread offsets determine
the spreading of each hypothesis, so that they are not too close together,
and so that they do not overlap other hypotheses.
====SPLIT====
The makeCloneMap function creates a 2D numpy array that serves as a map
from an input column to its corresponding output columns.  The output columns
are created by splitting the input column spatially.  For example, suppose you
have an input of width 5 and you want to create 3 output columns of width 2,
like so:
====SPLIT====
The numpyStr function takes a numpy array and returns a string representation of it.
====SPLIT====
The DiscreteDistribution.sample function takes a random number generator and returns a key from the
DiscreteDistribution object. The function uses the cdf to find an index in the keys array, then returns
the corresponding key.
====SPLIT====
The MultinomialDistribution.logProbability function computes the log probability of a given distribution.
The function takes as input a list or array representing the distribution, and returns the log probability of that
distribution.
====SPLIT====
The PoissonDistribution.sample function generates a random sample from the Poisson distribution.
The function takes one argument, rgen, which is an instance of numpy's RandomState class.
It returns a tuple containing two elements: x and logDensity(x).  The first element, x, is the random sample generated by calling rgen.poisson().  The second element is the log density corresponding to this sample.
====SPLIT====
The createDataOutLink function creates a link between the sensor region and the region
that is to be fed bottom-up. The function takes three arguments: network, sensorRegionName,
and regionName. It returns nothing.
====SPLIT====
The createSensorToClassifierLinks function creates a link between the sensor region and the classifier region.
The link is created by linking the bucketIdxOut, actValueOut, categoryOut outputs of the sensor to their corresponding inputs in
the classifier. The function also links all other outputs from the sensor to their corresponding inputs in 
the classifier.
====SPLIT====
The createNetwork function creates a network that has the following structure:
====SPLIT====
The getPredictionResults function returns a dictionary of dictionaries.
The top level keys are prediction steps and the values are themselves dictionaries
containing the predicted category label and prediction confidence for that label.
For example: {'t-0': {'predictedValue': 'C', 'predictionConfidence': 0.85}, 
't-2': {'predictedValue': 'D', 'predictionConfidence': 0.7}}
====SPLIT====
The runHotgym function runs the Hot Gym example network (consisting of a sensor region reading data from a
FileRecordStream, with inference enabled on a temporal memory region and then classification performed on the
temporal memory's active cells). It returns an array of results, one per iteration. Each result consists of four
fields:
====SPLIT====
The OPFDummyModelRunner._loadDummyModelParameters function loads the model parameters from a dictionary of parameters.
====SPLIT====
The OPFDummyModelRunner._getMetrics function is a private function that is called by the OPFDummyModelRunner.run
method to return a dictionary of metric names and values for the current record being processed by OPFDummyModelRunner.
The default implementation returns {self._optimizeKeyPattern:self.metricValue}, where self._optimizeKeyPattern is 
the key pattern specified by the user in the custom model's params dict (or in this case, hard-coded), and 
self.metricValue = self._currentRecordIndex+ 1, which means that if you run your dummy model with --numRecords=1000,
====SPLIT====
The OPFDummyModelRunner.__shouldSysExit function is a helper function that's called periodically
from the OPFDummyModelRunner.run method to determine whether or not it's time to exit.  This determination
is made based on whether or not we've been asked to exit after a certain number of jobs have completed, and
whether or not the current model is one of those that have completed the requisite number of jobs.
====SPLIT====
The DataGenerator.getDescription function returns a dictionary containing the name of the DataGenerator,
the names of its fields, and the number of records in each field. This function is used to generate a 
description file for use with other functions in this module.
====SPLIT====
The DataGenerator.generateRecords function generates a number of records,
and then writes them to the output file.  It does this by calling the DataGenerator.generateRecord function on each record in turn.
====SPLIT====
The DataGenerator.getRecord function returns a list of values from the fields in the order they are listed.
If no argument is given, it returns the last record in each field.
====SPLIT====
The DataGenerator.getAllRecords function returns a list of all records in the dataset.
The function takes no arguments and returns a list of lists, where each sublist is one record.
====SPLIT====
The DataGenerator.addValuesToField function adds a list of values to the specified field.
The function returns the list of values added to that field.
====SPLIT====
The DataGenerator.getSDRforValue function returns the SDR for a given value in a given field.
The function takes two arguments: i and j, which are integers representing the index of the field and record, respectively.
====SPLIT====
The DataGenerator.getZeroedOutEncoding function returns a zeroed out encoding for the specified
record number.  This is used in cases where we are generating predictions on the same data that
was used to train/optimize/etc.  In these cases, we want to return an encoding of all zeros, since
the prediction will be made using the raw value from that record number.  The DataGenerator class
uses this function when it needs to generate predictions on records whose encodings were generated by
the getEncoding() function (which includes random noise).   For example:
====SPLIT====
The DataGenerator.getTotaln function sums the number of data points in each field and returns that value.
====SPLIT====
The DataGenerator.getTotalw function sums the widths of all fields in the DataGenerator.fields list and returns
the result.
====SPLIT====
The DataGenerator.getEncoding function takes in a DataGenerator object and an integer n.
It then concatenates the encodings of all fields in the DataGenerator object, 
and returns this encoding as a 1D numpy array.
====SPLIT====
The DataGenerator.getAllEncodings function returns a list of all the encodings that are present in the dataset.
The DataGenerator class is used to generate batches of data for training and testing neural networks. 
This function is useful when you want to visualize what an encoding looks like, or use it as input into other functions.
====SPLIT====
The DataGenerator.saveRecords function saves the records in a CSV file.
The first row of the CSV file contains all field names, separated by commas.
The second row of the CSV file contains all data types (e.g., string or float) for each field, separated by commas.
The third row of the CSV file contains flags indicating whether each field is categorical (0) or numeric (2). 
All subsequent rows contain one record per line, with fields and values separated by commas.
====SPLIT====
The DataGenerator.removeAllRecords function removes all records from the DataGenerator object.
====SPLIT====
The _field.encodeValue function encodes the value of a field using the
field's encoder. If toBeAdded is True, then it also adds the encoded value to
the field's list of encoded values and increments numEncodings accordingly. It
returns an array containing one or more floats representing each encoding.
====SPLIT====
The _field._setTypes function is called by the _field._setEncoderAttrs function.
It sets the following attributes of the field:
  - dataType: The type of data that will be used for training and inference. This is
    based on each field's encoderType attribute, which was either specified in the model's
    params file, or autodetected by a call to self._getDataTypes(). If it was specified in
    the model's params file, then we don't change it; only if it wasn't specified do we set a value here.
====SPLIT====
The _field._initializeEncoders function is called by the _field._initialize function.
It takes in a dictionary of encoder specifications and creates an instance of the appropriate encoder class for each field.
The function returns a list containing all the encoders created, as well as updates self._encodersDict with key:value pairs where 
key=fieldName and value=listOfEncoderInstancesForThatField.
====SPLIT====
The loadExperiment function loads an experiment description script from the given path,
and returns a tuple of (modelDescription, modelControl) objects.
====SPLIT====
The loadExperimentDescriptionScriptFromDir function loads the experiment description script from the given directory.
====SPLIT====
The _loadDescriptionFile function imports the experiment description module
   and returns a handle to it.
====SPLIT====
The ResultsDB.getModelIDFromParamsHash function returns the modelID of a
model that has the same parameters as those specified in paramsHash.  If no such
model exists, it returns None.  This function is useful when you have a set of
parameters for which you want to know what modelID was assigned to it (for use in
the ResultsDB.getModelFields and ResultsDB.getModels functions).  For example:
====SPLIT====
The ResultsDB.bestModelIdAndErrScore function returns the model ID and score of the best model
in a swarm. If no swarm is specified, it will return the global best. If no generation is specified,
it will return the best model in all generations up to and including those that have been completed.
====SPLIT====
The ResultsDB.getParticleInfo function returns a tuple containing the particle state, model ID,
error score, whether or not the model has completed running and whether or not it is considered mature.
====SPLIT====
The ResultsDB.getParticleInfos function returns a list of particle states,
model IDs, and error scores for all particles in the given swarm. The function
takes four optional keyword arguments:
====SPLIT====
The ResultsDB.getOrphanParticleInfos function returns a tuple of lists.
The first list contains particleStates, the second list contains modelIds,
the third list contains errScores, the fourth list contains completedFlags and
the fifth list contain maturedFlags. The particleStates are those particles that have been generated by swarmId but are not yet associated with any models in ModelDB. The modelIds correspond to these particles and indicate which models would be created if these orphaned particles were to be instantiated as real models in ModelDB right now (i.e., they have no corresponding entry in ModelDB). The errScores indicate what error score is
====SPLIT====
The ResultsDB.firstNonFullGeneration function returns the generation number of the first
generation in which there were fewer than minNumParticles particles evaluated. If all generations
have at least minNumParticles, then it returns None.  The swarmId parameter is a string that uniquely identifies a swarm within a given experiment.
====SPLIT====
The ResultsDB.getResultsPerChoice function returns a dictionary of the form:
{'0.5': (0.5, [err_for_particle_with_position=0.5]), '2': (2, [err_for_particle_with__position=2])}
where each key is a string representation of the position for that particle and 
the value is a tuple containing the position and list of errors for particles with that position
====SPLIT====
The HypersearchV2._getStreamDef function generates a StreamDef object that
is used to specify the stream definition for the experiment. It is responsible
for generating aggregated fields where necessary. Aggregation can be specified
either by providing an aggregation dict in the model description, or by using a 
custom function through the aggFunctionsDict parameter. The resulting streamDef 
object will look something like this:
====SPLIT====
The HypersearchV2._okToExit function is meant to be used in conjunction with a periodic
status message printed by the HypersearchWorkerV2 instance. For example, you could have a
HypersearchWorkerV2 instance that prints &quot;I'm still alive&quot; every 30 seconds:
====SPLIT====
The HypersearchV2.recordModelProgress function is called by the engine each time a
model reports progress during the Hypersearch. The engine will call this function up to
three times per model, once each for:
====SPLIT====
The HypersearchV2.runModel function runs a single model, which has been
scheduled by the HypersearchWorker and will contain a  set of parameters.
The HypersearchV2.runModel function will:
====SPLIT====
The _engineServicesRunning function is used to determine whether there is a running
instance of the Grok engine.  Specifically, it returns True if there is a client job manager
running on this node and the CJM has registered with the nupic-engines service.  If so, then
the engine will be available for use by workers; otherwise, when trying to run work items on
workers that use an engine, an exception will be thrown notifying the user that first they need to start up their engines.
====SPLIT====
The runWithJsonFile function is a convenience function that runs an experiment
with the given JSON file.  The JSON file contains all of the parameters for
the experiment as well as which results files should be generated and which
environment script should be run.  See :ref:`result-format` for details on how to specify results files.
====SPLIT====
The runWithPermutationsScript function runs a canned permutations script.
====SPLIT====
The _backupFile function copies the given file to a new path, appending
a timestamp to the original filename.  For example, if the given filePath is
&quot;/path/to/file.txt&quot;, and no file exists at &quot;/path/to/file.txt.20150901_030405&quot;,
the _backupFile function will create that backup file with contents identical
to &quot;/path/to/file.txt&quot;.  If a file already exists at &quot;/path/to/.20150901_030405&quot;, then it will try again by appending a timestamp to the basename of the given path: &quot;./
====SPLIT====
The _iterModels function returns an iterator that can be used to
iterate over the list of models specified by modelIDs. The
_iterModels function accepts a single argument, `modelIDs`, which is a sequence
of Nupic model identifiers for which the iterator will return _NupicModelInfo instances. Note that:
====SPLIT====
The _HyperSearchRunner._launchWorkers function creates a number of
worker processes equal to the 'numWorkers' parameter, and then launches
one worker process for each value of the permutations_per_worker 
parameter. For example, if numWorkers is 10 and permutations_per_worker 
is 1000, then _HyperSearchRunner._launchWorkers will create 10 worker 
processes for each of the 1000 values in the hyperparameter search space.
====SPLIT====
The _HyperSearchRunner.__startSearch function creates a HyperSearch job by inserting
   it into the jobs table in the db. The following are also performed:
====SPLIT====
The _HyperSearchRunner.loadSavedHyperSearchJob function creates a _HyperSearchJob instance from the given
permWorkDir and outputLabel. It then uses the saved HyperSearch job ID to load in all of the search results, which
it returns as a list of HyperSearch instances. This function is used by clients that are running Hypersearches on
multiple Nupic engines and want to combine all of their results into one set.
====SPLIT====
The _HyperSearchRunner.__saveHyperSearchJobID function saves the given
hyperSearchJob object under the given filePath.  It backs up any pre-existing
file at that path before writing out the new one.
====SPLIT====
The _HyperSearchRunner.__loadHyperSearchJobID function is responsible for loading the
HyperSearch job ID from a file named 'hypersearch_job_id.pkl' in the model's output
directory, which is typically located at something like:
====SPLIT====
The _HyperSearchRunner.__getHyperSearchJobIDFilePath function is responsible for
creating the path to the file that will contain a single integer value, which
represents the ID of a HyperSearch job.  This function is called by both of the
_HyperSearchRunner.__runPermutations helper functions, but neither of them are
allowed to run if this function returns None.  If this function returns None as
the result, it indicates that HyperSearch has been disabled for the given test;
and in that case _runPermutations should skip all other processing and return 0.
====SPLIT====
The _ReportCSVWriter.emit function writes a single line to the report.csv file
using the following format:
&lt;jobID&gt;, &lt;modelID&gt;, &lt;status&gt;, &lt;completionReason&gt;, &lt;startTime&gt;, &lt;endTime&gt;
&lt;NNModelParamsName0&gt;&lt;value0&gt;|...|&lt;NNModelParamsNameN&gt;&lt;valueN&gt;| |...| |\n
====SPLIT====
The _HyperSearchJob.queryModelIDs function is used to get the modelIDs of all models that
have been saved to the models table in the HyperSearch database.  It does NOT include
models that are currently being run by workers or incomplete models.  The modelIDs can be used
to retrieve additional information about each model through other API functions.
====SPLIT====
The _PermutationUtils.getOptimizationMetricInfo function returns a JSON object
containing the optimization key name and other info that is useful when creating
a hypersearch. For example, if you want to know how the values of an optimization
metric are computed, such as the mean or min value over all permutations for that metric, this function can tell you.  It also returns other info about whether each metric is maximized or minimized, etc...
====SPLIT====
The _NupicModelInfo.getAllMetrics function combines the metrics returned by
the getReportMetrics and getOptimizationMetrics functions into a single dict.
It returns this dict, converting the keys from bytes to str objects as necessary.
====SPLIT====
The Distributions.getData function returns a list of n records from the data set.
   The function will return an empty list if there are not enough records in the data set.
====SPLIT====
The ModelTerminator.getTerminationCallbacks function is a helper function that returns a list of PeriodicActivityRequest objects.
The ModelTerminator class uses this list to schedule termination activities for each iteration.  The purpose of these activities is to determine if the model has reached an end state, such as convergence or maximum allowed iterations.
Each activity in the returned list calls the provided terminationFunc when triggered and provides two parameters: iteration and index.  
iteration - The current model iteration number (starts at 0).  This parameter can be used by the provided terminationFunc to determine if enough iterations have completed to reach an end state.  
index -
====SPLIT====
The StreamReader.getDataRowCount function returns the number of rows in a stream.
====SPLIT====
The PatternMachine.get function returns a Pattern object with the given number.
====SPLIT====
The PatternMachine.addNoise function adds noise to the input bits.
====SPLIT====
The PatternMachine.numbersForBit function returns a set of all the numbers that have a 1 in the bit'th position.
====SPLIT====
The PatternMachine.numberMapForBits function takes a list of bits and returns a dictionary
where the keys are numbers and the values are sets of bits that correspond to those numbers.
For example, if we pass in [0, 1] as our bit list then we get back {0: set([0]), 1: set([-])}.
This is useful for determining which patterns will be active when you present them to an agent.
====SPLIT====
The PatternMachine.prettyPrintPattern function takes a pattern and returns a string that
describes the pattern. The string is intended to be human readable rather than machine
readable. It shows which bits are set in the input, along with some other information:
====SPLIT====
The PatternMachine._generate function generates a set of unique patterns for the PatternMachine.
====SPLIT====
The PatternMachine._getW function is a helper function that returns the weight of a randomly selected
neuron in the pattern. If there are multiple neurons in the pattern, it will return one at random. 
If there is only one neuron, then it will always return that neuron's weight.
====SPLIT====
The ConsecutivePatternMachine._generate function generates a set of patterns for the ConsecutivePatternMachine.
The function takes in an integer n and an integer w, where n is the number of elements in each pattern and w is 
the length of each pattern. The function then creates a dictionary with keys ranging from 0 to (n/w) - 1, which 
represents all possible patterns that can be generated by this machine. Each key maps to another dictionary that has 
two keys: 'pattern' and 'count'. The value corresponding to the key 'pattern' maps to a list containing all unique 
permutations of size w found
====SPLIT====
The SDRClassifier.inferSingleStep function is a helper function that takes in a single input pattern and outputs the
predicted distribution of classes. This function is called by SDRClassifier.infer, which takes in an array of patterns and 
outputs the predicted classifications for each one.
====SPLIT====
The SDRClassifier._calculateError function calculates the error for each bucket index in the targetDistribution.
The function returns a dictionary of errors, where the key is number of steps and value is an array of errors.
====SPLIT====
The sort function takes a record stream and sorts it according to the
specified key. The key is specified as a list of field names, which can be
the field's name or its index. For example, if we want to sort the file by
the 'date' and thenby 'priority' fields we would call:
====SPLIT====
The _sortChunk function takes a list of records and sorts them by the given
key. If a chunkIndex is provided, it also writes the sorted records to a file
called 'chunk_&lt;index&gt;.csv'. This function does not return anything.
====SPLIT====
The _mergeFiles function takes a list of files and merges them into one file.
The function will take the first two files in the list, merge them, and store
the result in 'outputFile'. The function will then remove the first two files
and append 'outputFile' to the end of _mergeFiles. This process is repeated until 
only one file remains. At this point, all of the contents are merged with 
_mergeFiles.
====SPLIT====
The TemporalMemoryShim.compute function is a wrapper around the
TemporalMemory.compute function that handles several things:
====SPLIT====
The ConsolePrinterMixin.cPrint function prints a message to the console, but only if
the verbosity level is high enough.  The message can be printed along with some arguments
that will be expanded using the string.format method.  If there are more than one argument,
the string must contain a matching number of placeholders (i.e., {}).  The cPrint function also
takes an optional keyword argument that allows you to print the message on its own line or not:
====SPLIT====
The GeospatialCoordinateEncoder.coordinateForPosition function takes a longitude, latitude, and altitude
and returns an integer representing the coordinate in the GeospatialCoordinateEncoder's encoding space.
====SPLIT====
The GeospatialCoordinateEncoder.radiusForSpeed function computes the radius of a circle that will fully contain a given speed at a given timestep.
====SPLIT====
The Serializable.readFromFile function reads a serialized object from disk and returns an instance of the first-class class initialized using the data read.
====SPLIT====
The Serializable.writeToFile function writes the Serializable instance to a file.
====SPLIT====
The requireAnomalyModel function is a decorator that checks if the model
   instance has an anomaly classifier. If it does not, then it raises a RuntimeError exception.
====SPLIT====
The HTMPredictionModel.anomalyRemoveLabels function removes the anomaly labels from the HTMPredictionModel
between start and end times.  The labelFilter is a list of strings that are either in or not in the label set.
The filter must be exclusive (either all strings must be present or absent) and match exactly with what is 
in the model's current labelset.
====SPLIT====
The HTMPredictionModel.anomalyAddLabel function adds a label to the anomaly classifier.
====SPLIT====
The HTMPredictionModel.anomalyGetLabels function returns a list of labels
for the input metric.  The list is comprised of elements, one for each time
step from start to end.  Each element is a dictionary whose keys are the labels and whose values are numpy arrays with one element per anomaly score at that time step and label.
====SPLIT====
The HTMPredictionModel._anomalyCompute function computes the anomaly score
and adds it to the results dictionary.  The function performs two tasks:
====SPLIT====
The HTMPredictionModel._removeUnlikelyPredictions function removes predictions with likelihoods less than
the minimum likelihood threshold.  It also limits the number of predictions to include in a prediction set.
The maxPredictionsPerStep parameter determines how many predictions are allowed per step, and the minLikelihoodThreshold
determines what that minimum threshold is for including a prediction (e.g., if you want to include any prediction at all, then this would be 0).  The function returns a dictionary mapping input IDs to their associated likelihoods.
====SPLIT====
The HTMPredictionModel._getClassifierRegion function returns the classifier region of the network.
If no such region exists, it returns None.
====SPLIT====
The HTMPredictionModel._addAnomalyClassifierRegion function adds a KNN Anomaly Classifier Region to the network.
====SPLIT====
The PermuteChoices.setResultsPerChoice function is used to set the results obtained for each choice.
The function takes a list of tuples, where each tuple contains a choice value and the corresponding values obtained from running that choice.
For example: [(choiceValue_0, [value_00, value_01]), (choiceValue_0, [value_10])] would indicate that two runs were performed with 
the first choice having values 0 and 1 respectively; while one run was performed with the second choice having value 0.
====SPLIT====
The BasicPredictionMetricsLogger._translateMetricsToJSON function converts the metrics object into a JSON-compatible string.
====SPLIT====
The _BasicPredictionWriter.setLoggedMetrics function is used to set the list of metric names that will be written
to the prediction log.  The default behavior is to write all logged metrics, but this can be changed by calling
_BasicPredictionWriter.setLoggedMetrics with a list of strings specifying which metrics should be written.
====SPLIT====
The _BasicPredictionWriter.__getDictMetaInfo function generates a list of FieldMetaInfo objects describing the
inferenceDict.  The inference dictionary is expected to be a dict with string keys and integer values.
The FieldMetaInfo type can describe either an int or string field, so the _BasicPredictionWriter function determines this
from the data itself, for each key in the dict.
====SPLIT====
The _FileUtils.createExperimentInferenceDir function creates a directory for the inference data of an experiment.
====SPLIT====
The _allow_new_attributes function is a decorator that wraps around the
__init__() and __setstate__() methods of an object.  It allows attributes to be
added to the object after instantiation, but does not keep track of how many times it has been called.  This means that if you add an attribute in __setstate__(), then delete it in __init__(), then create another attribute, your new attribute will not be tracked correctly.
====SPLIT====
The generateRandomInput function creates a list of input vectors to be used in training.
The inputs are numpy arrays, and are created such that each vector has exactly 42 ones (true)
and then 100-42 zeroes (false). This is done by creating an array of length 400 with all zeroes,
and then setting 42 random elements to 1. The function returns the list of input vectors.
====SPLIT====
The appendInputWithSimilarValues function takes a list of inputs and appends to that list
a new input which is the same as an existing input except for one value.  For example, if the
list of inputs contains [0 1 0 1], then this function will append [0 0 1 1] to the end of
the list.  This function does not modify any other part of its arguments; rather it returns a new set containing all original elements plus one additional element.
====SPLIT====
The appendInputWithNSimilarValues function takes a list of inputs and appends to that list
a number of new inputs which are similar to the original input. The function does this by 
looking for transitions in the input vector, such as from [0, 1] -&gt; [0, 0] -&gt; [0, 1], and 
replacing each transition with an equivalent transition where both bits change (e.g., from 
[0, 1] -&gt; [0 , 0]. This is done up to numNear times for each input.
====SPLIT====
The modifyBits function takes in an input value and a number of changes to 
make. It then randomly selects that many changes from the set of possible 
changes, and makes them to the input value. The function returns a new copy of
the modified bit string.
====SPLIT====
The getRandomWithMods function takes an inputSpace and a maxChanges parameter.
It returns a random value from the inputSpace, with up to maxChanges modifications.
The modifications are bitwise mutations applied in sequence to a copy of the 
original value.
====SPLIT====
The createRecordSensor function creates a RecordSensor region called &quot;sensorRegion&quot; and 
specifies the dataSource as a file record stream instance. The function returns the sensorRegion.
====SPLIT====
The createNetwork function creates a network with the following layout:
====SPLIT====
The runNetwork function runs the network and outputs a CSV file containing
the actual, L2 predicted, L2 anomaly score, and L4 predicted values. The function
takes in a network object (passed implicitly from the caller) as well as an optional number of records to run. 
If no number of records is given then all four input files are used by default.
====SPLIT====
The clean function takes a string and removes all leading/trailing whitespace
from each line.  It also removes blank lines from the beginning and end of the
string.
====SPLIT====
The MetricsManager.getMetrics function returns a dictionary of the metrics that have been calculated since
the last time this function was called. The keys in the returned dictionary are the metric labels, and 
the values are their corresponding values. For example:
====SPLIT====
The MetricsManager.getMetricDetails function returns a dictionary containing the details of the metric with
the given label.  If no such metric exists, None is returned.
====SPLIT====
The MetricsManager._addResults function is the primary function responsible for taking a set of 
inference results and adding them to the MetricsManager's internal store of results. This function 
is designed to handle both temporal and non-temporal classifications, as well as with/without 
anomaly scores. The goal is to eventually create a classifier that can handle all three cases, 
though this work may take some time. For now, we will focus on simply handling temporal outputs.
====SPLIT====
The MetricsManager._getGroundTruth function is a helper function that retrieves the ground truth
value for a given inference element.  It does this by inspecting the InferenceElement enum to
determine which member of the sensorInput object should be used as an input to this inference element.
For example, if we wanted to know what color ball was present in the current frame, we would call:
====SPLIT====
The MetricsManager.__constructMetricsModules function is responsible for taking a list of MetricSpecs and
creating the corresponding MetricsModules.  It does this by calling the getModule function in metrics module, which
returns an instance of the class defined by the metric's 'class' variable.
====SPLIT====
The InferenceShifter.shift function takes a ModelResult object and shifts each inference within it back in time.
For example, if an inference was made at 2018-04-09T12:45:00Z and the input record's timestamp is 2018-04-09T12:44:59Z, then
the shifted inference will be written into the location for 2018-04-09T12:44:00Z. This is to help with comparing multiple
ModelResults that were produced by models at different sample rates or with different delays. The InferenceShifter leaves 
nonInference elements unchanged (i.e., rawInput,
====SPLIT====
The generateStats function collects statistics for each field in the data file.
The statistics are returned as a dictionary, where the keys are field names and
the values are objects containing various statistics for that field. For example:
====SPLIT====
The main function of this script is to run the experiment defined by
the command line arguments and report on the results.
====SPLIT====
The _abbreviate function takes a string and an integer as input. If the length of the string is greater than
the threshold, it returns a new string that contains the first few characters of the original string plus three periods.
Otherwise, it returns the original inputted string.
====SPLIT====
The ClientJobsDAO.__getDBNameForVersion function generates a name for the
database given a particular version number.  The name is generated based on
the parameters in the configuration file.  For example, if the prefix is 'jobs'
and suffix is 'nupic', then passing version 3 would return 'jobs_nupic_3'.
====SPLIT====
The ClientJobsDAO.connect function establishes a database connection.
====SPLIT====
The ClientJobsDAO._getMatchingRowsNoRetries function is a helper function that retrieves
the rows from the 'jobs' table that match the given parameters.  The caller can specify
whether or not to retry if there are no matching rows by setting retryIfNone=True/False.
The function will try to retrieve all matching jobs in one pass before returning, and it does not return any status messages about the query.  If there are no matches, then an empty list is returned.
====SPLIT====
The ClientJobsDAO._getOneMatchingRowNoRetries function is a helper function that retrieves one row from the
specified table, which must have a primary key defined. The search is performed without any retries and
without consulting the job retry cache. This function is therefore suitable for calling when executing DML 
operations on tables, such as UPDATE or DELETE. It is not suitable for functions that retrieve data, such as 
the get*() functions in this file or the GetFields() DAO function -- those are better suited for use with 
job-result rows that have been added to the job-results cache by ClientJobsDA
====SPLIT====
The ClientJobsDAO.jobInsert function creates a row in the jobs table.
It returns the jobID of the new row.  If the &quot;alreadyRunning&quot; parameter is True,
then it first searches for any pre-existing, unfinished job with matching client info and command line (the two primary values used to identify a unique job).
If such a pre-existing, unfinished job is found it returns its id; otherwise it inserts a new fresh job record and returns that id.
The returned status may be one of: QUEUED, RUNNING or FINISHED.
====SPLIT====
The ClientJobsDAO._startJobWithRetries function is a helper function that does the following:
====SPLIT====
The ClientJobsDAO.jobReactivateRunningJobs function is a utility function that updates the status of all running jobs to
running again. This function is used by unit tests to reset the state of the ClientJobsDAO so that jobs can be run again.
====SPLIT====
The ClientJobsDAO.jobGetDemand function is a special function that returns
a list of job demand tuples.  Each tuple has the form:
====SPLIT====
The ClientJobsDAO.jobCancelAllRunningJobs function marks all currently-running jobs as canceled. 
It is intended to be called by the application in response to a user action, such as pressing 
a &quot;Cancel&quot; button.
====SPLIT====
The ClientJobsDAO.jobCountCancellingJobs function counts the number of jobs that are in a
non-terminal state and have been marked for cancellation.  This is used by the
controller to determine if it can safely kill workers without losing any running jobs.
====SPLIT====
The ClientJobsDAO.jobGetCancellingJobs function is used to retrieve the list of job IDs for jobs that are currently being cancelled.
====SPLIT====
The ClientJobsDAO.partitionAtIntervals function is a helper function that can be used to divide up work
(data) into chunks.  It returns an iterator that will provide chunked subsets of the data parameter.
The ClientJobsDAO uses this function to divide computation (data) into chunks which are processed
and returned to the caller in a round-robin fashion, using Hadoop's capability to run map tasks in parallel.
Each map task is charged with the responsibility of handling one chunk from the return value of this function, 
so several instances of this class can be run in parallel across multiple nodes at once (if your H
====SPLIT====
The ClientJobsDAO.jobInfoWithModels function is a convenience function that retrieves
the job information for a given jobId and combines it with the model information
for that job (with model fields substituted in where applicable) into a single
result list. The ClientJobsDAO.modelsForJobInfo function may be used to retrieve only the models, instead of all of the jobs fields as well.  This is useful when you want to get an idea of how many models have been generated for particular hypersearch, without having to query the jobs table directly using JOINs (which can be slow). The ClientJobsDAO.jobInfoWithModels
====SPLIT====
The ClientJobsDAO.jobInfo function finds a row in the jobs table by jobID.
The returned namedtuple contains values for all of the fields in that row.
====SPLIT====
The ClientJobsDAO.jobSetStatus function is used to update the status of a job.
====SPLIT====
The ClientJobsDAO.jobSetCompleted function marks a job with the given jobID
as completed, setting its completionReason and completionMsg. It also sets the
end time of the job. If useConnectionID is True, then we first verify that
the job is assigned to this CJM using the connectionID (CJM-side connection
ID), otherwise we raise an exception. The caller may have connections to multiple 
CJMs, but it can be assured that *at most one* of those connections will be able 
to complete this job.
====SPLIT====
The ClientJobsDAO.jobCancel function marks a job for cancellation.
====SPLIT====
The ClientJobsDAO.jobGetModelIDs function returns a list of model IDs for the models
associated with the given jobID.
====SPLIT====
The ClientJobsDAO.getActiveJobCountForClientInfo function returns the number of jobs
currently running for a given client.  The function takes as an argument the &quot;clientInfo&quot;
field of a job, which is essentially the contents of the &quot;client_info&quot; column from table
&quot;ClientJobs&quot;.  This string can be parsed to determine information about where the client is running.
This function was originally created so that UI code could avoid performing multiple queries to
get this information; instead, one call can retrieve both pieces of data.
====SPLIT====
The ClientJobsDAO.getActiveJobCountForClientKey function returns the number of jobs
currently in progress on a given client.  This is used by the router to determine how many
clients or workers should be started for existing clients.  The function takes as input a string
clientKey, which is an identifier for the client and an integer maxJobs, which defaults to 1.  
The function returns an integer representing the number of active jobs on that client.
====SPLIT====
The ClientJobsDAO.getActiveJobsForClientInfo function is a helper function that retrieves
the jobIDs and names of all active jobs associated with the given clientInfo.
The clientInfo parameter is expected to be a string encoded in json format, containing
information about the environment that this application instance is running in.  The following
example json-encodes an example client info structure:
====SPLIT====
The ClientJobsDAO.jobUpdateResults function updates the results field in the job table.
====SPLIT====
The ClientJobsDAO.modelsClearAll function clears the models table.
====SPLIT====
The ClientJobsDAO.modelsInfo function returns a list of the models in the
database.  The ClientJobsDAO.modelsInfo function takes as input parameter
modelIDs, which is a sequence of model IDs for which information should be
returned. If modelIDs is empty, then information on all models are returned.
====SPLIT====
The ClientJobsDAO.modelsGetFieldsForJob function is a utility function that
returns the fields of a model given its job id.  The actual data values for the
model can be fetched with ClientJobsDAO.modelsGetData().  This function is most
useful in conjunction with ClientJobsDAO.modelsGetFieldsForJob() to fetch all
the fields of interest for a particular job, and use those as input parameters to
ClientJobsDAO.modelsGetData() to get the corresponding data instances (and, e.g., plot them).
====SPLIT====
The ClientJobsDAO.modelsGetFieldsForCheckpointed function returns a list of tuples where
each tuple contains the model id and a dictionary mapping field names to field values for that particular model.
The function takes two arguments: jobID, fields. The jobID argument specifies which models should be retrieved from the database. The fields argument is a list of strings specifying which fields should be returned for each checkpointed model. For example, if the method is called with jobID = 1234 and fields = ['model_state', 'status'], then each checkpointed model's id and status are returned in a list of tuples ordered by ascending id.
====SPLIT====
The ClientJobsDAO.modelsGetParams function is a utility function that
returns the params and params hashes for the models identified by the given
modelIDs.  The modelIDs parameter must be a sequence that contains at least
one modelID.  If it contains one or more modelIDs, then this function returns
a list of namedtuples, where each namedtuple has these fields:
====SPLIT====
The ClientJobsDAO.modelsGetResultAndStatus function is a utility function that accepts as input
a sequence of model IDs and returns for each the corresponding results and status. The result
and status fields are either populated with results, if they exist, or else 'None'. This function
is most often used in conjunction with a call to ClientJobsDAO.modelsGetUpdateCounters to fetch 
the update counters along with the results and status. The combination of these two functions 
allows one to iterate over models incrementally in order of increasing model ID or insertion time.
====SPLIT====
The ClientJobsDAO.modelAdoptNextOrphan function is a higher-level function
than modelAcquire.  It tries to acquire one of the models that are currently
idle and whose last update was at least maxUpdateInterval seconds ago.  If no
such models exist, it acquires the oldest model with status==RUNNING.  The
difference between this function and modelAcquire:
====SPLIT====
The KNNClassifierRegion._initEphemerals function initializes all of the _knn member variables.
====SPLIT====
The KNNClassifierRegion.enableTap function enables the KNNClassifierRegion to output its internal state (the &quot;knn&quot; member variable) to a file. 
The knn member variable contains the entire internal state of a KNNClassifierRegion, and is essentially a matrix of distances between input vectors and their associated class labels.
This function takes one argument: an absolute path where the output will be written. The output is formatted as follows:
====SPLIT====
The KNNClassifierRegion.disableTap function disables the tap stream on the input file handle.
====SPLIT====
The KNNClassifierRegion.handleLogOutput function writes the output of a KNNClassifierRegion's
compute() function to a log file.  The output is written as an array of numbers, where each number
is the classification score for one category.  The order in which categories are processed is determined
by KNNClassifierRegion's &quot;categoryList&quot; parameter (which maps to the --categories command-line argument).
====SPLIT====
The KNNClassifierRegion._storeSample function takes an input vector and a true category label.
It stores this sample in the KNNClassifierRegion's internal data structures, which are organized as follows:
  - self._samples is a numpy array of vectors representing all stored samples. Each row is a sample vector.
  - self._labels is a list of category labels corresponding to the rows in _samples. The i'th element of _labels
    corresponds to the i'th row in _samples, and specifies the category for that sample vector. Category labels are integers (0 or 1) by default, but can
====SPLIT====
The KNNClassifierRegion._finishLearning function is called after the learning
   phase of the KNNClassifierRegion has completed. In this function, we simply
   compute a leave-one-out validation accuracy to report as our final accuracy.
====SPLIT====
The generateStats function generates statistics for a given file and returns
them in a dict.  The statistics are determined by the ``statsInfo`` parameter,
which specifies which stats to collect.  Here's an example:
====SPLIT====
The _fixupRandomEncoderParams function is a helper function that fixes up the encoderParams for 
RandomDistributedScalarEncoder. The params argument is a dict containing an entry called 
&quot;modelConfig&quot;, which itself contains an entry called &quot;modelParams&quot;. This latter contains another dict, 
called &quot;sensorParams&quot;. That contains yet another dict, called &quot;encoders&quot;, whose values are either None or 
dicts of parameters for the encoders. For example:
====SPLIT====
The MonitoredTemporalMemory.read function reads a serialized MonitoredTemporalMemory object from the given proto
object and initializes all of its fields appropriately. This function is automatically called by PyRegion's
read function, which is why you should never need to call it yourself.
====SPLIT====
The pickByDistribution function takes a distribution and an optional random number generator.
It returns the index of a randomly chosen element from the distribution.
The function is useful for generating random numbers according to frequency distributions, such as:
====SPLIT====
The Indicator function takes a position and size, and returns an array of zeros
with a 1 at the specified position.  This is useful for creating indicator variables
for use in regression problems.
====SPLIT====
The MultiIndicator function creates a vector of zeros with one element set to 1.
====SPLIT====
The Distribution function takes a set of positions and the number of times
each position was observed, and returns an array with the normalized probability
of each position.  If a list of positions is given, then those are used as the
positions for which to calculate probabilities.  If only one position is given,
then that will be used as both the first and last index in the output array.
====SPLIT====
The ConditionalProbabilityTable2D.grow function is a helper function that
resizes the underlying SparseMatrix to accommodate new rows and columns.  If
the matrix has not yet been initialized, it initializes it with the given row
and column sizes.  Otherwise, if either of the dimensions are bigger than what
has already been allocated, then more memory is allocated for the respective
row or column (but never both).  Finally, if either of those dimensions are less than what has already been allocated then all probabilities will be renormalized.
====SPLIT====
The ConditionalProbabilityTable2D.updateRow function takes a row index and a distribution,
and updates the ConditionalProbabilityTable2D object accordingly.  The function first calls
the grow method to make sure that there is enough room in the underlying histogram for all of
the new rows and columns.  It then adds the given distribution to the appropriate cell in
the underlying histogram, updating all of its sums as it goes along.
====SPLIT====
The importAndRunFunction function imports a function from a module and runs it.
====SPLIT====
The MovingAverage.compute function computes the moving average of a new value.
   The function takes in four arguments:
     1) self, because it is a class method.
     2) slidingWindow, which is the list containing the data over which we are computing moving averages.  This argument has a default value of an empty list [] as when you instantiate an instance of this class MovingAverage, your instance will have no data in its slidingWindow attribute upon initialization (see __init__ function).  You access this variable by typing self.slidingWindow at any point after initialization (but not before...that would just cause Python to
====SPLIT====
The MovingAverage.next function computes the moving average of a given value.
The function takes three arguments:
    1) newValue - The new value to be added to the MovingAverage instance's window.
    2) slidingWindow - A list containing all values in the current window, 
       where each element is a tuple with two elements: (value, weight).  This argument is passed by reference.  
       The function modifies this argument as it adds up weights and updates values in order to compute the next average.
====SPLIT====
The MetricPassThruPrediction.addInstance function is used to add a new instance of the metric.
The groundTruth and prediction arguments are expected to be lists of values, with one value for each class.
The record argument is optional and can contain any information that you want to display in the results table (e.g., filename, ID).
The result argument is also optional if you have already computed the metric value for this instance.
====SPLIT====
The CustomErrorMetric.mostLikely function takes a dictionary of predictions and returns the most likely prediction.
If there is only one prediction, it will return that prediction. If there are multiple predictions, it will return the 
prediction with the highest probability.
====SPLIT====
The CustomErrorMetric.expValue function calculates the expected value of a set of predictions.
The function takes in a dictionary where each key is an item and each value is the probability that 
the item will be predicted. The function returns the sum of all keys multiplied by their corresponding 
probabilities.
====SPLIT====
The Encoder.getScalarNames function returns a list of the scalar values that are encoded by the encoder.
For example, for a category encoder, this function returns all the categories that are encoded. 
The return value is a list of strings. For example: ['attending', 'not-attending']
====SPLIT====
The Encoder._getInputValue function is a helper function that does the following:
====SPLIT====
The Encoder.getFieldDescription function returns a tuple containing the offset and length of the field specified by fieldName.
====SPLIT====
The Encoder.encodedBitDescription function returns a tuple containing the name of the field
and the bit offset within that field where some given bit can be found. This is in contrast to
the Encoder.getDescription function which returns a list of tuples, one for each field, with
the first element in the tuple being the field's name and second element being its offset from
the beginning of encoded message (in bits). The third element varies depending on whether it is 
a single-bit or multi-bit field.
====SPLIT====
The Encoder.pprint function prints the output of the encoder in a human-readable format.
It takes two arguments:
  - output is an array of bits that have been encoded by the Encoder instance.
  - prefix is an optional argument that can be used to add a prefix to each line of text printed.
====SPLIT====
The Encoder.decode function
====SPLIT====
The drawFile function takes in a dataset, the patterns and cells of that dataset,
the window size (w), and the figure number (fnum). It then creates a matrix with 
dimensions equal to the length of patterns by length of cells. The matrix is filled 
with values representing how many times each pattern matches with each cell. The 
matrix is then displayed using matplotlib's matshow function.
====SPLIT====
The Example.createInput function creates a random input vector of length self.inputSize and stores it in the Example's inputArray attribute.
====SPLIT====
The Example.run function performs the following steps:
====SPLIT====
The KNNClassifier.clear function clears the state of the KNNClassifier.
After calling this function, it will be as if you just created the classifier.
All category and partition lists/maps will be empty and all learned patterns
will be forgotten. The learning mode, k-nearest neighbor parameters, and sparse
memory parameters are preserved.
====SPLIT====
The KNNClassifier._removeRows function removes rows from the sparse matrix
representation of the training data.  This function is used when a previously learned
category is no longer considered by the classifier to be an active category.  The sparse
matrix representation will have multiple rows for a single category ID, each representing
a different offset in time for that particular pattern presentation.  Since we don't want to change what we view as &quot;categories&quot; just because they are older entries in our training data, we keep these multiple rows around and update them so that they reflect any changes made to their associated category over time (e.g., if it was
====SPLIT====
The KNNClassifier.getDistances function computes the distances between a given input pattern
and all the patterns in its training set.  It returns two values: (a) a list of distance values,
one for each training pattern and (b) a list of category indices, one for each training pattern.
The function is called as follows:
====SPLIT====
The KNNClassifier.infer function is the primary public interface to KNNClassifier.  It
takes a set of input patterns and returns the inferred categories for each.  In
addition, it returns several intermediate quantities that can be useful in
interpretting what the classifier learned:
====SPLIT====
The KNNClassifier.getClosest function takes a list of input patterns and returns the closest match
to one of the KNNClassifier's training vectors.  The function returns a tuple containing:
(winner, inferenceResult, topNCats)
where winner is the index into self._categoryList corresponding to the closest match.  In addition, if topNCats is specified (the default value is 3), then inferenceResult will be modified to contain an array representing up to topNCats categories that were tied for closest match with their distances from inputPattern.   If there are fewer than 'topKCategories' matches in 'inferenceResult', then all
====SPLIT====
The KNNClassifier.closestTrainingPattern function finds the closest pattern in a given category to an input pattern.
====SPLIT====
The KNNClassifier.getPattern function returns the pattern stored in the KNNClassifier instance's internal memory
at index idx. If sparseBinaryForm is True, then a numpy array containing a sparse binary form of the pattern is returned;
otherwise, if cat is not None, then the full non-zero (if sparseBinaryForm==False) or sparse binary form [non-zero entries = 1] 
of that category's prototype pattern at index idx within self._categoryList will be returned. If cat == None and idx == None 
then all patterns are returned in their full non-sparse binary forms
====SPLIT====
The KNNClassifier.getPartitionId function returns the partitionId of a given pattern.
====SPLIT====
The KNNClassifier._addPartitionId function adds a partitionId to the KNNClassifier._partitionIdList.
If no partitionId is provided, then the function will add numpy.inf to the _partitionIdList.
The function also updates KNNClassifier._partitionIdMap by adding a key-value pair of {partitionID: [indices]}.
====SPLIT====
The KNNClassifier._rebuildPartitionIdMap function is called by the KNNClassifier.learn function
and stores a mapping from each partitionId to its row indices in the training data.  This allows
the KNNClassifier.classify function to quickly determine which rows of the test data are associated with a given partitionId, and then query those rows of the training data for nearest neighbors.  For example, if there are 100 partitions (i.e., 10x10 grid), then _rebuildPartitionIdMap will create a dictionary where every key is an integer between 0 and 99 inclusive, and each corresponding value is a list containing all row indices that
====SPLIT====
The KNNClassifier._calcDistance function calculates the distance between a given input pattern and all of the
prototypes in memory.  The function takes two parameters:
- self: this is an instance of KNNClassifier class, which contains methods for storing prototypes and performing
        nearest neighbor classification.  This parameter allows access to class properties like memory, distanceNorm, etc.
- inputPattern: this is a numpy array containing one or more dimensions (the number of dimensions should be consistent with those used when calling KNNClassifier's train method).  This parameter allows access to data stored in self._Memory via calls to numpy's 'take
====SPLIT====
The KNNClassifier._getDistances function computes the distances between a given input pattern and all the
patterns in its training set.  The function first sparsifies the input vector using scipy.sparse.csr_matrix,
and then computes Euclidean distances between this sparse input vector and each of their corresponding training vectors,
using scipy's csr_matrix dot product function (which is much faster than computing cosine similarity).  If self._vt is not None,
then it will actually first project the sparse input into PCA space before computing distances to each of their corresponding training vectors.   Finally
====SPLIT====
The KNNClassifier.remapCategories function is a helper function that is used to remap the categories in the
KNNClassifier's internal category list. This function takes as input a mapping of old category indices to new
category indices, and calls KNNClassifier.remapCategories on this classifier (self). The purpose of this function
is to adjust what the KNNClassifier sees as its set of categories, without actually changing what it has been trained on.
====SPLIT====
The RecordSensor._convertNonNumericData function takes the spatial and temporal output from a RecordSensor encoder,
and converts it into scalar values.  This is necessary because NuPIC expects the output from an encoder to be a single value,
not a list of values.  The function iterates through each element in both spatial and temporal outputs (hence &quot;i&quot; in the for loop),
and checks if its type is an integer or float. If it is not, then it gets converted to one using Encoders' getScalars() method.
====SPLIT====
The RecordSensor.getOutputElementCount function is called by NuPIC before each learning, 
inference, and anomaly detection run. It returns the number of elements that will be produced 
by this node for each of the RecordSensor's output slots (named 'resetOut', 'sequenceIdOut', 
'bucketIdxOut', 'actValueOut', etc.). The number of elements returned in each list corresponds to the length of the list provided as an argument to getOutputElementCount. In our case, since we have no encoder set on our RecordSensor node yet (and therefore no outputs), it returns 0 for all output slots.&quot;
====SPLIT====
The RecordSensor.setParameter function allows you to set the following parameters:
   topDownMode (bool) - Determines whether or not the node will operate in &quot;topDownMode&quot;. 
                        If topDownMode is enabled, then the node will compute a signature for 
                        each encoder output and learn on those signatures. It also remembers what it learned previously. 
                        When operating in this mode, all data coming into the node is ignored except that which corresponds to entries in activeColumns.
====SPLIT====
The FileRecordStream.rewind function resets the stream to point at the first record.
This function is usually used in conjunction with a Python for loop, like so:
====SPLIT====
The FileRecordStream.getNextRecord function reads a line from the file and returns it as a list of fields. Note:
the FileRecordStream class is intended to be an interface, and should not change. To adapt the FileRecordStream API to
a new format, you should derive from FileRecordStream (in recordstream.py) and override the 'getNextRecordDict()' function.
====SPLIT====
The FileRecordStream.appendRecord function writes a single record to the end of the file.
The FileRecordStream uses a csv.writer object to write each record; this function also
handles advancing the writer's sequence (i.e., line number) and writing out any special
header lines that may be defined in field metadata.
====SPLIT====
The FileRecordStream.appendRecords function appends records to the end of a file.
====SPLIT====
The FileRecordStream.getBookmark function returns a string that can be used to
recreate the state of an instance of FileRecordStream.  The returned value is a
JSON-encoded dictionary with two keys: filepath and currentRow.  The value for
filepath is the path to the file being read, and currentRow is the number of rows
that have been read so far from that file.  If no bookmark has yet been created, or if there are no more records in this stream, None will be returned.
====SPLIT====
The FileRecordStream.seekFromEnd function seeks to the last record in the file.
The function returns a bookmark to this position which can be used later with 
FileRecordStream.seekFromBeginning or FileRecordStream.seekFromCurrent
====SPLIT====
The FileRecordStream._updateSequenceInfo function is responsible for maintaining information about the
sequence currently being processed.  It maintains:
- self._currSequence which holds the id of the current sequence or None if no sequence is active
- self._sequences, a set of all sequences that have been seen so far.  This is used to catch multiple sequences in a row and raise an exception.
- self._currTime, a timestamp indicating when the last record was received.  This is used to detect time travel (i.e., records from the future).
====SPLIT====
The FileRecordStream._getStartRow function is a helper function that takes in the bookmark and returns the currentRow.
The realpath is used to check if the filepath of File's filename matches with that of the bookmark. If it does, then we use 
the currentRow from bookMarkDict['currentRow'] and return it as an integer. Otherwise, we return 0 because there is no valid 
bookmark for this file.
====SPLIT====
The InferenceElement.isTemporal function is a static method that helps the 
inference engine determine whether an InferenceElement is temporal.  Temporal
elements are elements which can be considered to have a &quot;position&quot; in the input,
such as prediction and anomaly scores. Non-temporal elements include things like 
the list of inference elements or encoders (which don't really have a position) or 
input statistics (which always come at the beginning of inference). This function allows us to query whether an element's name indicates that it's temporal.
====SPLIT====
The InferenceElement.getTemporalDelay function is used to determine the delay between an input element and its corresponding output element.  For example, if the input were a time step value of 0, and this were mapped to output inference element &quot;prediction&quot;, then this would be delay of 1 time step (since prediction corresponds to a time step of 1).
====SPLIT====
The InferenceElement.getMaxDelay function returns the maximum delay between inference elements.
For example, if one were to compute Anomaly likelihoods over past 5 time buckets, 
the first bucket would have a delay of 0 and the last bucket would have a delay of 4. 
The InferenceElement.getMaxDelay function returns 4 as an integer value.
====SPLIT====
The InferenceType.isTemporal function returns True if the input inference type is a temporal inference type.
====SPLIT====
The Enum function creates a new type with the given names as instances of that type.
It also adds three functions to the created type:
getLabel(val), validate(val), getValues(), getLabels(), and getValue(label).
====SPLIT====
The makeDirectoryFromAbsolutePath function creates a directory at the specified absolute path.
If the directory already exists, it is not re-created. If the parent directory does not exist, an error is raised.
====SPLIT====
The ConfigurationBase._readConfigFile function reads in a file that contains
configuration parameters and adds them to the configuration object.  The
parameters are specified as name/value pairs in a simple XML format; for example:
====SPLIT====
The Configuration.setCustomProperties function is used to set custom configuration properties.
====SPLIT====
The Configuration.clear function clears the in-memory settings cache, forcing reload upon subsequent &quot;get&quot; request.
It also resets in-memory custom configuration info.
====SPLIT====
The Configuration.resetCustomConfig function clears all custom configuration properties.
====SPLIT====
The _CustomConfigurationFileWrapper.clear function clears the persistent configuration file.
====SPLIT====
The _CustomConfigurationFileWrapper.getCustomDict function returns a dictionary of the custom configuration file's properties.
====SPLIT====
The _CustomConfigurationFileWrapper.edit function writes the given properties to a custom configuration file.
====SPLIT====
The _CustomConfigurationFileWrapper._setPath function is a helper function that sets the _path attribute of the
_CustomConfigurationFileWrapper class to a specific path.  The _path attribute is used by other functions in this class
to access configuration files.  This function will first check if an environment variable called NTA_DYNAMIC_CONF_DIR exists, and if it does, then we use that as our base directory for configuration files.  If the environment variable doesn't exist, then we default to using /etc/nupic/custom/.
====SPLIT====
The Particle.getState function returns a dictionary containing the following:
====SPLIT====
The Particle.initStateFrom function is used to initialize the state of a particle from
a previously saved state.  This function is called by the Swarm class when creating new particles.
It should not be called by users directly.  The initStateFrom function accepts two arguments:
====SPLIT====
The Particle.copyVarStatesFrom function copies the variable states from another particle.
====SPLIT====
The Particle.getPositionFromState function returns a dictionary of the positions of all variables in the state.
====SPLIT====
The Particle.agitate function is a method of the Particle class.
It takes no arguments and returns nothing.
It simply updates the position of each particle in the swarm by adding a random number to each dimension, where that random number is drawn from a Gaussian distribution with mean 0 and standard deviation 1.
====SPLIT====
The Particle.newPosition function is called by the ParticleSwarmOptimizer
to generate a new position for the particle. It takes in a globalBestPosition
which is the best position found so far for any particle in the search space.
If not used, this argument will be None. The function returns a new position, which is
a dictionary mapping variable names to values (same as Particle.getPosition).
====SPLIT====
The ModelFactory.__getLogger function is a class method that initializes the logger for this 
class. It does not need to be called by the user, as it will automatically be invoked if @classmethod
is included in the definition of any subclass of ModelFactory. If no logger was previously created, 
then one will be initialized using default settings (logging to stdout with level set to INFO). The 
returned logger can then be retrieved using either getLogger or __getLogger.
====SPLIT====
The ModelFactory.create function creates a model instance from the provided
configuration.  The configuration is expected to be a dict with one or more of the following keys:
    'model':      Which model class to use (e.g., HTMPredictionModel)
    'modelParams': A kwarg dict that will be passed into the constructor of `model` during initialization.
====SPLIT====
The TemporalMemory.compute function is the primary entry point for computing
the active and predicted cells. It requires an array of active column indices,
and returns a two-element tuple (activeCells, winnerCells). Here is how you can 
use it:
====SPLIT====
The TemporalMemory.activateCells function accomplishes two tasks:
====SPLIT====
The TemporalMemory.activateDendrites function is the primary learning mechanism of the TemporalMemory class.
It activates a set of segments, updates the permanences on those segments, and then creates new synapses to 
those that were predicted by cells in layer 4. The function takes three parameters:
====SPLIT====
The TemporalMemory.reset function clears all cell activity.
====SPLIT====
The TemporalMemory.activatePredictedColumn function is the primary
mechanism for learning in the TemporalMemory. It creates a new segment if
the active column has predicted cells, and if a previous cell in that same
column was previously active. If there are no matching segments, nothing
happens. Otherwise, this function activates every matching segment and adds a
new segment to every active cell with an inactive (depolarized but not yet 
learned) synapse.
====SPLIT====
The TemporalMemory.punishPredictedColumn function is called by the TemporalMemory.compute function to punish the
predicted segments of a column that has been predicted and then subsequently active. The function iterates through
the segment indices in each matching segment, and decrements their permanences by self.predictedSegmentDecrement.
====SPLIT====
The TemporalMemory._createSegment function creates a new segment on the cell with the least recently used segment.
It also updates lastUsedIterationForSegment to reflect that this segment has been used in this iteration.
If maxSegmentsPerCell is reached, then it destroys the least recently used segment and returns None.
====SPLIT====
The TemporalMemory._destroyMinPermanenceSynapses function removes a small number of synapses from each segment, 
where the least-permanently connected synapse is destroyed. This function is called by TemporalMemory.compute on every iteration, 
and serves to enforce the &quot;minPermanenceSynapses&quot; parameter that's passed into the constructor. The reason this function exists in TemporalMemory 
is because it can be implemented very efficiently -- O(log n) time complexity where n is number of synapses on segment -- which makes it ideal for use in an efficient compute method.
====SPLIT====
The TemporalMemory._leastUsedCell function is a helper function that returns the cell with the fewest number of segments.
It does this by iterating through all cells in the TM and finding which one has the fewest number of segments.
The TemporalMemory._leastUsedCell function then returns this cell.
====SPLIT====
The TemporalMemory._growSynapses function creates synapses to the winners of previous time step.
It takes in a list of active cells from the previous time step, and a random number generator object.
The function iterates through each cell in activeCells and checks if it has any segments. If it does, 
it randomly selects nDesiredNewSynapes (the number of desired new synapses) from the candidate segment 
list for that cell (this is calculated by taking length(activeCells) * self._maxNewSynapseCount). It then 
creates that many synapses to a random set of previously un-connected
====SPLIT====
The TemporalMemory._adaptSegment function is called when a segment becomes active.
It creates new synapses to previous segments according to the following parameters:
permanenceInc = 0.03, permanenceDec=0.00, predSegmentDecrement=0.0003 (usually set these values in the constructor)
The function iterates through all of the active cells and checks if they are connected to a segment on this timestep (i.e., cell activity from previous step). If it is connected, then it adds permanence increment until we reach 1 or permanence limit parameter (usually set at 0.3 in constructor). If
====SPLIT====
The TemporalMemory.columnForCell function takes a cell as input and returns the column that contains it.
The function is useful for finding which columns are active during a particular timestep, or for determining 
whether two cells are in the same column.
====SPLIT====
The TemporalMemory.cellsForColumn function returns a list of cell indices that are in the given column.
====SPLIT====
The TemporalMemory.mapCellsToColumns function takes a list of cells and returns a dictionary where the keys are columns
and the values are sets of cells in that column. This function is used to map activeCells, predictiveCells, and 
winnerCells to their respective columns.
====SPLIT====
The TemporalMemory.getPredictiveCells function returns a list of the cells that are predicted to become active.
The TemporalMemory class keeps track of which cells have been active during each time step, and stores this information in the self.activeCells attribute. 
This function iterates through all segments on the self.activeSegments list, and appends any cell that is not already on the predictiveCells list to it.
====SPLIT====
The TemporalMemory.read function reads a TemporalMemory instance from serialized data.
====SPLIT====
The SequenceMachine.generateFromNumbers function generates a sequence of numbers,
where each number corresponds to a pattern.  For example, if the list of numbers is [0, 1],
then the SequenceMachine will generate an output sequence that contains two patterns:
the first pattern is from the 0th position in self.patternMachine and the second pattern is from 
the 1st position in self.patternMachine.
====SPLIT====
The SequenceMachine.addSpatialNoise function adds spatial noise to the sequence.
The amount of spatial noise added is determined by the 'amount' parameter, which specifies how much of a percentage
of each pattern in the sequence should be replaced with random bits. For example, if amount = 0.2 and there are 5 patterns in 
the sequence, then 1 out of every 5 bits will be replaced with a random bit (on average). The function returns a new list 
containing all patterns from the original list but with some randomly-generated bits replacing part of them.
====SPLIT====
The SequenceMachine.prettyPrintSequence function takes a sequence of patterns and prints them out in a human-readable format.
====SPLIT====
The MonitorMixinBase.mmPrettyPrintTraces function is a convenience function that displays the traces of
a network as an ASCII table. The table has one row for each element in the traces list, and one column
for each trace. The first column is a running index (from 0 to n-traces), and the remaining columns are
the data elements from those corresponding to each trace. A horizontal line appears after every reset, if any exist; otherwise you will just see a sequence of rows with no breaks.
====SPLIT====
The MonitorMixinBase.mmPrettyPrintMetrics function is a convenience function that prints out the mean, standard deviation, minimum and maximum values for each metric in the metrics list.
====SPLIT====
The updateAnomalyLikelihoods function takes a list of anomaly scores, computes 
a new aggregate score for the window, updates the historical likelihoods and returns 
the updated likelihoods along with a new aggregate score. The arguments are:
anomalyScores - A list of records. Each record is a tuple (timestamp, value). This is either the raw input data or values computed by another function such as computeAnomalyScore().
params - The parameters dictionary which contains historical data and settings. This must be in sync with what updateAnomalyLikelihoods expects to allow updating existing information without recalculating it all from scratch each time
====SPLIT====
The AnomalyLikelihood._calcSkipRecords function is a helper function that calculates how many records
should be skipped when updating the anomaly likelihood params.  It takes 3 arguments:
- numIngested - the number of data points that have been added to the AnomalyLikelihood object (integer)
- windowSize - the size of each sliding window (number of data points, integer)
- learningPeriod - number of windows used for learning, must be less than or equal to windowSize (integer)
====SPLIT====
The AnomalyLikelihood.read function reads a previously saved AnomalyLikelihood object from disk.
====SPLIT====
The AnomalyLikelihood.write function writes the state of the AnomalyLikelihood object to a file.
====SPLIT====
The AnomalyLikelihood.anomalyProbability function computes the probability that a value is an anomaly.
The actual code for this lives in nupic/algorithms/anomaly_likelihood.py, but we have to expose it here as well so that 
it can be used by NuPIC clients.
====SPLIT====
The OPFTaskDriver.replaceIterationCycle function is responsible for replacing the phase manager of the OPFTaskDriver instance. The phase manager is responsible for managing and executing phases in a given iteration cycle.
====SPLIT====
The OPFTaskDriver.handleInputRecord function is responsible for the following:
====SPLIT====
The _PhaseManager.__advancePhase function is responsible for advancing the phase of the game.
It does this by calling next() on self.__phaseCycler, which returns a reference to the next _Phase object in 
self.__phases (the list of phases). The new current phase is then initialized by calling enterPhase() on it.
====SPLIT====
The _PhaseManager.handleInputRecord function is responsible for advancing the current phase, 
and passing inputRecord to the current phase's model. The function returns a dictionary of results from the model.
====SPLIT====
The _IterationPhase.advance function is a generator function that advances the iterator and yields
the next value. If there are no more values to yield, it sets the _IterationPhase.__iter member to None.
====SPLIT====
The PreviousValueModel.write function writes the contents of the PreviousValueModel to a 
proto object. The proto object is then written out as a binary file. This function will be called 
by NuPIC before any model persistence takes place in OPF.
====SPLIT====
The PreviousValueModel.read function is a factory function that creates an instance of the
PreviousValueModel class.  It requires a single parameter, which is the OPF protobuf object
representing the PreviousValueModel instance to be created.
====SPLIT====
The lscsum function computes the following sum:
====SPLIT====
The normalize function takes a list of numbers and returns a normalized
list where the sum of the numbers equals 1.  For example, given [2, 4], it
returns [0.5, 0.5].  This is useful for making probability calculations.
====SPLIT====
The ExtendedLogger.debug function is a wrapper around the base logger's debug function.
It adds an additional optional parameter, msgExtension, which will be prepended to the message.
This allows for more context in log messages and makes it easier to correlate log messages with their corresponding code.
====SPLIT====
The ExtendedLogger.info function is a wrapper around the standard Python logging.Logger.info function,
and provides the following additional features:
====SPLIT====
The ExtendedLogger.warning function is a wrapper around the standard Python logging.warning function.
It adds an additional optional parameter, msgExtension, which will be prepended to the message before it is logged.
====SPLIT====
The ExtendedLogger.error function is a wrapper around the base logger's error function.
It adds an additional message to the end of the given message, which contains information about
the file and line number from which it was called.  This is helpful for debugging when trying to
determine where in code an error occurred.
====SPLIT====
The ExtendedLogger.critical function is a wrapper around the standard Python logging.Logger.critical function,
but it adds an additional optional parameter called &quot;extendedMsg&quot;.  This parameter allows you to add extra information
to your critical messages that will be displayed in the log file and/or on screen (if you have configured your logger
to show those levels).  The extendedMsg parameter can be any object with a __str__ method, or it can be a callable that
returns an object with a __str__ method.
====SPLIT====
The ExtendedLogger.log function is a wrapper around the standard Python logging.Logger.log
method, which adds an additional argument to the function call: msgExtension, which is used to
add extra information to all log messages generated by this logger instance.  This can be used
to add context-specific information such as a unique identifier for this particular logger.
====SPLIT====
The _filterRecord function is a helper function that takes in a list of tuples,
and checks each tuple to see if it passes the filter. If all filters pass, then 
the record is accepted and added to the filteredRecordSet. Otherwise, it's rejected.
====SPLIT====
The _aggr_sum function aggregates all the values in a list by summing them up.
It is used to aggregate the predicted values for a given timestep, across all
the records that belong to the same timestep.  The function ignores missing data;
i.e., if a value is missing, it is ignored in the aggregation process.
====SPLIT====
The _aggr_mean function calculates the mean of a list of values.
It returns None if it receives a list with only SENTINEL_VALUE_FOR_MISSING_DATA.
====SPLIT====
The _aggr_mode function takes a list of values and returns the most commonly
encountered value. If there is a tie for most common, it selects one at random.
If no non-None values are seen, None is returned.
====SPLIT====
The generateDataset function reads a file from the ``inputFilename``, aggregates
the data in the file by time period (defined by the aggregationInfo), and writes
the aggregated records to an output file. The function returns a string containing
the name of the output file.
====SPLIT====
The getFilename function takes an aggregationInfo dictionary and an inputFile
string as arguments. The aggregationInfo dictionary contains information about
how the data in the input file should be aggregated, including:
====SPLIT====
The Aggregator._getEndTime function takes a datetime object as an argument and returns
a new datetime object that is the first second of the month immediately following
the month specified in the input. For example, if you pass in a datetime object with
a year and month of 3/15/2012, it will return a new datetime object with a year and 
month of 4/01/2012. This function is used to create end times for aggregation periods.
====SPLIT====
The Aggregator._getFuncPtrAndParams function takes a single parameter, funcName.
If the input is a string, it will attempt to find the appropriate function pointer and parameters for that particular aggregation function. 
It will return two values: fp and params. 
fp is the appropriate aggregation function pointer (e.g., _aggr_sum) while params is either None or an integer value identifying which field in self._inputFields to use as parameters for this particular aggregation.
====SPLIT====
The Aggregator._createAggregateRecord function creates a record that contains aggregated values for each field.
The function iterates through the list of fields and parameters, and if the field is to be aggregated, it will call
the appropriate aggregation function on its slice of data. The resulting record is then returned.
====SPLIT====
The Aggregator.next function aggregates records from the input stream.
It is called by a consumer of the Aggregator, e.g., an HTM network.
The Aggregator maintains a state consisting of an array of &quot;slices&quot; (see below)
and an end time, which represents the rightmost point in time that is still included 
in the current aggregation window for all slices. The Aggregator's job is to 
update its state and generate new output records as input records arrive and/or 
its end time advances. There are two cases when new output will be generated:
====SPLIT====
The Model.run function is where the real work of the model gets done. This
routine accepts an input record, which is a Python dictionary-like object
containing all fields that are specified in the Model's 'inputs' list.  It also
accepts any number of keyword arguments (these will be discussed later). The run
method should return a ModelResult namedtuple object containing two attributes:
====SPLIT====
The Model._getModelCheckpointFilePath function is a helper function that returns the path to the model checkpoint file.
====SPLIT====
The Model.writeToCheckpoint function writes the model state to a file in the given directory.
====SPLIT====
The Model.readFromCheckpoint function reads a Model from the specified
checkpoint directory.  This is intended to be used in conjunction with
the ModelCheckpoint function, which periodically saves the model state to disk.
The readFromCheckpoint function re-creates the model from that saved state
and makes it available for continued training or inference.
====SPLIT====
The Model.writeBaseToProto function writes the ModelBase instance to a capnp
proto.ModelProto message. This function is automatically called by
writeToProto() on every subclass of Model, but it may be called explicitly in
order to construct proto messages for other models that subclass from Model.
====SPLIT====
The Model.save function produces a directory containing model-related files,
such as the model itself.  The Model.load function can then be used to re-create
the same model in a new instance of OPF.
====SPLIT====
The Model._getModelPickleFilePath function is a helper function that returns the path to where the model will be saved.
====SPLIT====
The runExperiment function is the top-level function that
will be called to run a model experiment.  It will first initialize
the model using the command-line options and then call its
&quot;runExperiment&quot; method.  The &quot;runExperiment&quot; method will return a trained ModelInfo object, which is used to generate various output files (e.g., .temporal, .components).
====SPLIT====
The reapVarArgsCallback function is a callback function that allows us to specify
that the option can be invoked multiple times. This means that it will be called
for each occurence of the option and each time it will consume all remaining 
arguments in parser.rargs so that the next option can be parsed correctly.
====SPLIT====
The _reportCommandLineUsageErrorAndExit function prints the usage of the command line and an error message, then exits with a non-zero exit code.
====SPLIT====
The _runExperimentImpl function ...
====SPLIT====
The _getModelCheckpointDir function creates a directory for storing the model checkpoint files.
The function returns the path to that directory, which is named with a time-stamped label.
====SPLIT====
The getCheckpointParentDir function returns the parent directory of the checkpoint
directory. The checkpoint directory is named &quot;savedmodels&quot; and is located in the experiment
directory. This function finds that path, then goes up one directory to get to baseDir.
====SPLIT====
The _checkpointLabelFromCheckpointDir function extracts the label from a checkpoint directory.
====SPLIT====
The _isCheckpointDir function is a helper function that returns True if the given directory name
is a valid checkpoint directory.  A valid checkpoint directory must:
- end in the default extension '.checkpoint'
- begin with a '.' character, e.g., '.checkpoint_00' or '.my_checkpoint_folder.checkpoint'
====SPLIT====
The _printAvailableCheckpoints function prints out a list of available checkpoints for the experiment.
The list is printed to standard output, and it is printed in sorted order. The user can then start from a checkpoint.
====SPLIT====
The _TaskRunner.run function is the main loop of the prediction framework. It
creates a Model instance for each task, and calls its OPFTaskDriver.handleInputRecord()
method on each input record in the dataset until the dataset is exhausted or an error occurs.
The function also performs other supporting activities, such as evaluating and reporting performance metrics, etc.
====SPLIT====
The _TaskRunner._createPeriodicActivities function creates a list of PeriodicActivityRequest objects.
Each object represents an activity that is to be executed at a given periodicity, and may only be
executed if the task is currently running.  The activities are:
====SPLIT====
The corruptVector function takes a vector of bits (a numpy array), an amount
of noise to add, and the number of active columns in the input.  It returns a
new vector with randomly chosen bits flipped according to the specified noise
level.
====SPLIT====
The showPredictions function prints out the active and predictive cells for each column in the TM.
The columns are organized as a 4x4 grid, with 10 cells per column. The function iterates through 
the columns top to bottom, left to right. Within each column, it prints out the active (green) and 
predictive (blue) cells.
====SPLIT====
The trainTM function takes a sequence of vectors and trains the temporal memory.
The function returns the TM instance, which contains all information about the trained TM.
====SPLIT====
The PassThroughEncoder.closenessScores function computes a single closeness score for each bit in the encoder output.
The closeness score is defined as the fraction of bits that are on, or &quot;lit up,&quot; in both the expected and actual bitmaps.
If this score is 1.0, then every bit in the encoder output that should be on is actually on; likewise if it's 0.0, then every
bit marked as being on in the expected result is off (and vice versa).  If this value is less than 1.0 but greater than zero,
then some bits are out of alignment with their expected state -- perhaps
====SPLIT====
The getCallerInfo function returns a tuple containing the name of the method
that called it, the file in which that method is defined, and the class whose
method was called.  For example:
====SPLIT====
The title function prints a string to the console with a title bar
above it.  The title bar contains the string, s, centered in the
title bar.  The remainder of the title is decorated with '='.
====SPLIT====
The getArgumentDescriptions function extracts documentation about the arguments from the function docstring and
stores it in a list of tuples, one for each argument. Each tuple consists of:
====SPLIT====
The _genLoggingFilePath function generates a path for logging files.
====SPLIT====
The aggregationToMonthsSeconds function converts an aggregation object into a number of months and seconds.
====SPLIT====
The aggregationDivide function divides an aggregation dict that contains months/years
by one that contains seconds. This is useful for taking a usage metric like
``cpu_user_frac`` and dividing it by ``cpu_total_secs`` to get the time-weighted average 
fraction of CPU used by each process. The result will be a dict with months/years as keys, 
and the fractional contribution toward each month's total CPU consumption as values.
====SPLIT====
The validateOpfJsonValue function validates a given JSON value against the
OPF json schema.  The OPF json schema is specified by the opfJsonSchemaFilename
parameter.  This function is intended to be called as part of an assert
statement, so if validation fails, an assertion failure will occur.
====SPLIT====
The initLogger function is a helper function that attaches a new logging
object to the class or object passed as an argument.  The name of the logger
will be &quot;com.numenta.&lt;module&gt;.&lt;class&gt;&quot;.  For instance, if you invoke this
on your main handler script (say, taurus_engine.py), all other modules in your
application will have access to the 'log' variable declared below and used in so many places:
====SPLIT====
The matchPatterns function takes a list of regular expressions and returns a list of keys that match any
of the patterns. If no patterns are provided, it returns None.
====SPLIT====
The LogEncoder._getScaledValue function is a helper function for the LogEncoder class.
It takes in an input value and scales it to fall within the range of 0 to 1. 
The way this is accomplished is by finding the log base 10 of each value, then scaling that number between 0 and 1. 
If a value falls outside of the minval or maxval bounds, it will be set to either self.minval or self.maxval depending on which one has been set by default when instantiating an encoder object from this class.
====SPLIT====
The NetworkVisualizer.export function creates a NetworkX graph object from the network.
====SPLIT====
The bitsToString function takes an array of bits and returns a string
representation of the binary number.  The function assumes that the input
array is a list of 1's and 0's, with each element representing one bit in
the binary number.  For example, if arr = [0, 1, 0], then bitsToString(arr)
returns '2'.  If arr = [0] or [] (an empty array), then bitsToString(arr) returns '0'.
====SPLIT====
The percentOverlap function computes the percentage of overlap between two binary vectors.
The function takes three arguments: x, y and size.  The first two arguments are the binary vectors (x and y) whose percentage of overlap we want to determine.  The size argument is used in the event that the vectors are different sizes -- it simply keeps track of how many non-zero elements there are in each vector.
====SPLIT====
The resetVector function takes two vectors as input and returns a vector that is the same as the first
vector, but with all of its values set to zero. The function does not modify x2.
====SPLIT====
The runCPU function runs the CPU prediction model on a separate thread.
It also creates and updates the matplotlib chart that displays the model's
output.  The actHistory and predHistory deques are used to store data for
the chart.
====SPLIT====
The _extractCallingMethodArgs function extracts the arguments passed to the calling method.
====SPLIT====
The BacktrackingTMCPP._getEphemeralMembers function is a helper function that serves to
tell the C++ code what it needs to know about the instance variables of this class.
The C++ code will store this information in an efficient way, and will then create and manage 
the actual instances of these variables.  The BacktrackingTMCPP._getEphemeralMembers function tells the C++ code which instance variables are ephemeral (i.e., need to be recreated each time).  This allows the C++ code to avoid wasting time and space by creating new copies of objects that we know we'll throw away when we resume computation later (at
====SPLIT====
The BacktrackingTMCPP._copyAllocatedStates function is a helper function that copies the
   C++ allocated states to our Python object instances.  This is needed for printing out
   various state information, and for checkpointing.  Specifically, the _copyAllocatedStates()
   function does:
====SPLIT====
The BacktrackingTMCPP._setStatePointers function is called by the BacktrackingTMCPP.compute function
to set the C++ pointers to appropriate states in a single column of cells.  This allows for a speedup
over other implementations that use Python-only methods for setting state pointers.
====SPLIT====
The BacktrackingTMCPP._slowIsSegmentActive function is a helper function that implements the
   same algorithm as the _isSegmentActive function in C++.  It is included here to help
   make it easy to port unit tests from C++ to python.
====SPLIT====
The RandomDistributedScalarEncoder.mapBucketIndexToNonZeroBits function takes an index and returns the non-zero bits that correspond to it.
====SPLIT====
The RandomDistributedScalarEncoder._createBucket function creates a new bucket for each 
index between the maxIndex and minIndex (inclusive). The buckets are stored in the bucketMap attribute.
The _createBucket function works by first checking if the index is less than or greater than 
the minIndex and maxIndex, respectively. If it is less than minIndex nothing happens; however, if it is greater 
than or equal to then:
====SPLIT====
The RandomDistributedScalarEncoder._newRepresentation function is a helper function for the RandomDistributedScalarEncoder.encodeIntoArray function. It takes in an index and a newIndex, which are both integers that represent the bucket indices of two different encodings (the input encoding and the new encoding, respectively). The goal of this function is to swap one bit in each representation so that they overlap as little as possible.
====SPLIT====
The RandomDistributedScalarEncoder._newRepresentationOK function checks whether a new representation can be added to the encoder.
====SPLIT====
The RandomDistributedScalarEncoder._countOverlapIndices function takes two indices, i and j, 
and returns the number of bits that are set to 1 in both iRep and jRep. 
iRep is the binary representation of i after dividing it by _resolution. 
jRep is the binary representation of j.
====SPLIT====
The RandomDistributedScalarEncoder._countOverlap function is a helper function that 
takes in two representations and returns the number of elements they have in common.
====SPLIT====
The RandomDistributedScalarEncoder._overlapOK function is a helper function of the RandomDistributedScalarEncoder class.
It takes in two integers, i and j, and an optional parameter overlap. It returns True if the overlap between 
the encodings of i and j is less than or equal to self._maxOverlap (defaults to 1). If there are not enough bits 
in their encodings to allow for an overlap that large, it returns False.
====SPLIT====
The RandomDistributedScalarEncoder._initializeBucketMap function creates a dictionary of bit representations for each bucket index.
====SPLIT====
The SDRClassifierFactory.create function creates a new SDRClassifier instance.
It accepts the same arguments as the constructor of SDRClassifier, but allows you to omit
some set-up steps (such as creating a network). This is convenient when reusing parts of an
experiment that have already been configured, such as when reusing the output layer. For 
example:
====SPLIT====
The TemporalMemoryMonitorMixin.mmGetMetricFromTrace function calculates the metric specified by the user.
The trace passed in is a list of lists, where each element of the outer list represents a single column, and each element of that list represents an SDR for that column.
The function returns a Metric object which contains information about how well this particular TemporalMemory performed on this input sequence.
====SPLIT====
The TemporalMemoryMonitorMixin.mmGetMetricSequencesPredictedActiveCellsPerColumn function computes the number of predicted =&gt; active cells per column for each sequence.
====SPLIT====
The TemporalMemoryMonitorMixin.mmGetMetricSequencesPredictedActiveCellsShared function returns a Metric object that contains the number of sequences each predicted =&gt; active cells appears in.
====SPLIT====
The TemporalMemoryMonitorMixin.mmPrettyPrintConnections function prints the connections of a TemporalMemory in a human-readable format.
====SPLIT====
The TemporalMemoryMonitorMixin.mmPrettyPrintSequenceCellRepresentations function is a helper function that prints out the active cells for each sequence.
====SPLIT====
The createTemporalAnomaly function creates a network that is capable of
predicting the next value in a sequence. The region is created by calling
the provided createNetwork function, and the spatial pooler region is then
set to learn and infer like an SPRegion. The TMRegion must be set to topDownMode
to get prediction outputs. This function returns the TMRegion instance for use in 
updating the anomaly likelihood parameters so that they will produce accurate anomaly scores.
====SPLIT====
The add function adds a value to the specified column of the input file,
and writes it out to an output file.  The add function takes five arguments:
the reader, writer, column and start and stop indices.  It reads through each
row in the input file from row number start until row number stop (inclusive),
adding a value to the specified column in each row read.  The result is written
out as rows into an output file.
====SPLIT====
The scale function takes a reader, writer, column and multiple as arguments.
It then reads the specified column from the reader and multiplies it by the
specified multiple. The result is written to the writer.
====SPLIT====
